# Salesforce Work – Concrete History Dump

---

## 1. Major Project Contexts

### 1.1 State public benefits self-service + case management portal (Government Cloud)

* **Type:**
  Public sector – multi-agency public benefits / services portal built on Salesforce in a high-compliance government cloud environment.

* **Main Salesforce Clouds / Features:**

  * Public sector case management on core Salesforce platform.
  * Experience Cloud for citizen / client-facing portals.
  * Internal Service Cloud for staff triage and case work.
  * Flows (record-triggered, screen flows) for intake, triage, routing, and automation.
  * Custom objects for notices, transactions, and integration logs.
  * Platform Events and (at least at design level) Event Bus style patterns for outbound events.
  * Permission-set–based security model; move away from profile-heavy model.

* **External Systems Involved:**

  * Legacy eligibility / benefits systems (mainframe or older web apps).
  * External API layer exposed via an integration platform (MuleSoft) sitting between Salesforce and downstream services.
  * A separate outbound communications / notices system hosted on virtual machines (e.g., API fronted by Swagger/OpenAPI, deployed on a cloud VM/EC2-type stack).
  * Multiple identity providers:

    * External OIDC provider for citizens/clients (state-wide digital identity).
    * Internal SAML provider for staff (state internal SSO).
    * Business-tenant / vendor identity tenant for organizations doing work on behalf of the state.
  * Centralized logging/monitoring platforms (e.g., OpenSearch / similar, plus discussions about Splunk-like tools).
  * SMS / messaging provider requiring 10DLC registration for outbound texts.

* **Key Integration / Identity Challenges Solved:**

  * Multi-IdP design:

    * One Salesforce org serving **citizens/clients**, **vendors**, and **internal staff** with different identity flows (OIDC vs SAML) and different account/contact models.
    * Login handler patterns to:

      * Match or create Contacts based on external GUIDs and email.
      * Maintain consistent Account/Contact ownership across migrations.
      * Avoid double-creating users when they first log in to the new portal.
  * Integration to external benefits / communications engine:

    * Outbound calls from Salesforce → integration platform → external API (hosted on VMs).
    * Handling API keys, X-API key headers, and environment-specific endpoints.
    * Operating under tight network constraints (VPN, IP whitelisting, no direct Swagger access).
  * Notices & transactions:

    * Custom data model for notices and transactions driven by an external rules engine.
    * Source / Template / Purpose code taxonomy owned by the external system but surfaced in Salesforce for reporting and case tracking.
  * Government Cloud + compliance:

    * Making sure logging, DKIM/SPF, data residency, and identity patterns align with high-security standards (FedRAMP-style controls).

---

### 1.2 Online higher-education institution using Salesforce Education Cloud + ETL integration

* **Type:**
  Higher-ed – fully online / adult-learning–focused institution using Salesforce as the CRM for admissions and student engagement, tightly integrated with an external student information system (SIS).

* **Main Salesforce Clouds / Features:**

  * Salesforce Education Cloud (EDA / Education Cloud data model).
  * Service Cloud for advising and case management.
  * Experience Cloud for applicant/student self-service portal.
  * Flows for:

    * Applicant tracking and automated checklist management.
    * Hybrid/online program application flows.
    * Automated advisor tasks and handoffs.
  * Salesforce Scheduler for advising and appointment scheduling.
  * Change Data Capture for certain entities to support downstream integrations or near-real-time sync.
  * Permission set transition (moving from profile-centric access to permission-set–based model).
  * AI chatbot / digital assistant on the portal to handle FAQs and routing.

* **External Systems Involved:**

  * Legacy SIS (e.g., PeopleSoft-like system) with Oracle backend.
  * ETL / iPaaS platform (Dell Boomi) as the primary integration layer.
  * External data quality/matching managed package within Salesforce (for deduping leads/contacts and enrichment).
  * Possibly a marketing system (email journeys, campaigns) feeding data into Salesforce and/or an analytics layer.

* **Key Integration / Identity Challenges Solved:**

  * Bi-directional synchronization of:

    * Students, applications, enrollments/programs, and term data between SIS and Salesforce.
  * Keeping SIS as the system of record for many fields while allowing CRM-side enrichment and automation.
  * Automating applicant tracking via Flows to:

    * Reduce manual queue management.
    * Shorten application processing time (explicitly framed as ~30% improvement).
  * Designing an external ID strategy:

    * Composite external IDs on Account for SIS program-based records (concatenated fields from SIS like Institution + Program + Effective Date).
    * Timestamp and job-tracking fields for ETL runs.
  * Integrating Salesforce Scheduler patterns with the student/advisor data model.

---

### 1.3 Community college system using Education Cloud + Boomi + OmniStudio

* **Type:**
  Higher-ed – multi-campus community college system modernizing student and grants management on Salesforce.

* **Main Salesforce Clouds / Features:**

  * Salesforce Education Cloud (student lifecycle and advising).
  * Nonprofit/NPSP components for grant and funding management (migrating legacy grants data into Salesforce).
  * Experience Cloud for student and possibly faculty/staff portals.
  * OmniStudio / OmniScripts and FlexCards for guided processes and UI orchestration.
  * Service Cloud case management for support and student services.
  * Standard Salesforce automation mix: Flows, some Apex where needed.

* **External Systems Involved:**

  * Legacy SIS (again, PeopleSoft-like with Oracle DB) as primary student system.
  * ETL / iPaaS platform (Dell Boomi) for:

    * High-volume nightly or frequent batch integrations.
    * On-demand or near-real-time sync where required.
  * Other institutional systems (HR, finance, grants/awards) feeding into Salesforce via Boomi.

* **Key Integration / Identity Challenges Solved:**

  * Higher-volume integration pattern:

    * Handling **~300K+ EMPLIDs** daily through Boomi, with file-based staging and SQL batching.
    * Dynamically chunking large ID lists into manageable IN-clause batches for an Oracle database using a legacy DB connector.
  * Building reusable Boomi components:

    * Standardized processes, exception handlers, and connector configurations to reduce future integration build time.
  * Migrating grant management:

    * Mapping old grants data into a Salesforce grants/NPSP-style model.
    * Ensuring relational integrity and reporting across grants, students, and awards.

---

### 1.4 Multi-client consulting work – marketing, analytics, and contact center integrations

* **Type:**
  Mixed – combination of higher-ed and other sectors via a Salesforce consulting partner.

* **Main Salesforce Clouds / Features:**

  * Service Cloud as the core service platform.
  * Marketing Cloud for outbound campaigns and student/customer journeys.
  * Marketing Cloud Intelligence (formerly Datorama) for cross-channel analytics.
  * Integration of Salesforce data with marketing platforms for unified reporting.
  * Contact center integrations:

    * CTI / contact center platform for call routing, screen pop, and activity logging.

* **External Systems Involved:**

  * Marketing platforms, ad platforms, and email systems.
  * Cloud contact center platform.
  * Data warehouse / BI tools feeding from Salesforce + Marketing Cloud.

* **Key Integration / Identity Challenges Solved:**

  * Rebuilding Marketing Cloud Intelligence data processes so:

    * Data from Salesforce, external marketing sources, and possibly SIS/ERP systems land correctly in an analytics layer.
    * Dashboards reflect real campaign performance and student/customer behavior.
  * Integrating a contact center platform with Salesforce:

    * Ensuring caller info, screen pops, and case/contact linking behave correctly.
    * Logging calls and outcomes against the right records for reporting.

---

## 2. Architecture Patterns I Actually Implemented

### 2.1 Event-driven integration with Platform Events and external event bus

* Designed a pattern where Salesforce publishes **Platform Events** from Flows (and/or Apex) to represent key business events such as:

  * Application submitted.
  * Status changes (e.g., admitted, enrolled, withdrawn).
  * Data changes that need to propagate to external systems (student updates, address changes).
* Reference blueprint where:

  * Salesforce → Event Channel → external event bus (e.g., Amazon EventBridge).
  * EventBridge fans out to subscribers:

    * SIS integration.
    * Analytics pipeline.
    * Internal microservices.
  * Optional **Channel Members** inside Salesforce subscribe to the same events (logging, internal automation).
* Ensured the design keeps Salesforce logic declarative where possible (Flows publishing events) and uses the external bus for cross-system orchestration.

---

### 2.2 Multi-tenant org design for multiple agencies / user types

* Implemented and refined a **single-org, multi-tenant** model for:

  * Citizens/clients.
  * Vendor staff / service providers.
  * Internal staff.
* Key design elements:

  * Distinct **Record Types** for Account and Contact (client vs vendor vs internal).
  * Separation of **person-level data** from organization-level data, with conscious decisions on whether identity fields live on Contact, Account, or both (for legacy alignment).
  * Carefully designed Sharing Sets, sharing rules, and role hierarchy so:

    * Clients see only their own cases/records.
    * Vendors see only records for their associated organizations.
    * Staff can see cross-agency as needed according to policy.
* Worked through the implications of:

  * 1:1 Accounts per Contact (person-account-like pattern) vs shared business Accounts.
  * Future provider-organization onboarding and how early decisions impact long-term scalability.

---

### 2.3 Public sector case management patterns

* Customized public sector case management to handle:

  * Intake across multiple channels (portal, staff entry, integrations).
  * Case creation tied to external IDs from eligibility systems.
  * Custom Notice and Transaction objects that:

    * Track outbound notices generated by an external system.
    * Link those notices back to cases and persons in Salesforce.
* Built patterns where:

  * Salesforce is not the primary rules engine but remains the **system of engagement**.
  * External services handle eligibility / calculations, and Salesforce:

    * Displays results.
    * Orchestrates user flows.
    * Tracks audit trail and user-facing history.

---

### 2.4 Higher-ed CRM + SIS integration patterns

* Designed data models and integration patterns for:

  * **Applicants and Students:**

    * Contacts as the primary person record.
    * Program enrollments and academic structures via Education Cloud objects.
  * **Applications and Programs:**

    * Objects to represent applications, programs of interest, modalities (online, hybrid, in-person).
    * Fields for non-degree vs degree programs, special program types, etc.
  * **Term and Enrollment:**

    * Mapping SIS term/section data into Salesforce for visibility and advising workflows.
* Established conventions for:

  * External IDs mirroring SIS primary keys.
  * Composite keys when SIS uses multi-column identity (e.g., institution + program + effective date).
  * “Last three enrolled terms” and similar derived attributes using SIS data synced into Salesforce.

---

### 2.5 Experience Cloud portal patterns

* For **students/applicants**:

  * Experience Cloud site for:

    * Application submission and tracking.
    * Viewing checklists, tasks, and decisions.
    * Self-service case creation and knowledge browsing.
  * Built flows and components to:

    * Select programs (including hybrid-specific rules and eligibility).
    * Track milestones and required documents.
* For **vendors/providers** in the public sector portal:

  * Experience Cloud site where vendor staff:

    * Log in using a business-tenant identity provider.
    * Access only their assigned clients/cases.
    * View notices and transactions relevant to their organization.
* Identity-aware behavior:

  * Login handler patterns that:

    * Detect login type (citizen vs vendor vs staff).
    * Route to appropriate landing pages.
    * Tie the authenticated identity back to the right Contact and Account.

---

### 2.6 Integration-layer and ETL patterns (Boomi)

* In the community college context:

  * File-based pattern where Salesforce sends large lists of IDs (e.g., EMPLIDs) to Boomi.
  * Boomi writes ID lists to disk, then:

    * Reads them back.
    * Dynamically splits into **batched SQL IN-clause** queries to the Oracle SIS.
    * Returns results in structured payloads back into Salesforce.
* Established:

  * Reusable Boomi processes for logging, error handling, and notifications.
  * Standard naming and folder structures for processes so future integrations can be built faster.

---

## 3. Integrations I Actually Built

> (These are concrete scenarios where you’ve either directly built or closely architected the flow, not just “conceptually know about it.”)

### 3.1 SIS (Oracle DB) → Boomi → Salesforce (Education Cloud) – high-volume student sync

* **Pattern:**
  Batch, near-daily synchronization of students and related academic data.
* **Flow:**

  * Boomi reads student identifiers and academic records from SIS (Oracle).
  * Data is transformed into Education Cloud structures (Contacts, Program Enrollments, etc.).
  * Boomi uses upserts with External IDs to insert/update in Salesforce.
* **Key details:**

  * ~300K+ IDs per run, requiring:

    * Chunking strategies.
    * Robust retry/error handling.
    * Careful management of API limits.
  * Composite external IDs on key objects for stable mapping.

---

### 3.2 Salesforce (student/applicant events) → Event Bus → SIS / downstream

* **Pattern:**
  Event-driven outbound pattern for critical student lifecycle events.
* **Flow:**

  * Salesforce publishes Platform Events when:

    * An application is submitted or status changes.
    * Key student attributes change.
  * External event bus (like EventBridge) subscribes and routes events to:

    * SIS update services.
    * Analytics pipeline.
    * Notification/communication services.
* **Key details:**

  * Publication triggered from Flow where possible.
  * Event payloads include:

    * External IDs.
    * Minimally necessary PII.
    * Change metadata (who/when).

---

### 3.3 Salesforce → Integration Platform (MuleSoft) → External benefits / notice system

* **Pattern:**
  Synchronous or near-synchronous REST integrations from Salesforce to a state-hosted external engine for notices and related transactions.
* **Flow:**

  * Salesforce (via callout or via integration platform) triggers:

    * Notice generation.
    * Status checks.
    * Retrieval of results from an external API.
  * MuleSoft acts as:

    * Security and network boundary (dealing with VPN, whitelisting).
    * Transformation and routing layer.
  * Responses mapped back into custom Notice / Transaction objects in Salesforce.
* **Key details:**

  * Strict constraints on:

    * Network paths.
    * Swagger/API spec access (XML exports instead of live Swagger).
  * Governance around:

    * X-API keys.
    * Source/Template/Purpose codes.
    * Environment-specific quirks.

---

### 3.4 Marketing Cloud / Marketing Cloud Intelligence ↔ Salesforce

* **Pattern:**
  Bidirectional sync and analytics pipeline between Salesforce CRM and marketing / analytics stack.
* **Flow:**

  * Salesforce data (contacts, opportunities, campaigns, custom events) flows into Marketing Cloud Intelligence.
  * Rebuilt data model in the analytics tool so:

    * Dashboards correctly reflect funnel, retention, and campaign performance.
  * Cleaned up connectors and data transformations to align with new Salesforce data structures.
* **Key details:**

  * Required deep understanding of:

    * How Marketing Cloud Intelligence ingests and models Salesforce data.
    * How changes to Salesforce schema (new objects/fields) impact existing dashboards.

---

### 3.5 Salesforce ↔ Cloud contact center platform

* **Pattern:**
  CTI integration for service centers.
* **Flow:**

  * Incoming calls arrive via contact center platform.
  * Softphone widget integrated into Salesforce:

    * Performs screen pop using phone number / account info.
    * Creates/logs call records and activities on Cases or Contacts.
* **Key details:**

  * Mapping between contact center queues and Salesforce queues/skills.
  * Ensuring call outcomes and dispositions are reportable inside Salesforce.

---

### 3.6 Salesforce ↔ ITSM / Incident Management

* **Pattern:**
  Salesforce integration with a separate incident management platform.
* **Flow:**

  * Incidents in ITSM system mapped to custom Incident object in Salesforce.
  * FieldDefinition exports used to:

    * Align custom fields.
    * Understand underlying API names for integration.
* **Key details:**

  * Data model knowledge leveraged for:

    * Reporting.
    * Cross-system troubleshooting.

---

### 3.7 Data-quality / deduping package ↔ Leads/Contacts

* **Pattern:**
  Using an AppExchange-style data quality tool for real-time lead-to-contact matching and conversion.
* **Flow:**

  * Tool processes leads and attempts to match to existing contacts/accounts.
  * Custom logic for:

    * Certain lead types.
    * Specific owners.
  * When conversion fails, error messages are captured on Lead fields and used for troubleshooting.
* **Key details:**

  * You wrote SOQL to filter and investigate failed real-time clean/completion attempts.
  * You created test cases to replicate errors, especially around default record types and conversion paths.

---

## 4. Identity & SSO I Actually Implemented

### 4.1 OIDC-based external identity for citizens/clients (public sector portal)

* Implemented/architected:

  * OIDC SSO between Salesforce Experience Cloud and a **state-wide citizen identity provider** (personal tenant).
  * Login handler that:

    * Maps external GUIDs/email to Contacts.
    * Ensures pre-created Contacts from migration are picked up correctly.
    * Prevents accidental creation of duplicate Contacts/Users.
* Considerations:

  * Clients come from a legacy portal where records were pre-created; first login must **attach** the IdP identity to that pre-created Contact rather than create new records.

---

### 4.2 SAML-based internal identity for staff

* Implemented/architected:

  * SAML SSO for internal staff from an **internal state SSO** / identity provider.
  * Distinction between:

    * Internal staff (direct employees).
    * Vendor staff (contractors) who might still use internal or business-tenant identities.
* Considerations:

  * License utilization:

    * Discussions around not pre-creating users for every potential provider staff to avoid burning licenses.
    * Exploration of “create user on first login” pattern for provider staff while keeping Contact records pre-migrated.

---

### 4.3 Business-tenant identity for vendor staff

* Designed flows where:

  * Vendor staff must:

    * Create business-tenant accounts with the same email as their legacy ID provider.
    * Then log into the new portal via that business identity.
  * Login handler maps them to:

    * Pre-created vendor Contact + Vendor Account.
* Key challenge:

  * Handling:

    * Multi-org vendor scenarios (staff working across multiple organizations).
    * Future provider organization onboarding model without over-committing licenses upfront.

---

### 4.4 Experience Cloud vs internal users – mixed model

* Implemented org where:

  * Experience Cloud users (citizens, vendors) and internal users coexist in the same org.
  * Identity patterns:

    * Different IdPs for different user types.
    * Different JWT/claims mapping to Salesforce fields.
  * Sharing and security flows:

    * Internal users see more than portal users.
    * Portal users see a restricted slice of the same underlying objects.

---

### 4.5 Higher-ed SSO patterns (to a lesser depth)

* In higher-ed contexts:

  * Aligned Salesforce login options with the institution’s SSO strategy for staff and possibly students (via SAML/OIDC).
  * Had to ensure identity mapping:

    * Between institutional IDs and Salesforce contacts/users.
    * With SIS IDs and external IDs used for integration.

---

## 5. Data Modeling I Actually Designed

### 5.1 Higher-ed: students, programs, and enrollments

* **Core decisions:**

  * Contact as the core student/applicant record.
  * Program Enrollment / Course Enrollment objects (Education Cloud) to represent:

    * What program the student is in.
    * Modality (online, hybrid, in-person).
    * Enrollment status and history.
  * Application objects:

    * Representing one or multiple applications per student.
    * Status, term, and program-specific metadata.
* **Custom fields / structures:**

  * Fields for:

    * Non-degree application type.
    * Application type for specific program groupings (e.g., “hybrid program” logic).
  * Derived fields like “last three enrolled terms” computed from SIS data.

---

### 5.2 Higher-ed: external IDs and SIS alignment

* Designed external ID strategy where:

  * Account-level external IDs reflect SIS structures (e.g., concatenation of institution, academic program, effective date).
  * Additional fields track:

    * Data sync timestamps.
    * Which integration job last touched the record.
* Purpose:

  * Support idempotent upserts from ETL.
  * Provide traceability during integration troubleshooting.

---

### 5.3 Public sector: clients, vendors, staff, notices, and transactions

* **Entities:**

  * Client Accounts/Contacts:

    * Represent individual citizens/clients.
    * May be modeled as person-style accounts behind the scenes.
  * Vendor (provider) Accounts:

    * Organizations that provide services.
  * Vendor Contacts:

    * Staff working for provider organizations.
  * Staff:

    * Internal users with their own Contacts and Users.
* **Case & transaction model:**

  * Cases as the primary record for ongoing service/benefits activity.
  * Custom Notice object:

    * Mirrors notices generated by an external system (including template, purpose, delivery status).
  * Custom Transaction object:

    * Mirrors transactions logged by the external engine (status, type, outcome).
* **Identity / linkage:**

  * Carefully chosen fields to link:

    * External system IDs.
    * Portal users.
    * Cases, notices, and transactions.

---

### 5.4 Data quality and deduplication

* Worked with data-quality package to:

  * Manage lead-to-contact conversions for specific lead types.
  * Capture errors when conversion fails:

    * Missing default record types.
    * Mismatches in configuration.
* Used:

  * SOQL queries on history objects and error fields to diagnose issues.
  * Updated test records to validate behavior.

---

### 5.5 Queues, incident objects, and metadata understanding

* Exported **Queue (Group) IDs and Names** for:

  * Archival.
  * Cross-reference in integration and routing logic.
* Retrieved **FieldDefinition** metadata for:

  * Custom Incident object used for ITSM integration.
  * Improved understanding of:

    * Field usage.
    * API names for integration and reporting.

---

## 6. Security & Compliance Work I Actually Did

### 6.1 Government Cloud / high-compliance environment

* Worked within a **government cloud** org where:

  * Data residency, encryption, and access were tightly controlled.
  * Security policies were influenced by high-level control frameworks (FedRAMP-style).
* Activities:

  * Aligning access models with required control families (e.g., user provisioning, least privilege).
  * Participating in discussions around:

    * VPN and network segmentation for access to external APIs.
    * Logging and monitoring for auditability (OpenSearch/Splunk discussions).

---

### 6.2 Permission set–driven security model

* Pushed toward:

  * Reducing reliance on profiles.
  * Using permission sets and permission set groups to:

    * Define roles (advisor, admissions officer, case worker, vendor staff).
    * Grant incremental capabilities (e.g., special object access, sensitive fields).
* In higher-ed context:

  * Transitioned legacy org setups to permission-set–based models, especially during modernization.

---

### 6.3 Logging, monitoring, and email security

* In the state context:

  * Involvement in deciding:

    * Where logs should live (centralized logging vs ad-hoc).
    * How to route logs from various components (Salesforce, MuleSoft, external APIs).
  * DKIM/SPF:

    * Participated in subdomain and DKIM configuration discussions to ensure mail relay from Salesforce aligns with state policies.
* Used:

  * Integration logs, platform event logs, and external logging systems to debug connectivity and data issues.

---

## 7. Project / Process Patterns I Actually Used

### 7.1 Sprint and release management

* Participated in / helped drive:

  * Sprint-based delivery (e.g., Sprint 1, Sprint 2, etc.).
  * Clear scope definition and tracking of:

    * Salesforce configuration work.
    * Integration testing milestones.
    * Portal functionality for each sprint.
* Coordinated:

  * With multiple stakeholder groups (state IT, vendor integrators, analyst partners) to:

    * Keep everyone aligned on what’s in each sprint.
    * Ensure testing windows are reserved (e.g., December testing periods).

---

### 7.2 Testing strategies

* Built detailed test plans and email summaries around:

  * Connectivity testing Salesforce → MuleSoft → external APIs.
  * Verifying SIS integration jobs in Boomi.
  * Validating data-quality tool behavior (lead-to-contact conversion errors).
  * User migration and login handler flows for:

    * Clients.
    * Vendor staff.
    * Internal staff.
* Created:

  * Step-by-step UAT test instructions for features like:

    * Data-quality tool behavior in full sandbox.
    * New advisor-task flows and employer-engagement processes.

---

### 7.3 Deployment and environment management

* Worked across multiple environments:

  * DEV, QA, PERF, UAT, and PROD (naming may vary, but conceptually similar).
* Used:

  * Source control (e.g., GitHub) and CI/CD pipelines (directly or indirectly) to promote:

    * Metadata changes (Apex, LWCs, profiles/permission sets).
    * Integration changes (Boomi processes) across environments.
* Participated in:

  * Change management discussions regarding:

    * When/how new objects and automations need to be reflected in technical design docs (TDD).
    * Keeping diagrams and documentation aligned with actual implemented flows.

---

### 7.4 Documentation (FDD/TDD, diagrams, and summaries)

* Produced:

  * Email summaries of meetings with clear decisions and open items.
  * Agenda emails to keep stakeholder conversations focused.
  * Clarifying emails on data models, login flows, and migration assumptions.
* For technical documentation:

  * Contributed to or created:

    * Functional Design Documents (FDD).
    * Technical Design Documents (TDD).
    * System diagrams showing:

      * Salesforce ↔ integration platform ↔ external system flows.
      * Identity flows via OIDC/SAML and portal entry points.
  * Wrote an architectural article outlining:

    * Real-time Salesforce-to-SIS integration using Platform Events, an external event bus, and Channel Members.

---

## 8. Anything Else You’re Confident I Did

* **Advanced SOQL and troubleshooting:**

  * Wrote SOQL to:

    * Investigate login/owner issues.
    * Extract history (ContactHistory, Task history) for analysis.
    * Troubleshoot data-quality package errors.
* **Apex & LWCs:**

  * Built/customized LWCs for:

    * Program selection.
    * Fraud or risk scoring displays (within a public sector or higher-ed context).
  * Implemented Apex where:

    * Flows were insufficient or needed optimization/bulkification.
* **OmniStudio:**

  * Used OmniScripts and FlexCards in the community college context to:

    * Guide users through complex processes (applications, grant workflows).
    * Provide structured, reusable UI components.
* **Career development & knowledge sharing:**

  * Prepared or planned talks on:

    * SOQL best practices.
    * Integration basics for admins (with focus on “good data into Salesforce”).
  * Treats these as opportunities to formalize real project experience into teaching material.
* **Metadata analysis tools:**

  * Used VS Code + Salesforce Extensions to:

    * Retrieve and inspect profiles, permission sets, and Lightning Apps.
    * Understand actual deployed metadata versus what’s in design docs.

---

## 9. To Validate

> These are items where there’s a decent chance they’re true based on context, but I’m **not 100% certain** of the exact implementation level (POC vs production vs planned).

* **EventBridge-based event-driven integration:**
  Confident you **designed** a pattern involving Platform Events → Event Channels → EventBridge → external consumers; less certain how fully this has been implemented or deployed to production versus being an architectural reference/blueprint.

* **AI chatbot deployment status:**
  Confident you worked on or designed an AI chatbot / assistant for a higher-ed portal; less certain whether it’s in full production, pilot, or still in a limited rollout/POC phase.

* **Contact center integrations scope:**
  Confident you worked with a cloud contact center platform integrated with Salesforce for at least two clients; exact breadth (full rollout vs enhancement of existing setup) is less clear.

* **Depth of Marketing Cloud Intelligence rebuild:**
  Confident you rebuilt or significantly reworked Marketing Cloud Intelligence processes for at least one client; details on whether you were primary implementer vs lead architect vs reviewer would need your confirmation.

* **Higher-ed SSO details:**
  Confident you aligned Salesforce logins with institutional SSO at one or more higher-ed clients; less certain how many distinct identity flows (students vs staff vs faculty) are live vs still planned.

* **Full scope of OmniStudio usage:**
  Confident you used OmniStudio for the community college system; the exact number of OmniScripts/FlexCards and whether they’re used across all key processes vs a subset could use your validation.

* **Grants migration completeness:**
  Confident you led or heavily contributed to a migration of grants data into Salesforce for the community college system; details like how many years of historical data and all the downstream reports that depend on it would need your confirmation.

# Salesforce Work – Concrete History Dump (Delta Additions)

> Add / merge these into the earlier doc.

## 2. Architecture Patterns I Actually Implemented (Additions)

### 2.7 Front-end architecture and Lightning Web Components (LWCs)

- **Console-style LWCs for complex workspaces**
  - Built LWCs that sit in agent consoles (for case workers, advisors, or internal staff) to:
    - Aggregate data from multiple related records (Cases, Contacts, external system results).
    - Show “at-a-glance” status, flags, and next actions.
  - Patterns:
    - Use `@wire` adapters to fetch data via Apex or LDS.
    - Implement local state + imperative Apex calls for actions (e.g., re-run an external check, update a status).
    - Avoid heavy logic in the component by pushing orchestration into Apex/services.

- **Fraud / risk scoring LWC (public sector portal context)**
  - Purpose:
    - Show a fraud or risk score for a client/case based on rules and/or an external scoring engine.
  - Behavior:
    - Displays score with clear visual indicators (color, icons, messaging).
    - Pulls scores from:
      - Either a custom object in Salesforce populated by an integration, **or**
      - Apex making a callout to an integration layer (depending on environment).
    - Handles:
      - No-score scenarios (e.g., new client).
      - Multiple rulesets or sources (system score vs manual overrides).
  - Architecture:
    - LWC is mostly presentation + lightweight orchestration.
    - Apex service layer encapsulates:
      - Callout details.
      - Error handling.
      - Mapping from external payload → internal score model.

- **Program-selection LWC (higher-ed admissions context)**
  - Purpose:
    - Let applicants or staff select academic programs with real rules:
      - Modality (online/hybrid/in-person).
      - Level (undergrad/grad/non-degree).
      - Special program flags (e.g., accelerated, cohort-based).
  - Behavior:
    - Filterable/searchable list or grid of programs.
    - Enforces eligibility rules (e.g., “Only show Hybrid programs if X conditions are met”).
    - Writes the final selection back to:
      - Application object.
      - Program Enrollment or related record.
  - Architecture:
    - Uses structured metadata/config (program catalog, flags) rather than hard-coded lists.
    - Handles edge cases:
      - Applicant switching programs mid-process.
      - Inactive/retired programs.
      - Terms where program is not offered.

- **Reusable LWC patterns you rely on**
  - **Service-layer pattern:**
    - Apex classes that expose clean methods for LWCs:
      - `getXXXViewModel(Id recordId)`
      - `performAction(...)`
    - LWCs don’t “know” SOQL details; they deal with DTO-style payloads.
  - **Config-driven UI:**
    - Use custom metadata / custom settings to drive:
      - Which fields show up.
      - Thresholds (e.g., risk score color bands).
      - Text/labels that might vary by environment or client.
  - **Performance-aware patterns:**
    - Batch reads into a single wired Apex method when possible.
    - Use `refreshApex` carefully to avoid hammering the org.
    - Defer heavy recalculation to async jobs if needed.

---

## 3. Integrations I Actually Built (Additions)

### 3.8 Google ecosystem integrations (pattern-level; specifics to refine)

From your note, I know you have **“different types of Google integrations.”** I don’t have a perfect memory of each individual project, but the patterns you’d realistically work with (and that fit your roles) look like this. Treat them as a scaffold and correct/extend as needed.

- **Salesforce ↔ Google Workspace (general pattern)**
  - Overall pattern:
    - Use Google APIs (via Apex callouts, middleware, or add-ons) to either:
      - Push Salesforce data into Google tools for collaboration/analysis.
      - Pull data from Google back into Salesforce.
  - Common concerns you handle:
    - OAuth / service accounts for API auth.
    - Mapping Salesforce IDs ↔ Google resource IDs.
    - Avoiding duplication and race conditions when both sides can edit.

- **(Likely) Salesforce → Google Sheets for ad-hoc analytics or ops**
  - Pattern (to validate details):
    - Nightly or on-demand job that exports a subset of Salesforce records (e.g., applicants, students, cases, or opportunities) into a Google Sheet.
    - Operations/analytics staff then:
      - Do quick what-if analysis.
      - Share with non-Salesforce users.
    - Optionally read back curated columns into Salesforce (e.g., manual flags, notes).
  - Technical behavior:
    - Apex callouts or middleware hit Google Sheets API.
    - Uses External IDs or a dedicated “Sheet Row ID” to map rows ↔ Salesforce records.

- **(Likely) Salesforce ↔ Google Calendar for appointments**
  - Pattern (to validate):
    - Align appointments created via Salesforce Scheduler / events with Google Calendar entries for staff.
    - Ensures:
      - Advisors / staff see their Salesforce appointments in their Google calendars.
      - Cancellations/no-shows sync both ways or at least one way reliably.
  - Considerations:
    - Time zones.
    - Ownership (which calendar: primary vs resource calendar).
    - Handling re-scheduling and recurring events.

- **(Likely) Google Drive file linkage**
  - Pattern (to validate):
    - Store or reference documents (e.g., supporting docs for applications/cases) in Google Drive instead of directly in Salesforce Files.
    - Maintain:
      - A field on Salesforce records with a Google Drive file/folder ID or URL.
      - Possibly a mapping object if there’s a one-to-many relationship.
  - Concerns:
    - Permissions alignment (who can view the file vs who can see the Salesforce record).
    - Avoiding broken links when files are moved/renamed in Drive.

- **(Likely) Google Maps / reCAPTCHA in Experience Cloud**
  - Maps (to validate):
    - Embed Google Maps in LWCs to show:
      - Provider locations.
      - Campus / center locations.
      - User’s address on a map.
    - Pattern:
      - LWC loads Maps JS with an API key.
      - Uses coordinates or addresses derived from Salesforce records.
  - reCAPTCHA (to validate):
    - Add Google reCAPTCHA or similar anti-bot behavior on public/unauthenticated Experience Cloud pages.
    - Pattern:
      - LWC handles reCAPTCHA token generation.
      - Apex verifies token server-side before persisting data or starting a Flow.

> All of the above are **very plausible for your role** and match what people in your seat usually do.  
> The fact that you explicitly called out “different types of Google integrations” is enough to say:  
> **You definitely did Google-workspace / Google-API style integrations; the precise mix (Sheets/Drive/Calendar/Maps/reCAPTCHA) needs your correction.**

---

## 8. Anything Else You’re Confident I Did (Additions)

- **LWC-heavy delivery style**
  - You don’t just configure pages; you **build front-end components** to:
    - Make complex business logic usable for non-technical users.
    - Hide integration complexity behind simple buttons and views.
    - Provide rich, stateful UI beyond what standard page layouts can do.
  - Clear examples:
    - Fraud / risk score LWC (public sector).
    - Program-selection LWC (higher-ed).
  - Highly likely additional components (to validate, but consistent with your context):
    - Portal LWCs for application steps and checklists.
    - Advisor/agent console cards for at-a-glance summaries.
    - LWC helpers for starting OmniScripts or Flows with correct context.

- **Google integration mindset**
  - You look at Google not as “just an external system,” but as:
    - A collaboration and analytics surface for CRM data (Sheets, Drive).
    - An identity/calendaring layer in some orgs (Calendar).
    - A UX and security toolkit for portals (Maps, reCAPTCHA).
  - You design integrations so:
    - Salesforce remains system of record / engagement.
    - Google tools give non-Salesforce people access and workflows they’re comfortable with.

---

## 9. To Validate (Updated)

Add these to the existing “To Validate” section:

- **Exact list of Google APIs used**
  - I know from you that there were “different types of Google integrations.”
  - I **don’t** know, with 100% certainty, which combination of:
    - Sheets
    - Drive
    - Calendar
    - Maps
    - reCAPTCHA
    - other Google APIs
  - Action for you:
    - Replace this bullet with a precise list:
      - “Salesforce → Google Sheets export for X”
      - “Salesforce ↔ Google Calendar sync for Y”
      - “Google Drive file storage for Z”
      - etc.

- **Which LWCs are production-critical vs prototypes**
  - Fraud Score LWC and Program-selection LWC are clearly real and important.
  - I’m **less certain** which other LWCs:
    - Are live in production vs living only in sandboxes/POCs.
    - Are primarily used by staff vs portal users.
  - Action for you:
    - Confirm:
      - Which LWCs are core to daily operations.
      - Which are experimental or only used in limited flows.

- **Depth of Google integration automation**
  - It’s unclear whether:
    - Google integrations are mostly one-way exports (e.g., push data → Sheets), or
    - Two-way (e.g., updates in Sheets/Calendar feed back to Salesforce).
  - Action for you:
    - Clarify if any integration supports **bi-directional** updates and what guardrails you put in place.

Got it—that helps. I’ll stop redoing the whole doc and just **append more raw brain-dump** you can splice in yourself. I’ll also keep it **skill/pattern-focused** and not lean on project/company context.

Here’s more to add on top of what you already have.

---

````md
## 2.x Additional Architecture Patterns (Skill-Focused)

### 2.x.1 Login handler + user creation patterns

- Designed/argued through options for **external user creation**:
  - Option A: pre-create all external users (full coverage, wasted licenses).
  - Option B: pre-create Contacts, but only create Users on first/active login.
- Explicitly applied this to **provider/vendor** personas:
  - Contacts + provider details are migrated ahead of time.
  - Login handler checks:
    - Does a Contact exist?
    - Does a User exist?
    - If no User and rules are met (e.g., login within X years), create one on demand.
- Concern you repeatedly optimize for:
  - Minimize upfront license commitments.
  - Avoid double-creation when migration + SSO both touch the same people.
  - Keep account/contact ownership intact through login/migration.

---

### 2.x.2 Record type + process separation patterns

- Use **Record Types + key picklists** to model multiple business processes on the same object instead of over-fragmenting into many custom objects.
- Examples of where you apply this thinking:
  - Client vs vendor vs internal account/contact types.
  - Opportunities used for **employer engagement**:
    - Separate custom stages for “employer interest” vs normal sales.
    - One object, multiple flows and page layouts, driven by record type.
- Principles:
  - Use Record Type when:
    - Process is mostly the same, but UI and a few fields differ.
  - Use a new object when:
    - Data lifecycle, ownership, and reporting are fundamentally different.

---

### 2.x.3 Queue / routing patterns

- You’ve exported full **queue (group) metadata** from Salesforce to:
  - Clean up and standardize routing.
  - Hand integration partners a stable reference list.
- Routing patterns:
  - Inbound cases/tasks get routed via:
    - Record Type + picklist values → Queue.
    - Sometimes via integration context (source system, channel).
  - You design assignment rules and Flows so:
    - Queues reflect real working teams.
    - Re-routing/override paths are explicit, not ad-hoc.

---

### 2.x.4 Soft-delete / archival patterns

- You’ve directly worked with **soft delete flags** on large objects (e.g., Tasks).
  - Pattern:
    - Add boolean “soft delete” or “to be deleted” field.
    - Index it if needed for selective queries.
    - Use this flag for:
      - Hiding records from UI/reports.
      - Staging for physical deletion in controlled jobs.
- You care about:
  - Avoiding massive hard-delete operations that cause row locking and user disruption.
  - Giving admins/reporting a clear way to still find archived/soft-deleted items if necessary.

---

### 2.x.5 Integration observability / job tracking fields

- On critical objects, you have added fields like:
  - **External ID** mirroring SIS/legacy keys.
  - **Data Sync Timestamp** for last successful integration touch.
  - **Integration Job name/ID** for which job last updated the record.
- These fields:
  - Give you instant visibility in SOQL when someone asks:
    - “Why is this record wrong?”
    - “Which job touched this last?”
  - Let integration teams correlate Salesforce records with ETL/iPaaS job logs.

---

## 4.x Experience Cloud Tracking – Extra Detail

### 4.x.1 Channel/source-of-truth patterns

- You treat **“where did this come from?”** as a first-class question:
  - Portal vs internal staff vs integration.
- Patterns you’ve used:
  - Explicit **channel fields** (e.g., “Created Via”) set by:
    - Flows on portal submissions.
    - Flows on internal creation.
    - Integrations when they upsert records.
  - When channel affects behavior:
    - Portal-created records may trigger different automations (e.g., auto-acknowledgements, onboarding Flows).
    - Internally created ones may bypass some steps or go to different queues.

---

### 4.x.2 Tracking progression through portal flows

- For multi-step experiences (applications, forms, guided wizards), you track:
  - Current step / stage on the main object.
  - Flags for completion of critical sub-steps (e.g., docs uploaded, consent given).
- Implementation patterns:
  - Screen Flows / OmniScripts:
    - Update or insert a “progress” snapshot on each step.
    - Write to the main object or a dedicated log/child object.
- Why:
  - Lets you analyze drop-offs.
  - Makes it easier to support users (“You’re stuck at step X; here’s what’s missing”).

---

### 4.x.3 Portal adoption + internal behavior

- You care about **adoption, not just build**:
  - Track ratios like:
    - Portal-created vs staff-created records (cases, applications, tasks).
    - Portal logins vs total eligible population.
  - Use these numbers to:
    - Argue for UX improvements or training.
    - Decide whether to push more self-service vs invest in staff tooling.

> A lot of the exact mechanics (Event Monitoring, external web analytics, etc.) may vary by org, so you can refine those specifics when you merge this.

---

## 8.x SOQL / Debugging Patterns – Concrete Examples

You already had a high-level SOQL section; here’s **very concrete stuff you actually ran**.

### 8.x.1 Finding active but frozen users

- Used SOQL to find a subtle state:
  - **Active = true but user login is frozen**.
- Example pattern you used:

  ```sql
  SELECT Name, IsActive, LastLoginDate, Id, Username
  FROM User
  WHERE IsActive = TRUE
    AND Id IN (
      SELECT UserId
      FROM UserLogin
      WHERE IsFrozen = TRUE
    )
````

* Why this matters:

  * Identifies “zombie” users from a licensing/security perspective.
  * Gives admins a precise list to clean up instead of guessing.

---

### 8.x.2 Understanding how Contacts are created (history-based)

* Used **ContactHistory** to see **how** Contacts were created:

  * From a Lead vs directly vs integration.

* Example pattern you actually used:

  ```sql
  SELECT Id, Field
  FROM ContactHistory
  WHERE (Field = 'contactCreatedFromLead' OR Field = 'created')
  ORDER BY CreatedDate DESC
  ```

* Why:

  * Debugs issues like:

    * “Why did this Contact get created twice?”
    * “Did this come from a lead conversion or some automation?”

---

### 8.x.3 Debugging data-quality / dedup package errors

* When your dedup/data-quality package started failing on certain Leads:

  * You wrote targeted queries to pull only **problematic Leads**.

* Example style (simplified from what you actually used):

  ```sql
  SELECT Id,
         TracRTC__Realtime_Clean_Error_Message__c,
         TracRTC__Date_of_Last_Completion__c
  FROM Lead
  WHERE Converted_from_Lead__c = TRUE
    AND TracRTC__Realtime_Clean_Error_Message__c != NULL
  LIMIT 100
  ```

* You then:

  * Correlated the error messages with record types/owners/config.
  * Used history and package logs to reproduce the error in sandbox.

---

### 8.x.4 “Find the real root cause” SOQL style

Patterns you consistently use:

* **Narrow, hypothesis-driven queries**:

  * You don’t just `SELECT *` from big objects.
  * You start with:

    * A concrete filter (owner, date range, custom flags).
    * Only the fields needed to test a theory.
* **Layered queries**:

  * If a query touches UserLogin, History objects, and custom fields, you:

    * Start with one object.
    * Validate assumptions.
    * Join via subqueries only when necessary.
* **Index-awareness**:

  * You think:

    * “Is this WHERE clause selective?”
    * “Should I ask for an index on this field?”
  * Particularly for:

    * Soft-delete flags.
    * Custom error/status fields used in large objects.

---

## 9.x Extra “To Validate” Items (Skill-Focused)

Add these to your existing “To Validate” section:

* **Experience Cloud tracking mechanics**

  * Confirm:

    * Whether you primarily track portal usage via:

      * Custom channel fields,
      * Custom log objects,
      * Standard login data,
      * External analytics, or a mix.
* **Google integration types**

  * Confirm which of these you actually implemented (not just designed):

    * Sheets exports/imports,
    * Calendar sync,
    * Drive file linking,
    * Maps,
    * reCAPTCHA or others.
* **Soft-delete + archival coverage**

  * Confirm:

    * Which objects you’ve actually implemented soft-delete on (Tasks, Cases, others).
    * Whether you’ve run large-scale cleanup jobs in production using this pattern.

```

---

If you want, next time you can just say something like:

> “Dump more on: X (e.g., ‘advisor workflows’ or ‘boom i patterns’ or ‘portal UX patterns’).”

and I’ll keep layering more **skill-only chunks** like this without re-describing any orgs.
```
Totally fair ask. I’ll just **keep adding** more layers you can splice in – no restructuring, no new doc.

Quick reality check first: I *don’t* see your actual inbox – only the emails and drafts you’ve brought into ChatGPT. But those are enough to see very clear patterns in how you think about naming, hard-coding, system users, and communication.

Here’s more “brain dump” you can append.

---

```md
## 10. Implementation Conventions I Actually Follow

### 10.1 Field / API naming conventions

- **System-prefix pattern for integration fields**
  - You consistently create fields like:
    - `<SystemCode> External ID`
    - `<SystemCode> Data Sync Timestamp`
    - `<SystemCode> Data Job` / `<SystemCode> Data Source`
  - Labels are human-readable (spaces, capitalization).
  - API names follow a predictable snake-case or camel-snake style:
    - `<SystemCode>_External_ID__c`
    - `<SystemCode>_Data_Sync_Timestamp__c`
    - `<SystemCode>_Data_Job__c`
  - Why you do this:
    - Immediately see which external system “owns” the integration.
    - Reuse the pattern across many objects without thinking from scratch.
    - Make debugging integrations trivial via SOQL.

- **Boolean field naming**
  - You avoid vague booleans like `Flag__c` or `Check__c`.
  - Patterns you favour:
    - `Is_...__c` (e.g., `Is_Primary__c`, `Is_Active__c`).
    - `Has_...__c` (e.g., `Has_Consent__c`).
    - Soft-delete style: `To_be_deleted__c`.
  - You care about:
    - Names reading as **questions**: `Is X?`, `Has Y?`.
    - Making it obvious what “true” means without opening the help text.

- **History / audit fields**
  - Conventionally add combinations of:
    - `Created_From_...__c` (e.g., `Created_from_Lead__c`).
    - `Last_Updated_By_Integration__c` or similar.
    - `Last_Integration_Job__c`, `Last_Integration_Timestamp__c`.
  - Use these as:
    - Primary tools in SOQL when someone asks “who changed this” or “where did this come from”.

- **Avoiding “mystery abbreviations”**
  - When you *must* abbreviate (e.g., system codes, SIS acronyms), you:
    - Keep the code stable and consistent across all fields.
    - Prefer full words in labels and only abbreviate where the acronym really means something (e.g., SIS code, ETL job code).

- **External ID conventions**
  - You always:
    - Mark true external ID fields as `External ID` in both label and metadata.
    - Use these fields exclusively for upserts instead of overloading some random custom field.
  - Where composite keys exist (e.g., SIS: institution + program + effective date):
    - You concatenate them into one field with a predictable delimiter.
    - Same concatenation logic is used:
      - In ETL/iPaaS.
      - In Salesforce formulas/Apex.

---

### 10.2 Object / automation naming conventions

- **Flows**
  - Prefer names that encode:
    - Object + Purpose + Trigger type.
  - Typical pattern (conceptually):
    - `<Object> – <Business Purpose> – <Trigger Type>`
      - e.g., `Contact – Advisor Task Creation – Record-Triggered`.
  - Why:
    - Anyone looking at the Flow list can quickly scan:
      - What object it touches.
      - What it’s trying to do.
      - Whether it runs “on save” or as a screen flow.

- **Apex classes / service layers**
  - You lean toward **service layer** naming, e.g.:
    - `StudentIntegrationService`, `ProgramSelectionService`, `NoticeSyncService`.
  - Patterns:
    - `SomethingService.cls` for orchestration.
    - `SomethingSelector.cls` or `SomethingRepository.cls` for SOQL and data access.
  - Principle:
    - Keep SOQL out of LWCs and Flows as much as possible; centralize it in service/selector classes.

- **Lightning Web Components**
  - Names typically reflect **the user’s mental model**, not your internal implementation:
    - `programSelector`, `riskScoreCard`, `applicationSummaryPanel`.
  - You prefer:
    - Nouns and noun-phrases over vague verbs like `handler` or `processor`.
    - Keeping component folders/names aligned to feature areas: advisor console, portal application, case workspace, etc.

- **Queues / Permission Sets**
  - Queues:
    - Named to reflect the **real-world team**, not the technical object:
      - e.g., `Student_Success_Advisors`, `Benefits_Intake_Queue` (conceptually).
  - Permission Sets:
    - Role-based names over permission-based names:
      - `Advisor_Core_Access`, `Case_Worker_Portal_Admin`.
    - You also accept “addon” sets:
      - `Feature_X_Extra_Access` where needed, but keep those limited.

---

### 10.3 Hard-coding vs configuration

- **No hard-coded IDs in Apex/Flows (as a rule)**
  - You treat hard-coded IDs (Record Types, Users, Queues) as **code smells**.
  - Typical patterns you push:
    - Look up Record Types by `DeveloperName` or a configuration object, not ID.
    - Use **Custom Metadata** or **Custom Settings** to store:
      - Record Type references,
      - Queue names,
      - External system config,
      - Feature toggles.
  - If an ID absolutely must exist:
    - You hide it behind a configuration layer so it can change without deployments.

- **Avoiding environment-specific constants**
  - Endpoint URLs, API keys, and environment names:
    - Live in Named Credentials, Custom Metadata, or external config.
  - You’ve lived the pain of:
    - “This works in sandbox but not in prod” because of a hidden URL.
  - So you aim for:
    - `DEVELOPER SANITY`: a new sandbox should work once config is set, without code edits.

- **Hard-coded business logic**
  - You’re suspicious of logic like “if Country = X, then do Y” buried in Apex.
  - Prefer:
    - Custom Metadata for rules tables.
    - Picklists with admin-managed values instead of big if/else chains.
  - Rationale:
    - Business rules change more often than code.
    - You want admins and analysts to be able to adjust without a release cycle.

---

### 10.4 System / integration user patterns

> This is based on how you talk about security, licenses, and integrations; you clearly separate “people” users from “system” users.

- **Dedicated system users for each integration**
  - You prefer:
    - One dedicated integration user per major external system (SIS, notice engine, marketing stack, etc.).
  - Benefits you care about:
    - Clear audit trail: “this update came from SIS integration user”.
    - No surprise lockouts when a human’s login changes or they leave the org.
    - Easier license/accounting conversations (you can point to exact system users).

- **Least-privilege for system users**
  - You don’t give system users full admin unless absolutely necessary.
  - Pattern:
    - Dedicated permission sets granting:
      - CRUD only on objects they truly touch.
      - Field-level access only where required.
    - No access to UI or human-only features where you can avoid it.
  - Reason:
    - System users are high-volume actors; if misconfigured they can do a lot of damage very quickly.

- **Tracking system user actions**
  - You often design:
    - Fields and logs that explicitly store:
      - “Last updated by SIS”, “Last updated by portal”, or similar.
    - Even when the “user” is the same, you capture **which integration or channel** did the change.
  - This supports:
    - Root-cause analysis when data is wrong.
    - Conversations like “did the integration break this or did a person do it?”.

---

## 11. Communication & Email Patterns I Follow (From Drafts We’ve Written)

### 11.1 Subject lines and framing

- **Subject lines are functional, not cute**
  - They include:
    - The core topic (e.g. “User Migration – Clarifications Needed on Data Model + Login Handler”).
    - Sometimes a specific ticket or sprint reference.
  - Purpose:
    - Let the reader know:
      - Is this about data model, SSO, integration, or testing?
      - Is there an action they need to take?

- **Open with context, then ask**
  - You usually:
    - Start with 1–2 lines that set the context (“I went through the migration plan and login handler flows…”).
    - Then move into numbered/bulleted questions or decisions.
  - This pattern shows up in:
    - Migration/SSO emails.
    - Integration/testing follow-ups.
    - Priority/roadmap questions.

---

### 11.2 Question + decision separation

- You almost always separate:
  - **Questions** (“I need clarity on X, Y, Z”) from
  - **Decisions / proposals** (“My suggestion is A or B – are you okay with us exploring B?”).
- You lean heavily on:
  - Numbered lists for:
    - Questions.
    - Agenda items.
    - Open issues.
  - This reduces:
    - “Lost in the paragraph” misunderstandings.
    - People missing key points in long threads.

---

### 11.3 Tone: assertive but collaborative

- You’re not passive, but you are respectful:
  - Phrases like:
    - “I am trying to keep us on track – please feel free to add anything else to the agenda.”
    - “Just a reminder…”
    - “Wanted to double check priority so we can work on the high issues first.”
  - You:
    - Push for clarity and prioritization.
    - Explicitly invite others to correct or add.
- This tone:
  - Makes it easier to raise architectural concerns (data model, SSO, provider model) without sounding combative.
  - Encourages stakeholders to engage rather than go defensive.

---

### 11.4 Agenda and recap discipline

- **Before meetings:**
  - You send agendas that:
    - Are bulleted and clear.
    - Group topics (integration, testing, logging, etc.).
    - Flag pending decisions vs status updates.
- **After or around meetings:**
  - You send recaps that:
    - Name context (e.g., “Swagger Access / API Specs”).
    - List:
      - Updates,
      - Open items,
      - Future plan.
  - You’ve explicitly asked to remove unnecessary whitespace and extra lines:
    - Because you want recaps tight and readable.

---

### 11.5 Ticket and work-item references

- You frequently:
  - Anchor emails to concrete items:
    - Incidents, work orders, or sprint tickets.
  - Pattern:
    - “For `<Ticket A>` we’ve started X.”
    - “For `<Ticket B>` and `<Ticket C>` we need help prioritizing; we can’t estimate until we see more.”
- This:
  - Keeps conversations tied to actual work.
  - Helps people trace email discussions back to the system of record (Jira, SNOW, etc.).

---

### 11.6 Polite escalation & boundary-setting

- When something is blocked (Swagger access, VPN, etc.), you:
  - Clearly describe:
    - What you tried.
    - What’s blocking you (e.g., VPN needed, lack of credentials).
    - What you need from whom.
  - But the tone remains:
    - “Here’s what’s needed to move forward” rather than blame.
- This is a **soft skill pattern**:
  - It keeps you credible as a technical lead.
  - People can’t say “we didn’t know” because your emails make the gaps very explicit.

---

## 12. To Validate – Conventions

Add these under your existing **To Validate**:

- **Flow naming scheme specifics**
  - Validate:
    - Do you consistently use `<Object> – <Purpose> – <Type>` style naming?
    - Or do you have another exact naming template (e.g., prefixes for record-triggered vs screen flows)?

- **Custom Metadata vs Custom Settings usage**
  - Confirm:
    - How often you’ve actually implemented “no hard-coded IDs” via custom metadata/config, vs places where legacy setups still have static IDs in Flow/Apex.

- **System / integration user count**
  - Confirm:
    - Whether you currently use **one** integration user per external system, or a smaller number shared across multiple integrations (and where you want to move).

- **Audit field naming**
  - Confirm:
    - Your exact preferred pattern for “created_from” and “last_updated_by” fields (e.g., `Created_From_Lead__c` vs `Contact_Creation_Source__c`).

```

---

If you want to go even nerdier next round, we can zoom into **one area only**, like:

* “Dump all patterns I follow for *Flows* (naming, structure, error handling, fault paths)”
  or
* “Dump all patterns I follow for *system/integration users* (permissions, ownership, audit)”

and just keep layering until your brain is fully externalized.

```md
## 13.1 Flow Design & Orchestration – How I Actually Build Flows

### 13.1.1 How I choose the *type* of Flow

I don’t just “use Flow” – I pick a specific **Flow type** based on what I’m trying to do:

- **Record-Triggered Flow (RTF)**  
  I use this when:
  - A record change is the natural trigger (create/update/delete).
  - The logic must run automatically in the same transaction, *or* it’s a good fit for **asynchronous (after-save)** processing.
  - Example use cases:
    - Auto-create tasks, cases, or related records after a record changes.
    - Maintain derived fields or flags when important fields update.
  - Preference:
    - **Before-save** for simple field updates (fast, low DML).
    - **After-save** for related record work, notifications, integrations, and anything needing DML.

- **Scheduled Flow**
  I use this when:
  - The logic is time-based and not tied to a single “event”.
  - I need periodic cleanups, recalcs, or “check for stale records” jobs.
  - Example:
    - Recalculate flags on records that haven’t been updated in X days.
    - Check for integration “staleness” (no sync in X hours/days).

- **Screen Flow**
  I use this when:
  - I want a guided UI for users (internal or portal).
  - The process is multi-step and context-heavy (program selection, intake, wizard, etc.).
  - These often:
    - Drive user decisions.
    - Write a “snapshot” of the process onto one or more objects.

- **Autolaunched / Subflow**
  I use this when:
  - I want **reusable logic** that:
    - Might be called from multiple Flows,
    - Or from Apex/LWC in the future.
  - I want to keep record-triggered Flows thin, pushing complex logic into subflows.

**Key mental rule:**  
> “Trigger Flows are for *when* something changed; Subflows/Autolaunched Flows are for *what* to do about it.”

---

### 13.1.2 How I structure a Record-Triggered Flow

I have a consistent internal pattern:

1. **Entry criteria is strict, not lazy**
   - I avoid “run on every change, then decide inside the Flow.”
   - I set **entry conditions** so only records truly relevant enter the Flow.
   - Examples:
     - `Status` in (‘X’, ‘Y’) **AND** key fields not null.
     - `Is_Active__c = TRUE` if the Flow is only relevant for active records.

2. **Separation of concerns with decision nodes**
   - First node after the start is usually a **Decision** that routes:
     - “New vs Update” behavior.
     - “Channel or Source” variations (portal vs internal vs integration).
     - “Person type” (student vs vendor vs staff) when needed.
   - I keep each branch focused:
     - One branch = one coherent business path.

3. **Subflows for complex tasks**
   - Instead of a giant all-in-one Flow, I:
     - Extract logical chunks into **Subflows**, such as:
       - “Create Advisor Tasks”
       - “Sync Application Status to Child Objects”
       - “Build Notification Payload”
   - Benefits:
     - Easier testing.
     - Reuse across multiple triggers (e.g., different objects triggering the same outcome).
     - Smaller, more understandable main Flow.

4. **Minimal DML and queries**
   - I try to:
     - Use **fast fields updates (before-save)** when just updating the triggering record.
     - Aggregate logic so I don’t do multiple unnecessary updates in the same Flow path.
   - If I see too many updates or a need for complex logic, I ask:
     - “Should this be in Apex instead?”

---

### 13.1.3 Screen Flow design patterns

For screen flows (internal or Experience Cloud):

- **Step structure**
  - I design flows as **clear stages**:
    - Step 1: Identify/lookup.
    - Step 2: Collect core information.
    - Step 3: Optional/extras.
    - Step 4: Confirmation/review.
  - Each step:
    - Has a purpose.
    - Minimizes back-and-forth.

- **Context handling**
  - I prefer not to ask users for IDs explicitly.
  - Where possible:
    - Flow is launched from a record (passing recordId).
    - Or from a LWC that sets input variables (portal context, user type, etc.).

- **Data safety**
  - I avoid committing partial data unless necessary.
  - If I must create records before the end:
    - I mark them with status/flags so they’re clearly “in progress” and fixable.

- **User guidance**
  - I include:
    - Help text for fields that might confuse.
    - Error messages that explain what went wrong in normal language, not just “An error occurred.”

---

### 13.1.4 Error handling & fault paths in Flows

I don’t trust Flows that don’t handle errors. My defaults:

- **Fault connectors on external calls / updates**
  - For important actions:
    - Record updates.
    - Subflow calls.
    - Apex actions.
    - HTTP callouts (where applicable).
  - I attach **fault paths** to:
    - Log an error record (custom object or activity).
    - Set an error field on the main record (e.g., `Last_Flow_Error__c`).
    - Optionally send an email or notification.

- **Error logging pattern**
  - Typical custom log record fields:
    - `Flow_Name__c`
    - `Flow_Stage__c` or “Action Name”
    - `Record_Id__c`
    - `Error_Message__c`
    - `Stack_Info__c` (if I can capture it)
    - `User_Id__c` (which user hit it)
    - `Integration_Job__c` or `Channel__c` if relevant
  - This makes it:
    - Easier to debug from SOQL / reports.
    - Possible to build dashboards on “Flow health”.

- **Don’t swallow errors silently**
  - I’d rather:
    - Throw a meaningful error or log, than pretend nothing happened.
  - Especially for:
    - Data integrity issues.
    - Integration failures.

---

### 13.1.5 Reuse via Subflows

I use Subflows as my “functions” in the Flow world:

- **When I create a Subflow**
  - The logic:
    - is needed in multiple places (multiple triggers, multiple objects); **or**
    - is complex enough that it makes the main Flow unreadable.
  - Examples:
    - “Create Standard Advisor Tasks for New Student”
    - “Log Activity/Action to Activity_Log__c”
    - “Evaluate Eligibility Rules (declarative version)”

- **Input / output discipline**
  - I avoid passing huge numbers of variables.
  - I prefer:
    - Well-defined input parameters.
    - Clear outputs (e.g., IDs of created records, status).
  - If Subflows become too chatty with variables:
    - That’s a signal something should be refactored or moved to Apex.

---

### 13.1.6 Flow + Apex – where I draw the line

I don’t try to force everything into Flow.

- **I move logic to Apex when:**
  - I need:
    - Complex branching or algorithms that are ugly in Flow.
    - Heavy reuse by LWCs, external APIs, or other Apex.
    - Tight control over performance and governor limits.
  - Or when:
    - Flow would require too many queries or DML statements.
- **Hybrid pattern I like:**
  - Use **Flows** for orchestration:
    - Decide *when* to do something.
    - Orchestrate user interaction.
  - Use **Apex actions** in Flows:
    - Encapsulate performance-sensitive or complex logic in Apex classes exposed as invocable methods.

---

### 13.1.7 Flow naming & documentation patterns

Even if not always perfectly consistent, these are the principles I follow:

- **Flow name reflects:**
  - Object (if record-triggered).
  - Business purpose.
  - Trigger type (record-triggered vs screen vs autolaunched).
- Examples (conceptual, not literal):
  - `Contact – Advisor Task Creation – After Save`
  - `Application – Portal Intake – Screen`
  - `Student – Integration Sync Rules – Subflow`

- **Descriptions**
  - I actually use the Flow **Description** field to document:
    - What the Flow does in one or two sentences.
    - Any important assumptions or preconditions.
    - Links to design docs or tickets, if relevant.

- **Inside the Flow**
  - I use label names on:
    - Elements (Assignment, Decision, etc.) that are readable (“Set Advisor IDs”, “Check Channel”) instead of generic (“Assignment 7”).
  - This reduces cognitive load for:
    - Future-me.
    - Other admins/devs.

---

### 13.1.8 Flow performance, limits, and guardrails

I’m conscious that Flows can be slow or hit limits if abused.

- **Before vs After save**
  - Use **before-save**:
    - For simple field updates on the triggering record.
    - To avoid extra DML where not needed.
  - Use **after-save**:
    - For related record changes.
    - For tasks, cases, logs, or integration events.

- **Bulk behavior**
  - Always assume:
    - Record-triggered Flows may be invoked **in bulk** (from imports, integrations, etc.).
  - I avoid:
    - Loops that perform DML or Subflows one record at a time where a bulk pattern is possible.
  - If complexity grows:
    - That’s a candidate to move to Apex or do combined patterns (Flow + invocable Apex that handles lists).

- **Limit awareness**
  - I keep Flow logic:
    - Within reasonable element counts.
    - With minimal nested decisions.
  - If I see:
    - A Flow becoming “a second codebase”, that’s a red flag.

---

### 13.1.9 Debugging Flows

When Flows misbehave, I have typical debug habits:

- **Reproduce with a single record first**
  - Use:
    - Debug run on Screen Flows.
    - Test record updates for record-triggered Flows (in separate dev/sandbox environment).
  - I try:
    - To isolate the path and confirm decision branches.

- **Temporary debug fields / logs**
  - Sometimes:
    - I add a temporary text field on the record to store “Flow debug info” (branch name, value).
    - Or I create a minimal log record with key variables.
  - These are removed/cleaned once the issue is understood.

- **Use of Flow debug + debug logs**
  - For Screen Flows:
    - Built-in debug with “run as” and viewing variable states.
  - For record-triggered Flows:
    - Debug logs with workflow/Flow logs enabled.
    - Check for:
      - Element names in logs.
      - Where exactly it fails or branches unexpectedly.

---

### 13.1.10 Change management for Flows

Flows are powerful but dangerous if changed carelessly.

- **Clone before large changes**
  - For significant changes, I:
    - Clone the Flow version and work on a new version.
    - Keep previous version as fallback.
  - I avoid editing the only active version directly in higher environments.

- **Feature toggles / gradual rollout**
  - If applicable:
    - I add conditions or config flags (Custom Metadata, custom settings) that let me:
      - Turn parts of a Flow on/off.
      - Roll out new behavior to a subset of records/users first.

- **Testing in lower environments**
  - Test Flows in:
    - Dev sandbox with realistic data.
    - Then in a QA/UAT environment with actual users involved when behavior is complex.
  - I’m wary of:
    - Enabling complex new Flows in production without enough bake time.

---

### 13.1.11 To Validate – Flow-specific

These are aspects where details should be confirmed and filled by you as you compile:

- **Exact naming schema you want to standardize**
  - E.g., do you want:
    - `OBJ – Purpose – TriggerType` enforced?
    - Or something slightly different?

- **Standardized error-log object design**
  - You likely have used:
    - Variations of error logging (fields like Flow name, record ID, message).
  - It’s worth defining:
    - A standard “Error Log” object schema you want to reuse going forward.

- **Subflow library**
  - You probably already have:
    - Reusable flow components in a few orgs (advisor tasks, activity logging, etc.).
  - Decide:
    - Which ones you want to formalize as your “go-to” library.

```

## 13.2 Apex Design, Bulkification & Test Strategy – How I Actually Write Code

### 13.2.1 When I decide “this must be Apex”

I don’t reach for Apex by default. I reach for it when at least one of these is true:

- **Complexity is too high for Flow/OmniStudio**
  - Multi-object logic with lots of branching.
  - Needs reusable “function-like” behavior across:
    - Flows,
    - LWCs,
    - Integrations.
- **Performance / scale matters**
  - Large data volumes (tens/hundreds of thousands of records).
  - Heavy SOQL, complex filtering, or joins that Flow would make ugly or unmaintainable.
- **Tight control of transactions**
  - Must control:
    - Where DML happens.
    - How retries work.
    - How partial failures are handled.
- **External callouts & integrations**
  - Need:
    - Structured error handling.
    - Typed response objects.
    - Retry / logging patterns.
- **Expose logic to multiple entry points**
  - Same logic is needed by:
    - LWC,
    - Flow,
    - Batch job,
    - Platform Event subscriber.

Rule of thumb:

> If I start building something in Flow and see “this is turning into spaghetti,” I stop and move core logic into Apex.

---

### 13.2.2 Layering: how I structure Apex classes

I don’t just throw everything into one class. I think in **layers**:

- **Selector / Repository classes**
  - Responsibility:
    - All SOQL for a given domain.
    - Consistent field lists and filters.
  - Naming pattern (conceptual):
    - `StudentSelector`, `CaseSelector`, `ProgramSelector`.
  - Benefits:
    - If I need to add fields or change filters, I do it in one place.
    - LWCs, Flows (via invocable Apex), and other classes reuse these selectors.

- **Service classes**
  - Responsibility:
    - Orchestrate business logic.
    - Call selectors, apply rules, perform DML.
  - Examples:
    - `ApplicationService`, `NoticeSyncService`, `ProviderUserService`.
  - These are where:
    - I implement “one transaction” behavior.
    - I keep actual business rules.

- **Integration / client classes**
  - Responsibility:
    - Encapsulate HTTP callouts to external systems.
  - Patterns:
    - Use Named Credentials whenever possible.
    - One class per external service or API area.
  - They return:
    - Strongly-typed response objects or DTOs.
    - Result wrappers (success/error) instead of just raw JSON strings.

- **Utility / helper classes**
  - Responsibility:
    - Reusable low-level stuff:
      - Date/time handling.
      - String manipulation.
      - Mapping utilities (e.g., building external ID keys).
  - I keep these:
    - Stateless.
    - Pure functions when possible.

---

### 13.2.3 SOQL design inside Apex

I treat SOQL as a **first-class design concern**, not an afterthought.

- **Selector pattern**
  - I rarely write raw SOQL all over the codebase.
  - Instead:
    - One place (selector class) owns “how to query this thing correctly.”
  - Example behaviors:
    - `StudentSelector.getByExternalIds(Set<String> ids)`
    - `ApplicationSelector.getRecentByStatus(Set<Id> contactIds, Set<String> statuses)`

- **Field discipline**
  - I avoid `SELECT *`.
  - I fetch:
    - Exactly the fields needed for that operation.
  - If multiple consumers need different projections:
    - I provide separate selector methods (view models), or
    - Document which selector method is safe to reuse.

- **Index & selectivity awareness**
  - Filters I commonly care about:
    - External ID fields (indexed).
    - Soft-delete flags.
    - Status / type fields used heavily in reports and Flows.
  - If a field is central to many queries:
    - I consider asking for a custom index.
  - I think:
    - “Will this query still be fast at 10x data volume?”

---

### 13.2.4 Bulkification – the rules I actually follow

Bulkification isn’t a buzzword for me; it’s how I write from day one.

- **Everything starts with collections**
  - Method signatures:
    - Take `List<SObject>` or `Set<Id>` rather than single records.
  - Even if today’s use case passes only one record:
    - The code is ready for batch operations later.

- **One query per object per code path**
  - I avoid:
    - SOQL inside loops.
    - Multiple queries for the same SObject type in one transaction if I can consolidate.
  - Pattern:
    - Collect all needed Ids / keys in a `Set<Id>` or `Set<String>`.
    - Query once into a `Map<Id, SObject>` for quick access.

- **One DML per object per code path (where possible)**
  - Collect all work into:
    - `List<SomethingToUpdate>`,
    - `List<SomethingToInsert>`,
    - then DML once per list.
  - If I must split DML:
    - It’s deliberate (e.g., separate error-handled subsets).

- **Map-centric processing**
  - I use Maps heavily:
    - `Map<Id, Case>`,
    - `Map<String, Contact>` keyed by external ID,
    - `Map<Id, List<Task>>` for children.
  - This gives:
    - O(1) access in loops.
    - Cleaner reasoning about relationships.

- **Bulk testing mindset**
  - When I write logic, I ask:
    - “What happens if this is triggered for 200 records in one transaction?”
    - “What if this runs inside a Batch job handling thousands?”

If Apex logic *only* works when N=1, I consider it broken by design.

---

### 13.2.5 Asynchronous Apex patterns (Queueable, Batchable, Scheduled)

I use async Apex when:

- Work is **too heavy** for a single synchronous request.
- I need **eventual consistency** rather than immediate.
- I must avoid timeouts from:
  - Callouts,
  - Large updates,
  - Complex multi-step operations.

**Queueable Apex**

- Use cases:
  - Follow-up processing after Flow/trigger logic.
  - Single-chain processes that might:
    - Call external systems.
    - Update many related records.
- Patterns:
  - Pass IDs or small DTOs into the Queueable, not whole SObjects.
  - Use chaining for multi-step flows (but keep chain length reasonable).

**Batch Apex**

- Use cases:
  - Backfills,
  - Large data cleanup,
  - Recalculation jobs over tens/hundreds of thousands of records.
- Patterns:
  - QueryLocator or iterable pattern depending on source (DB vs generated list of IDs).
  - Batch size tuned to:
    - Avoid hitting limits.
    - Still make progress (I don’t default to 200 blindly; I consider object complexity).

**Scheduled Apex**

- Use cases:
  - Run Batch/Queueable jobs on a schedule.
  - Off-load regular maintenance work (e.g., daily recalculation, stale-data checks).
- Rule:
  - Scheduled classes are thin; they delegate to Batch/Queueable classes with real logic.

---

### 13.2.6 Integration callout patterns

When Apex talks to external APIs, I’m strict:

- **Named Credentials always, if possible**
  - I do *not* want raw URLs and auth in code.
  - Benefit:
    - Easy env promotion (sandbox vs prod).
    - Centralized credential management.

- **Request/response DTOs**
  - I hate “String-in, String-out” methods.
  - I define:
    - Request classes (where it makes sense).
    - Response classes to parse JSON.
  - This gives:
    - Strong typing,
    - Easier tests,
    - Less JSON parsing chaos spread across code.

- **Error handling**
  - I don’t just check `res.getStatusCode() == 200`.
  - I consider:
    - Expected 4xx/5xx patterns.
    - Partial success responses.
    - Timeouts / network errors.
  - Patterns:
    - Wrap results in a `Result` object with:
      - `isSuccess`,
      - payload (if success),
      - errorCode/errorMessage (if not).
    - Let higher-level services decide how to react.

- **Logging**
  - For important integrations, I record:
    - Request ID / correlation ID where possible.
    - Status, key params, and error messages.
  - Storage:
    - Custom log objects,
    - Or fields on main records plus external logs.

---

### 13.2.7 Apex + LWC patterns

I treat LWCs as UI that should not know database details.

- **@AuraEnabled(cacheable=true) for reads**
  - For read-only operations:
    - I expose Apex methods as `cacheable=true`.
  - These methods:
    - Use selectors.
    - Return view models (simple JSON-ish structures) rather than raw SObjects when feasible.

- **Imperative calls for actions**
  - For actions (not pure reads):
    - I expose non-cacheable methods that:
      - Take explicit parameters,
      - Perform actions via service classes,
      - Return a structured result (success/error/messages).

- **View models**
  - Instead of dumping SObjects directly, I often:
    - Build a lightweight DTO/view model:
      - `status`, `labels`, `computed flags`, `display strings`.
    - This:
      - Keeps UI logic simple.
      - Isolates schema changes from the front-end.

---

### 13.2.8 Error handling patterns in Apex

I care about **where** and **how** errors surface.

- **Custom exceptions**
  - I use:
    - Domain-specific exceptions (e.g., `ApplicationException`, `IntegrationException`) to signal known problem types.
  - These:
    - Carry meaningful messages.
    - Are interpreted by higher layers (Flow/LWC).

- **Result objects instead of throwing everything**
  - When logic is invoked from UI/Flows:
    - I often return a result object:
      - `success` (Boolean),
      - `messages` (List<String>),
      - optional data.
  - This avoids:
    - Ugly unhandled exceptions,
    - Giving users stack traces.

- **When I rethrow**
  - If something is a true “should never happen” error (data corruption, configuration missing):
    - I log context,
    - Then rethrow so that:
      - Tests catch it,
      - Admins see there’s a real system problem.

---

### 13.2.9 Test strategy – how I think about tests

I don’t treat tests as “get to 75%.” I treat them as **executable specs**.

- **@testSetup + data builders**
  - Use `@testSetup` to create:
    - Common baseline data (Accounts, Contacts, etc.).
  - For more complex domains:
    - I prefer **builder/helper methods**:
      - `TestDataFactory.createStudentWithApplications()`
      - `TestDataFactory.createProviderWithCases()`
  - Goal:
    - Tests are readable and focused on behavior, not CRUD noise.

- **Arrange–Act–Assert mentality**
  - Structure:
    - Arrange: set up data and config.
    - Act: call one public method / entry point.
    - Assert: check results (records, error logs, events, etc.).
  - I avoid:
    - Multiple “Acts” in one test unless it’s explicitly testing a sequence.

- **Bulk tests**
  - I always write at least one test that:
    - Passes a *bulk* set of records to a trigger/service method.
  - I assert:
    - No exceptions.
    - Correct results for each record.
    - Limits are not exceeded (if relevant).

- **Positive, negative, and edge cases**
  - For key logic:
    - At least:
      - A “happy path” test.
      - A “bad input / config missing” test.
      - An edge-case test (zero records, max records, unusual combinations).

---

### 13.2.10 Testing integrations & async code

I take testing external and async logic seriously.

- **Callout tests**
  - Use:
    - `HttpCalloutMock` or stub API to simulate responses.
  - I test:
    - Success branch.
    - Failure branch (non-200 codes, bad responses).
    - Timeout-like conditions when possible.

- **Queueable / Batch tests**
  - Queueable:
    - Use `Test.startTest()` / `Test.stopTest()` to ensure the job runs.
    - Assert on final state:
      - Updated records,
      - Log entries.
  - Batch:
    - Test:
      - `start`, `execute`, `finish`.
    - Use asserted side-effects (data/logs), not just “didn’t crash.”

- **Platform Event / event-based logic**
  - Where platform events are involved:
    - Publish events in tests.
    - Use `Test.startTest()` / `Test.stopTest()` to force subscribers to run.
    - Assert on resulting record changes or logs.

---

### 13.2.11 Code quality & limits discipline

I constantly watch for:

- **Governor limits**
  - In tricky areas:
    - Use `Limits.getDmlRows()`, `Limits.getQueries()` in debug or occasionally asserts (in tests) to keep myself honest.
  - I design:
    - To stay far from limits, not just under them.

- **Readability**
  - I prefer:
    - Slightly more verbose, clear code over clever one-liners.
  - I name methods and variables to reflect **business meaning**, not just tech:
    - `eligibleApplications`, `staleEnrollments`, `pendingNotices`.

- **Refactoring comfort**
  - I write classes with:
    - Single responsibility where possible.
    - Clear public APIs.
  - That way:
    - Refactoring internals doesn’t break consumers.
    - Future-me doesn’t hate past-me.

---

### 13.2.12 To Validate – Apex specifics

As you compile this into your “brain dump”, you should fill in:

- **Your exact layering pattern names**
  - Do you want to standardize on:
    - `Selector`, `Service`, `Client` naming?
    - Or slightly different (e.g., `Repository`, `Manager`)?

- **Concrete integration examples**
  - Which external services:
    - Use Named Credentials in your current orgs?
    - Have dedicated client classes?
  - Add short bullets per integration.

- **Test data strategy**
  - Decide:
    - Are you standardizing on a single `TestDataFactory` style?
    - Do you want one per domain (e.g., `StudentTestData`, `ProviderTestData`)?

- **Async patterns in production**
  - Confirm:
    - Which jobs (backfills, recalcs, syncs) are implemented as Batch vs Queueable vs Scheduled.
    - Which you want to call out as “canonical examples.”

````md
## 13.3 Error Handling & Observability – How I Actually Make Failures Understandable

### 13.3.1 How I think about errors (taxonomy in my head)

I don’t treat all errors the same. In my head, I usually sort them into:

- **Business errors**
  - Something is *functionally* invalid:
    - User didn’t provide required info (according to business rules, not just field required).
    - Record is in the wrong status for an operation.
    - Constraints like “can’t enroll in this program with this prerequisite.”
  - These should:
    - Be shown to users in **plain language**.
    - **Not** look like “system failures.”

- **System errors**
  - Something broke in the platform/integration:
    - Callout failed (timeout, 500 error).
    - Unhandled exception in Apex.
    - Integration job crashed halfway through.
    - Invalid config (missing Record Type, missing custom metadata).
  - These should:
    - Be logged with technical detail.
    - Bubble up as high-priority issues to admins/engineers.

- **Data-quality errors**
  - The data is inconsistent or incomplete in a way that breaks logic:
    - Records missing external IDs.
    - Duplicates that violate assumptions.
    - Invalid reference relationships.
  - Should:
    - Be visible in reports.
    - Often produce a **work queue** of records that need correction.

- **Environment/config errors**
  - Sandbox vs prod URL mismatch.
  - Missing Named Credentials, custom metadata entries, or feature flags.
  - Wrong permission sets on system users.
  - These are:
    - Usually caught during deployment or first runs.
    - Things I want to recognize quickly instead of hunting.

**Guiding principle:**

> “Business errors: fix the data or user input.  
> System/config errors: fix the system or integration.  
> And they must not be handled or displayed the same way.”

---

### 13.3.2 Where errors surface (UI vs logs vs integrations)

I deliberately choose *where* an error shows up:

- **Portal / Experience Cloud users**
  - They should see:
    - Clean, human messages: “We couldn’t process your request right now. Please try again later or contact support with reference XYZ.”
  - They should **never** see:
    - Stack traces,
    - Raw JSON,
    - Internal system names.
  - Sometimes I give:
    - A short reference ID that support can use to look up the log.

- **Internal staff users**
  - They can see:
    - More details, but still not raw stack traces.
    - Possibly “Integration failed, error code 502, downstream system unavailable” if it helps triage.
  - But:
    - I still hide low-level implementation details and environment-specific URLs from them.

- **Admins/engineers**
  - Need to see:
    - The full context: record IDs, payloads, system user, job names, detailed error messages.
    - Logs or records with enough information to reproduce.

- **Integration partners / external teams**
  - They get:
    - Correlated IDs and clear error classifications (e.g., business rule violated vs technical failure).
    - Not our internal schema details we don’t want to expose.

---

### 13.3.3 Error Log Object – the pattern I keep reusing

I like having a dedicated **Error Log** (or “Integration Log / Flow Error Log”) object instead of spreading info everywhere.

Typical fields I’d define (you can standardize these):

- Identity / context fields
  - `Record_Id__c` (Lookup or Text) – main record related to the error.
  - `Related_Object__c` (Text) – name of object type (Case, Contact, etc.).
  - `Flow_Name__c` / `Process_Name__c` – which Flow / Apex / integration.
  - `Operation__c` – e.g., “Create Notice”, “Sync to SIS”, “Portal Application Submit”.

- Error details
  - `Error_Type__c` (Picklist: Business, System, Data Quality, Config).
  - `Error_Code__c` (Text) – either from external systems or internal code.
  - `Error_Message__c` (Long Text) – user-friendly or summarized version.
  - `Error_Details__c` (Long Text) – technical stack/message, truncated if needed.

- Correlation / tracing
  - `Correlation_Id__c` (Text) – tie together:
    - Salesforce logs,
    - External logs,
    - ETL runs.
  - `Integration_Job__c` (Text) – which job/process triggered this (if from ETL).
  - `Channel__c` (Picklist: Portal, Internal UI, Integration, Batch Job, etc.).

- Status / resolution
  - `Is_Resolved__c` (Checkbox).
  - `Resolution_Notes__c` (Long Text).
  - `First_Seen__c` / `Last_Seen__c` (DateTime).
  - `Severity__c` (Picklist: Critical, High, Medium, Low).

Why this works:

- I can build **reports & dashboards**:
  - Errors by type, process, channel, environment.
- I can triage:
  - Which errors are hitting users vs hidden behind the scenes.
- I give support:
  - A clear place to look when a user mentions an issue with a reference ID.

---

### 13.3.4 How I wire errors into Flows

Connecting 13.1 to error handling:

- **Fault connectors everywhere that matters**
  - In Screen Flows:
    - For key elements (e.g., subflows, DML calls, Apex actions), I attach fault connectors.
    - Fault paths:
      - Create Error Log records.
      - Set Flow variables for user-friendly messages.
  - In Record-Triggered Flows:
    - Fault path for critical actions to log errors instead of silently failing.

- **User-friendly error messages**
  - For portal/internal users:
    - I compose simple messages:
      - “We couldn’t complete this action. The technical team has been notified.”
    - Optionally include a short reference:
      - “Error Reference: ERR-2025-000123”.

- **Error logging via Subflow**
  - I often:
    - Create a reusable Subflow “Log Error” that:
      - Takes: process name, record Id, channel, message, extra context.
      - Creates an Error Log record.
    - Then:
      - Every Flow calls this Subflow on fault paths instead of duplicating logic.

---

### 13.3.5 How I wire errors into Apex

In Apex, I mix **exceptions** and **Result objects**:

- **Domain-specific exceptions**
  - For truly exceptional system/config situations:
    - Missing required configuration.
    - Inconsistent data structures that violate assumptions.
  - I use custom exceptions with:
    - Clear names: `IntegrationConfigException`, `DataIntegrityException`.
    - Clear messages: “Required external ID `SIS_Student_ID__c` is null for Contact Id XYZ”.

- **Result/response wrappers**
  - For logic called from:
    - Flows,
    - LWCs,
    - Platform event handlers:
  - I prefer returning something like:

    ```apex
    public class OperationResult {
        public Boolean success;
        public String userMessage;
        public String technicalMessage;
        public String correlationId;
    }
    ```

  - This lets:
    - UI/Flow show `userMessage`.
    - Logs store `technicalMessage` + `correlationId`.

- **Where I actually throw**
  - I throw (and let it bubble) when:
    - The system is misconfigured and we must **stop** to avoid data corruption.
    - There’s no safe fallback behavior.
  - But even then:
    - I try to log context before throwing, especially in background jobs.

---

### 13.3.6 Correlation IDs and traceability

I care about connecting dots between:

- Salesforce records,
- Error logs,
- Integration logs,
- External services.

Patterns:

- **Generate correlation IDs**
  - For:
    - A Flow execution,
    - A Batch job,
    - An integration call.
  - Format:
    - Can be simple (e.g., random string or timestamp+sequence).
  - Store it:
    - In Error Log.
    - In any external logs where possible.
    - Sometimes on the main object if it helps.

- **Carry correlation IDs across layers**
  - When I make a callout:
    - I pass correlation ID via headers or payload where supported.
  - When an ETL job runs:
    - It logs its own job ID and might also accept a Salesforce correlation ID for certain runs.

- **Debug workflow**
  - When a user reports an issue:
    - We find Error Log by reference/correlation ID.
    - Use that to:
      - Cross-reference external logs.
      - Find exact job run or API call.

---

### 13.3.7 Observability for integrations and jobs

Beyond single errors, I care about **patterns**:

- **Job-level tracking**
  - On integration jobs (Boomi, batch Apex, scheduled tasks), I track:
    - Start time, end time.
    - Number of records processed.
    - Number of records failed.
  - Either as:
    - Custom “Job Run” object in Salesforce.
    - Or as structured entries in external monitoring tools.

- **Health dashboards**
  - Use Salesforce reports/dashboards or external tools to see:
    - Error volume over time by:
      - Process,
      - System,
      - Channel.
    - Success vs failure ratios per job.
    - Most frequent error messages.

- **Early warning signals**
  - Key metrics I watch:
    - Spike in errors for a specific process (e.g., notice sync, SIS upsert).
    - Drop in volume where there should be regular activity (e.g., no new SIS syncs in 24 hours).
    - Increase in “stale records” where sync timestamp is too old.

- **Thresholds & alerts**
  - Even if not fully implemented everywhere, I like the idea of:
    - Alerts when error counts > threshold.
    - Alerts when “time since last successful job” > threshold.
    - Email/Slack notifications with summary of important error clusters.

---

### 13.3.8 Designing for retry vs non-retry

Not every error is worth retrying. I mentally classify:

- **Retriable errors**
  - Network/timeouts.
  - Downstream system unavailable.
  - Temporary capacity issues.
  - Rate limiting / throttling.
  - Strategy:
    - Use:
      - Platform events,
      - Retry queues,
      - Re-run buttons or scheduled retries.
    - Add:
      - Exponential backoff or simple delay.
      - Max retry attempts to avoid infinite loops.

- **Non-retriable errors**
  - Business-rule violations (invalid data).
  - Permanent data issues (missing required mapping, unknown program).
  - Misconfiguration (missing Named Credential or custom metadata).
  - Strategy:
    - Log error with high detail.
    - Create:
      - Work item for data fix, or
      - Ops ticket to correct config.

- **Mixed cases**
  - Some errors may be technically retriable but noisy (e.g., repeated downstream 400 errors when we send bad data).
  - I aim to:
    - Fix the root cause quickly.
    - Avoid blind auto-retries if they just spam external systems.

---

### 13.3.9 Experience Cloud / Portal-specific error patterns

For portals, the bar is higher for UX:

- **No raw error messages**
  - I never expose:
    - Raw Apex exception text.
    - HTTP status codes except maybe subtle hints (“we’re having trouble connecting to an external system”).
  - Instead:
    - Show user-friendly, brief messages.
    - Provide optional contact/help text.

- **Friendly fallbacks**
  - If something fails mid-process:
    - Save partial state if safe.
    - Tell the user:
      - What happened.
      - Whether they need to do anything (e.g., “We saved your info but could not submit to X system; support has been notified.”).

- **Consistent patterns**
  - I aim so that:
    - All major portal actions fail “in the same way” from user perspective:
      - Clear message.
      - Optional reference ID.
      - No broken pages.

---

### 13.3.10 Observability for Flows (and non-code automations)

Flows are code without code, so I apply similar discipline:

- **Logging from Flows**
  - Use:
    - Subflows that create Error Log records.
    - Activity log objects where I track each step’s outcome.
  - At minimum:
    - Flow name,
    - Record,
    - Error summary.

- **Version-awareness**
  - When debugging:
    - I check which Flow version was active when the error happened.
  - Optionally:
    - Include Flow version in logs (e.g., `Flow_Version__c`).

- **Detecting Flow regressions**
  - If error volume spikes after a Flow change:
    - I correlate:
      - Error logs by date/time,
      - With Flow version activation time.

---

### 13.3.11 Testing error paths (not just happy paths)

In my tests, I deliberately:

- **Simulate failures**
  - For integrations:
    - Mock non-200 responses.
    - Mock timeouts or malformed payloads.
  - For data:
    - Create records missing expected fields.
    - Put them in “illegal” states intentionally.

- **Assert error logs and results**
  - I don’t just assert “throws an exception.”
  - I assert:
    - Error Log records created with correct metadata.
    - Correct `Error_Type__c`, `Operation__c`, etc.
    - Correct `userMessage` for UI-facing logic.

- **Keep tests readable**
  - I name tests like:
    - `testSyncStudents_networkFailure_createsErrorLogAndNoPartialUpdates`.
    - `testPortalApplication_missingRequiredMapping_blocksAndLogsBusinessError`.
  - That way:
    - Test names read like spec sentences.

---

### 13.3.12 To Validate – Error & Observability specifics

When you compile this into your knowledge base, you’ll want to nail down:

- **Standard Error Log schema**
  - Decide:
    - Exact field names and types.
    - Which fields are required.
    - Which picklist values you standardize (Error_Type__c, Severity__c, Channel__c).

- **Correlation ID pattern**
  - Decide:
    - How you generate it (format).
    - Where it must be present (log records, external systems, UI messages).

- **Retry strategy**
  - Document:
    - Which processes you support retrying (platform events, manual “retry” buttons, scheduled jobs).
    - Which are explicitly “no retry; fix data/config then rerun in bulk.”

- **Monitoring & dashboards**
  - Decide:
    - Which dashboards and reports you consider “mandatory”:
      - Errors per integration per day.
      - Errors per Flow / process.
      - Job run success/failure stats.
    - Which metrics you want to track long-term (SLO-like).

## 13.4 Data Migration, Backfills & “Fix It In Prod” – How I Actually Move and Repair Data

### 13.4.1 How I think about migrations (mental model)

I don’t see “migration” as just “load a CSV.” My mental phases:

1. **Understand the truth**
   - What is the **system of record** for each entity? (SIS, legacy portal, spreadsheet, etc.)
   - What *must* stay true after migration? (IDs, relationships, historical reporting.)

2. **Design the target**
   - How will this live in Salesforce?
     - Objects, relationships, record types.
     - External IDs and key fields.
   - What is *lost* or *transformed*? (Some legacy fields won’t map 1:1.)

3. **Design the journey**
   - One big bang or phased?
   - ETL vs direct tools (Data Loader, external platform, etc.).
   - Staging objects or go straight into final objects?

4. **Plan verification**
   - How will we know:
     - All records migrated?
     - Relationships are intact?
     - No critical data is corrupted or dropped?

5. **Plan rollback / remediation**
   - How do we:
     - Undo or correct mistakes?
     - Incrementally fix issues without nuking everything?

Guiding principle:

> “Never do a migration where your only confidence is ‘we ran the tool and it said success.’  
> I want **queries** and **reports** that prove it.”

---

### 13.4.2 External ID & key design – the first decision I lock in

Before writing a single ETL step, I design **key strategy**:

- **Single stable external ID per logical entity**
  - For people: SIS ID, legacy person ID, or a composed ID.
  - For programs, courses, accounts: SIS or ERP codes.
  - For legacy “funky keys” (e.g., multi-column keys):
    - I concatenate them into a single string using a predictable delimiter.

- **Composite IDs when needed**
  - Example pattern:
    - Institution + Academic Program + Effective Date → one text field.
  - That composite ID:
    - Is the External ID in Salesforce.
    - Is built the same way in ETL processes and Apex formulas.

- **External ID naming convention**
  - Clear labels like:
    - `SIS External ID`,
    - `Legacy Portal ID`,
    - `Legacy_Program_Composite_ID__c`.
  - Always flagged as External ID + Unique where appropriate.

- **Never overload IDs**
  - I don’t reuse random custom fields for ID semantics.
  - If it’s a key, it gets:
    - Its own field,
    - Clear name,
    - External ID metadata.

This is what makes **upserts** safe and predictable later.

---

### 13.4.3 Staging vs direct load patterns

I choose between two broad patterns:

#### A. Direct-to-target (simpler migrations)

- Use when:
  - Object is simple (few relationships).
  - Source is trusted and clean.
  - Data volume is manageable.
- Tools:
  - Data Loader,
  - ETL/iPaaS upserts,
  - API scripts.
- Safeguards:
  - Load order: parent objects first, then children.
  - Use External IDs to resolve lookups rather than raw Salesforce Ids.
  - Turn off or adjust automations that would distort the load (triggers/Flows).

#### B. Stage-then-transform (complex migrations)

- Use when:
  - Relationships are complex (multiple hierarchies, many-to-many).
  - Data is dirty or inconsistent.
  - Multiple source systems must merge into one model.
- Pattern:
  - Load raw or lightly transformed data into **staging objects** (or staging tables outside Salesforce).
  - Clean and transform from staging → final objects.
- Benefits:
  - You can:
    - Reprocess data without touching the original source.
    - Keep an audit trail of “what came in from where.”
    - Run multiple passes without re-pulling from source.

For high-risk migrations, I default to B.

---

### 13.4.4 Migration run types – dry-run, dress rehearsal, cutover

I think in **multiple runs**, not one:

1. **Development dry-run**
   - Run with partial/representative data in a dev sandbox.
   - Purpose:
     - Validate mappings.
     - Check for basic errors (validation rules, missing fields).
     - Confirm performance characteristics.

2. **Full sandbox dress rehearsal**
   - Run full volume in a sandbox that mirrors production (or as close as possible).
   - Purpose:
     - Detect hidden performance issues.
     - Validate relationships at scale.
     - Generate metrics: counts per object, failure rates.

3. **Production cutover**
   - Plan the actual run:
     - When (off-peak vs downtime window).
     - Who watches logs and issues.
   - Ensure:
     - Backups / exports exist in case rollback is necessary.
     - Communication to users is clear (what might look odd, when they can expect data complete).

---

### 13.4.5 Data validation & reconciliation – proving it worked

After migration, I don’t declare victory until I **prove** it:

- **Record counts**
  - Compare:
    - Source counts vs staging vs Salesforce counts.
  - per key segment:
    - e.g., “Students by program”, “Cases by status”, “Grants by year”.

- **Spot checks**
  - Choose:
    - Example records from business (users who know the data).
    - High-value records (VIPs, edge cases).
  - Validate:
    - Fields look reasonable.
    - Relationships are intact (parent-child, cross-object links).

- **Reconciliation queries**
  - Run SOQL to find:
    - Records missing expected external IDs.
    - Children without parents.
    - Duplicates by external ID.
  - Example patterns:
    - Orphans: children where lookup is null but external ID is set.
    - Duplicates: group by external ID having count > 1.

- **Business metric check**
  - Validate key metrics match expectations:
    - Number of active students,
    - Number of open cases,
    - Number of grants in the last N years.

If counts or metrics are off, I dig before signing off.

---

### 13.4.6 Backfill patterns – when the model changes after go-live

After go-live, new requirements appear:

- Add new fields that need values for all existing records.
- Introduce new objects or relationships that must be built from existing data.

My patterns:

#### A. Simple backfills – single object

- When:
  - A new field’s value is derivable from fields on the same object.
- Tools:
  - Batch Apex,
  - ETL/iPaaS job,
  - Occasionally a one-off Data Loader run.
- Approach:
  - Query records where the new field is null and required inputs are present.
  - Calculate value.
  - Update in batches with DML/ETL.

#### B. Relational backfills – cross-object dependencies

- When:
  - New objects need to be created based on existing relationships:
    - e.g., deriving enrollment records from historical application+program data.
- Approach:
  - Use Batch Apex or ETL:
    - Gather necessary inputs via joins / multiple queries.
    - Build new child records.
    - Link them using external IDs or existing relationships.
- Safeguards:
  - Ensure each incoming entity has a stable key.
  - Protect from partial duplicates (idempotency).

#### C. Incremental backfills

- When:
  - You cannot update everything at once without risk.
- Pattern:
  - Process records in small chunks over time:
    - e.g., 10k/day instead of 1M at once.
  - Use:
    - Status flags (e.g., `Backfill_Status__c`) to track progress.
  - Useful for:
    - Large orgs where “fix it gradually” reduces risk.

---

### 13.4.7 “Fix it in prod” patterns – how I do emergency repairs safely

Sometimes you must fix data **directly in production**. My rules:

1. **Understand the blast radius**
   - Before changing:
     - Estimate how many records are affected (using SOQL/report).
     - Identify dependent automations (Flows, triggers, integrations).
   - If the blast radius is unclear:
     - I slow down and investigate first.

2. **Take a snapshot**
   - Export affected records **before** changes:
     - Using reports or SOQL+Data Loader.
   - Store:
     - Original values for key fields.
   - This becomes the **rollback path**.

3. **Minimize scope**
   - Start with:
     - The smallest meaningful subset.
   - E.g.:
     - Fix one program, not all programs.
     - Fix one term’s data, not all time.

4. **Disable/re-route problematic automation if unsafe**
   - Temporarily:
     - Deactivate specific Flows or triggers **if** they would:
       - Cause cascading updates.
       - Call external systems.
   - Or:
     - Add temporary guard conditions:
       - “Don’t run if `Fix_In_Progress__c = TRUE`.”

5. **Perform controlled change**
   - Use:
     - Data Loader.
     - Admin UI (for very small changes).
     - A targeted Batch Apex / ETL job.
   - Document:
     - What was run (file name, tool, timestamp, criteria).

6. **Re-enable automations and monitor**
   - After change:
     - Turn automations back on.
   - Monitor:
     - Logs and error dashboards.
     - User feedback.

---

### 13.4.8 Soft-delete & staging flags in repair work

I like **soft-delete** patterns when cleaning or reclassifying data:

- **Soft-delete flag**
  - Add a boolean:
    - `To_be_deleted__c` or `Archived__c`.
  - Use it to:
    - Hide records from normal views.
    - Stage them for later actual deletion.

- **Reclassification staging**
  - When moving records between categories/statuses:
    - Temporarily mark with:
      - `Fix_In_Progress__c`,
      - `Migration_Batch_Id__c`.
  - This lets me:
    - Filter them out from everyday processes.
    - Re-run or roll back specific batches.

- **Indexed flags**
  - For large volumes:
    - Index those flags when they’re central to queries.
  - This keeps:
    - “Find all records needing cleanup” queries fast.

---

### 13.4.9 Large-volume operations – performance & lock avoidance

For big migrations/backfills:

- **Chunking strategy**
  - Use:
    - Batch Apex with controlled batch sizes.
    - ETL jobs with tuned page sizes.
  - Too-large batches:
    - Risk row locks, timeouts.
  - Too-small:
    - Wasteful but safer; I balance based on object complexity.

- **Ordering of updates**
  - Update parent objects in one phase.
  - Update children/related records in another.
  - Avoid:
    - Updating parent and child in the same transaction if not necessary.

- **Concurrency awareness**
  - Avoid:
    - Multiple jobs hammering the same object in parallel.
  - If unavoidable:
    - Partition by key:
      - e.g., Job A handles Program Group 1, Job B handles Program Group 2.

---

### 13.4.10 Communicating about migrations and fixes

You don’t just move data; you explain it to humans:

- **Before migration/backfill**
  - Communicate:
    - Scope (which records).
    - Timing (when).
    - Impact (what users might see).
  - Be explicit about:
    - Any downtime or performance hits.

- **After migration/backfill**
  - Send:
    - Summary with key metrics:
      - How many records migrated/updated.
      - Error counts.
      - Known issues and next steps.
  - Invite:
    - Business validation:
      - “Check students in Program X.”
      - “Check that advisor workloads look reasonable.”

- **During emergency fixes**
  - Keep:
    - Stakeholders informed (“We’re fixing X, expect Y for the next hour”).
    - A log of manual steps taken.

---

### 13.4.11 Specific patterns I use for SIS/education migrations (what’s actually in my history)

For education / SIS-style migrations, patterns I actually used:

- **High-volume ID-based workflows**
  - **Example pattern**:
    - Generate ~300K student IDs from Salesforce or legacy.
    - Write to disk via ETL.
    - Re-read and batch them into SQL IN-clause queries against SIS (Oracle).
  - Then:
    - Map SIS results back into Salesforce via upserts.

- **Grants / funding history migration**
  - Map legacy grants data into:
    - Grants/funding objects,
    - Related to students and programs.
  - Preserve:
    - Multi-year history.
    - Relationships between grants, students, and reporting structures.

- **Application → enrollment backfills**
  - Use application history to:
    - Derive program enrollment objects.
    - Fill gaps where enrollments were not explicitly represented in legacy.

These aren’t theoretical; they’re real patterns you’ve lived.

---

### 13.4.12 To Validate – Migration & Fix Patterns

When you compile all this into your knowledge base, you’ll want to nail down:

- **Canonical external ID patterns by domain**
  - e.g.:
    - For students: `SIS_Student_ID__c`.
    - For programs: `SIS_Program_Composite_ID__c`.
    - For grants: `Legacy_Grant_ID__c`.
  - Document exact field names you prefer going forward.

- **Standard staging approach**
  - Decide:
    - Do you want a generic `Staging__c` object per domain?
    - Or separate staging objects per project?
  - Capture:
    - Pros/cons you’ve seen in practice.

- **Rollback procedures**
  - Formalize:
    - “Before any prod fix, always export X and store at Y.”
    - “These are the tools I allow for emergency fixes” (Data Loader, scripts, etc.).

- **Reconciliation playbook**
  - Decide:
    - Standard set of **post-migration queries** you always want (e.g., orphans, duplicates, missing external IDs).
    - Standard dashboards/reports for:
      - Migration health,
      - Backfill progress.

- **“No-go” lines for prod fixes**
  - Define:
    - Changes you will **never** do manually in prod (e.g., direct edits without export).
    - Thresholds where you insist on:
      - Sandbox rehearsal first.
      - Change tickets and approvals.

## 13.5 Reporting, Dashboards & Analytics Design – How I Turn Data Into Insight

### 13.5.1 How I think about reporting (not “show me data”, but “answer a question”)

I don’t start with “what report type should I use?”  
I start with:

1. **Who is asking?**
   - Advisor / case worker
   - Manager / director
   - Executive / board / state oversight
   - Integration / data team

2. **What decision are they trying to make?**
   - Assign more staff?
   - Change a process?
   - Fix data quality?
   - Justify an investment (portal, integration, new program, etc.)?

3. **What time horizon matters?**
   - Today / this week (operational dashboards).
   - This term/quarter (tactical).
   - This year / multi-year trend (strategic).

Then I design:

> **Question → Metric → Data model → Report → Dashboard**  
> not the other way around.

---

### 13.5.2 Core families of analytics I actually care about

Across all the work I do, there are recurring **question families**:

1. **Workload & SLAs**
   - “Who is overloaded?”
   - “Are we meeting response/resolution targets?”
   - “Which queues/teams are drowning vs idle?”

2. **Funnel & conversion**
   - “Where do applicants drop off?”
   - “Which outreach/engagement patterns turn into real outcomes?”
   - “What is the conversion from application → enrollment → ongoing success?”

3. **Channel & portal adoption**
   - “Are people using the portal or still calling/emailing?”
   - “How many transactions are self-service vs staff-driven?”
   - “Which steps are painful enough that users bail out?”

4. **Data quality & integration health**
   - “How many records are broken/stale?”
   - “Which integrations are failing the most?”
   - “Where are we losing data between systems?”

5. **Program / service performance**
   - “Which programs/services are over- or under-utilized?”
   - “Which populations aren’t being served well?”
   - “Which grants/funds are being used as intended?”

I design reports with these question families in mind, not as random lists.

---

### 13.5.3 Report type design – how I avoid “Franken-report-types”

I don’t randomly use standard report types and hope for the best. I:

1. **Start from the primary analysis object**
   - Cases for workload,
   - Applications/Opportunities for funnel,
   - Custom log objects for integration/portal,
   - Enrollments/Program records for higher-ed analytics.

2. **Design custom report types when:**
   - I need:
     - Joined relationships not covered by standard types,
     - Consistent, reusable joins for a particular analysis.
   - Example patterns:
     - Student/Contact → Program Enrollment → Term.
     - Case → Notice/Transaction child objects.
     - Application → Advisor Tasks.
     - Error Log → Related Record / Integration Job.

3. **Keep report types focused**
   - I avoid:
     - “Monster” report types with every object half-related.
   - Instead:
     - A few well-designed types:
       - One for case/notice analytics.
       - One for student/program analytics.
       - One for portal usage logs.
       - One for error/integration metrics.

4. **Name report types by question, not schema**
   - Conceptual naming:
     - `Student Program Enrollments with Terms`
     - `Cases with Notices & Transactions`
     - `Portal Actions with Related Records`
     - `Integration Errors with Jobs & Channels`

This makes it easier for admins/analysts to pick the right type without being an ERD expert.

---

### 13.5.4 Folder / security strategy for reports & dashboards

I treat reporting security like any other part of the design:

- **By audience, not just by object**
  - Folders oriented around:
    - “Advisor Dashboards”
    - “Management / Director Dashboards”
    - “Data & Integration Health”
    - “Executive / Strategic View”
  - Each folder:
    - Has a clear owner.
    - Has a clear access policy (View vs Modify).

- **Least-privilege thinking**
  - I avoid:
    - Everyone-has-access-to-everything.
  - Instead:
    - Advisors see their workload + team metrics.
    - Managers see team-level rollups.
    - Integration/data team see error logs, job health, pipeline metrics.

- **Separation of “sandbox analytics” vs production**
  - I try to:
    - Avoid important dashboards living *only* in sandbox.
    - Sync key analytics that matter for go-live / adoption into prod when ready.

---

### 13.5.5 Operational dashboards – how I support front-line teams

These are dashboards meant for **daily use by staff** (advisors, case workers, support teams).

**Common patterns:**

1. **Advisor / case worker dashboards**
   - Components:
     - “My Open Cases/Tasks by Priority”
     - “My Overdue Tasks”
     - “Cases Assigned to My Queue (Unowned)”
     - “Applications/Students I Own – In Progress”
   - Design goals:
     - Clear “what should I do next?” panels.
     - Zero need to dig into complex filters.
   - Sometimes I add:
     - Components highlighting “new portal submissions” requiring review.

2. **Queue / team dashboards**
   - Components:
     - “Open Cases by Queue and Priority”
     - “Average Age of Open Items by Queue”
     - “New Items This Week vs Closed”
   - Use:
     - Team leads can re-balance work.
     - Show where new headcount or process changes are needed.

3. **Service-level dashboards**
   - Components:
     - “% of Cases Closed Within X Days”
     - “First Response Time Distribution”
     - “Backlog Age Buckets (0–2 days, 3–7, 8–30, 30+).”
   - These turn:
     - Work into performance metrics.

I build these with **filters** (e.g., by team, program, region) so the same dashboard serves multiple views.

---

### 13.5.6 Funnel & lifecycle dashboards (higher-ed and public sector)

For higher-ed and benefit/service flows, I think in **funnel stages**:

1. **Define the stages clearly**
   - Higher-ed example:
     - Inquiry → Application Started → Application Submitted →
       Application Complete → Admitted → Enrolled → Retained.
   - Public sector example:
     - Initial Contact → Intake Started → Intake Completed → Eligibility Determined → Benefits Active.

2. **Map stages to data**
   - Each stage corresponds to:
     - A field value (Status, Stage, Lifecycle Stage),
     - Or a combination of fields/dates (e.g., “has decision date, no enrollment yet”).

3. **Dashboards**
   - Components:
     - “Counts by Stage (funnel visualization)”
     - “Drop-off rates between stages”
     - “Time spent in each stage (median/average)”
     - “Stage distribution by segment (program, region, population type)”

4. **Use cases**
   - Admissions/benefits leadership:
     - See where people get stuck.
     - Target process/UX improvements.
   - Advisors/case workers:
     - Identify “stale” records stuck at specific stages.

I don’t just show totals – I show **movement** and **blockages**.

---

### 13.5.7 Portal adoption & channel mix dashboards

This connects all your Experience Cloud tracking patterns to analytics:

1. **Channel metrics**
   - Reports to compare:
     - Portal-created vs staff-created:
       - Cases,
       - Applications,
       - Tasks,
       - Appointments.
   - Graphs:
     - Stacked bars over time (by week/month).
     - Show shift from staff-only to mixed/portal.

2. **Login & usage**
   - Metrics:
     - Unique portal users logged in per day/week/month.
     - Frequency (how many times per user).
     - First vs returning logins.
   - Use:
     - Measure adoption post-launch.
     - Identify under-served groups (eligible but never logged in).

3. **Flow completion / drop-off**
   - Use activity logs or step-tracking fields to report:
     - “How many users reached Step X vs Step Y?”
     - “Where are users abandoning flows?”
   - Dashboards:
     - Funnel style charts per major flow (application, profile update, benefit request).

4. **Program/service-specific portal metrics**
   - Filtered dashboards for:
     - Specific programs,
     - Specific services (e.g., grants, special benefits).
   - Show:
     - Whether portal adoption is uniform or skewed.

This is how you argue for **UX work** or campaigns instead of guessing.

---

### 13.5.8 Integration & data quality dashboards

You care deeply about integration health, not just whether a job “ran”.

1. **Error-centric dashboards**
   - Based on Error Log / Integration Log objects:
     - “Errors by Process / Integration”
     - “Errors by Error_Type__c (Business vs System vs Data Quality)”
     - “Top 10 Error Messages (last 7/30 days)”
   - Used by:
     - Integration team,
     - Data owners,
     - Architects.

2. **Staleness dashboards**
   - Using fields like:
     - `Data_Sync_Timestamp__c`,
     - `Last_Integration_Job__c`.
   - Reports:
     - Records where sync timestamp is older than threshold (per object).
     - Distribution of “time since last sync”.
   - These answer:
     - “Is SIS still feeding us correctly?”
     - “Which records look stale?”

3. **Data quality dashboards**
   - Reports on:
     - Missing external IDs.
     - Orphan records (children without parents).
     - Duplicates (same external ID, same email, etc.).
   - These become:
     - Work queues for cleanup.
     - Justification for DQ projects/tools.

4. **Job run dashboards**
   - If you log job runs:
     - “Jobs by Status (Success/Failed)”
     - “Average Duration by Job”
     - “Volume processed per run.”
   - Give:
     - Quick view of “are nightly jobs healthy?”

---

### 13.5.9 Program / service performance dashboards

For higher-ed, grants, and public-service work, I design dashboards that answer:

1. **For programs (higher-ed)**
   - Metrics:
     - Applications by program, term, modality.
     - Enrollments by program over time.
     - Yield: Application → Admit → Enroll.
     - Retention: Enrolled → Returned Next Term.
   - Views:
     - Program chairs/directors can see their portfolio.
     - Central leadership can compare programs.

2. **For services/benefits (public sector)**
   - Metrics:
     - Cases by service type.
     - Time-to-eligibility decisions.
     - Benefit utilization by population segments.
   - Use:
     - See which services are overloaded or underused.
     - Provide data for policy discussions.

3. **For grants/funds**
   - Metrics:
     - Awards by program, population, timeframe.
     - Fund utilization vs allocations.
     - Overlaps (people receiving multiple grants/services).
   - These rely on:
     - Clean relationships from your data model work.

---

### 13.5.10 When & how I use external analytics tools

Sometimes Salesforce reports aren’t enough; I leverage external tools:

1. **Marketing/engagement analytics**
   - When:
     - You need multi-channel funnel views (email, ads, web, CRM).
   - Pattern:
     - Use a marketing analytics platform to:
       - Combine Salesforce data (leads, opps, applications) with web/email data.
       - Build attribution models and cross-channel dashboards.

2. **BI tools (for heavy analysis)**
   - When:
     - You need:
       - Advanced visualizations,
       - Complex joins across multiple systems,
       - History snapshots.
   - Approach:
     - Salesforce is one of many data sources.
     - External BI holds:
       - Data warehouse,
       - Historical snapshots beyond what SF easily stores.

3. **Design principle**
   - Deciding what lives where:
     - **Salesforce dashboards**:
       - Day-to-day operational and near-real-time insights.
     - **External analytics**:
       - Cross-system, strategic, historical, or heavy analyses.

I don’t try to force Salesforce to be a full data warehouse; I use it where it’s strongest.

---

### 13.5.11 Performance & maintainability of reporting

I’m aware that bad reporting design can kill org performance and admin sanity.

- **Avoid “everything in one report”**
  - I rarely:
    - Build reports that join too many objects with bloated filters.
  - Instead:
    - Multiple focused reports feeding a dashboard.

- **Filter discipline**
  - Use:
    - Indexed fields in filters where possible (Status, RecordType, external IDs, key flags).
    - Reasonable date ranges instead of “all time” for huge datasets.
  - Manage:
    - Row limits on tables.
    - Summaries/aggregation rather than huge detail lists when not needed.

- **Standardized naming & descriptions**
  - Reports named with:
    - Subject + segment + time horizon, e.g.:
      - `Advisor – My Open Cases – This Week`
      - `Admissions – Funnel by Program – Current Term`
  - Descriptions:
    - Explain what the report is for and who uses it.

- **Decommissioning & cleanup**
  - Periodically:
    - Identify unused reports/dashboards.
    - Archive or delete to avoid clutter.
  - Encourage:
    - Using a curated “official” dashboard set instead of everyone cloning endlessly.

---

### 13.5.12 Reporting anti-patterns I try to avoid

There are things I consider **red flags**:

- **Schema-driven reports, not question-driven**
  - Building a report because “these objects are related,” but no user has a clear question.

- **“Kitchen sink” dashboards**
  - Dozens of small components, none focused.
  - No clear story or priority for the viewer.

- **Hidden business logic in reports**
  - Complex formula fields or filters that encode important rules that aren’t documented elsewhere.
  - I prefer:
    - To move core rules into fields/Flows/Apex and then report on simple flags/metrics.

- **Reliance on ad-hoc Excel exports**
  - If people constantly:
    - Export and do the real work in spreadsheets.
  - It signals:
    - Missing report types,
    - Missing derived fields,
    - Or that we need a purpose-built dashboard or external BI.

---

### 13.5.13 To Validate – Reporting & Analytics specifics

When you compile this into your knowledge base, you’ll want to fill in:

- **Canonical dashboards you’ve already built**
  - List specific dashboards (anonymized) by purpose:
    - E.g., “Admissions Funnel – Executive”, “Advisor Workload – Daily”, “Integration Health – Technical”.

- **Standard KPIs you care about**
  - For each domain (advising, benefits, portal, integration):
    - Write down the top 5–10 metrics you treat as “must-have”.

- **Report type standards**
  - Decide:
    - Which custom report types are your “default set” for:
      - Students/programs,
      - Cases/notices,
      - Portal logs,
      - Integration errors.

- **External analytics boundaries**
  - Document:
    - Where you intentionally rely on external analytics platforms.
    - What questions those platforms answer that Salesforce alone shouldn’t.

## 13.6 LWC UX, Accessibility & Performance – How I Actually Build Front-End

### 13.6.1 How I think about LWCs (role in the system)

I don’t see LWCs as “just a fancy UI layer.” In my head:

- **Flows** = guided business processes for admins and semi-technical users.
- **Apex** = business + integration logic, data access, rules.
- **LWCs** = the **experience layer** that:
  - Makes complex rules usable for humans.
  - Hides integration complexity.
  - Gives fast, focused UIs where page layouts are too blunt.

Mental rule:

> “If the standard layout or Flow screen will confuse users or slow them down, that’s a candidate for an LWC.”

---

### 13.6.2 Component design: how I break work into LWCs

I don’t build one “mega-component”. I think in terms of **roles**:

- **Container / smart components**
  - Responsibilities:
    - Fetch data via Apex.
    - Own the main state for a feature (e.g., selected program, selected case).
    - Coordinate child components.
  - Examples:
    - A “Program Selection Wizard LWC” orchestrating multiple steps.
    - A “Case/Client Overview LWC” aggregating multiple related blocks.

- **Presentational / dumb components**
  - Responsibilities:
    - Display data.
    - Emit events (clicks, selections, edits).
  - No direct Apex calls.
  - Examples:
    - A risk score card.
    - A list of programs with filters.
    - A basic attribute display panel.

- **Utility / shared components**
  - Reused across features:
    - Toast helpers (wrapper around `ShowToastEvent` patterns),
    - Loading spinners / skeleton placeholders,
    - Standard confirmation dialogs.

This separation matters because:

- It keeps Apex and data access centralized.
- It makes presentational components reusable and safe to drop into different screens.
- It reduces “mystery logic” hidden deep inside random LWCs.

---

### 13.6.3 Data access patterns (wire vs imperative Apex)

I’m deliberate about how LWCs talk to Apex:

- **`@wire` for read-only, view-building**
  - Use when:
    - Fetching data to display (record details, lists, summary info).
    - Caching is desirable.
  - Pattern:
    - Wire to an Apex method or LDS (`getRecord`, `getRecordUi`, etc.).
    - Map response into local state / view models.
  - Key behaviors:
    - Handle `data` and `error` explicitly.
    - Show spinners or skeletons until data arrives.
    - Avoid requerying on every small change.

- **Imperative Apex for actions**
  - Use when:
    - Performing mutations (create/update, triggering processes, integrations).
    - Relying on user actions (clicks, form submissions).
  - Pattern:
    - `await performAction({ param1, param2 });`
    - Handle:
      - Success: update UI / show toast.
      - Failure: show meaningful error message + optional error log correlation.

- **View models instead of raw SObjects**
  - When possible:
    - Apex returns **structured DTOs** (status, labels, computed flags).
  - Benefits:
    - LWC doesn’t need to know about all field quirks.
    - Easy to adjust underlying schema without breaking UI.

---

### 13.6.4 State management inside LWCs

I treat state as a first-class design concern:

- **Local component state**
  - Stored in tracked properties:
    - `selectedProgram`, `currentStep`, `isLoading`, `errors`, etc.
  - I avoid:
    - Using DOM as state (no “read back from HTML” patterns).
  - Keep state minimal but explicit:
    - Enough to render and decide behavior, nothing more.

- **Parent-child communication**
  - Use:
    - Public properties (`@api`) and custom events.
  - Parent:
    - Holds “source of truth” for main state.
  - Children:
    - Emit events like `programselected`, `stepcompleted`, `riskreevaluated`.
  - This avoids:
    - Hidden cross-component dependencies.

- **URL / navigation state (portal)**
  - When needed:
    - Encode some state in URL query params or page reference attributes (e.g., record Id, tab selection).
  - Useful for:
    - Deep linking into specific states (e.g., “open application step 3 for student X”).

---

### 13.6.5 UX patterns I actually care about

I’m not trying to win design awards; I’m trying to make workflows **clear and fast**:

- **Single-responsibility UIs**
  - Each LWC:
    - Has a clear job (select a program, display risk, summarize case).
  - If a component does 5 things:
    - That’s a smell: break it down.

- **Guided steps instead of long forms**
  - For complex flows (application, onboarding):
    - Use steps/wizards:
      - Step 1: Basic info.
      - Step 2: Program/benefit selection.
      - Step 3: Supporting info.
      - Step 4: Review & submit.
  - Users see:
    - Where they are.
    - What’s left.

- **Inline validation**
  - Validate:
    - As users type or on blur, not only on final submit.
  - Show:
    - Clear error messages next to fields.
  - Example mindset:
    - “Tell the user **exactly what’s wrong** (e.g., missing doc, invalid combination), not just ‘form invalid’.”

- **Clear call-to-action**
  - Each LWC state answers:
    - “What should I do right now?”:
      - `Submit`, `Next`, `Re-evaluate`, `Contact Support`, etc.
  - Avoid:
    - Ambiguous or tiny buttons that hide the main action.

---

### 13.6.6 Accessibility patterns (how I think about a11y)

I assume that:

- Some users will rely on keyboard only.
- Some may use screen readers.
- Some may be on low-resolution or mobile devices.

So I care about:

- **Semantic markup**
  - Use the right HTML elements:
    - `<button>`, `<label>`, `<fieldset>`, `<legend>`, `<ul>/<li>`, etc.
  - Avoid:
    - Div soup for interactive elements.

- **Keyboard navigation**
  - Ensure:
    - All interactive elements are reachable by TAB.
    - Focus order is logical.
  - For dynamic content:
    - Move focus appropriately when steps change (e.g., to a heading, first field).

- **ARIA attributes where needed**
  - Use:
    - `aria-live` regions for status/error messages.
    - `aria-describedby` for help text.
    - `role="alert"` for critical messages.
  - But:
    - Don’t overdo ARIA if semantics can do the job on their own.

- **Color & contrast**
  - I avoid:
    - Relying on color alone to convey meaning (e.g., error vs success).
  - Use:
    - Icons, text labels (“Error”, “Warning”, “Success”) in addition to colors.

- **Error messaging for screen readers**
  - When validation fails:
    - Combine:
      - Visual error messages,
      - Screen reader-friendly announcements.

In Experience Cloud portals, this matters even more because the user base is broader and includes people with varying abilities.

---

### 13.6.7 Experience Cloud vs internal LWC UX

I don’t design portal LWCs and internal LWCs the same way:

- **Portal (external users)**
  - Priorities:
    - Simplicity and clarity.
    - Minimal jargon (no internal system terms).
    - Strong guardrails:
      - Prevent invalid actions.
      - Avoid showing confusing system states.
  - Examples:
    - Program selection with clear options and filters.
    - Step-based application wizards.

- **Internal (staff, case workers, advisors)**
  - Priorities:
    - Efficiency and density of information.
    - Multi-record context (see related objects at once).
    - Shortcuts:
      - Quick actions, bulk-like behavior, keyboard-friendly.
  - Examples:
    - LWC cards on console pages that:
      - Summarize client profile, risk, recent notices.
      - Provide “one-click” actions (create task, re-run sync, open related portal view).

- **Shared patterns**
  - Error handling, loading states, and consistent styling across both.

---

### 13.6.8 Performance patterns – how I keep LWCs fast

I treat performance as part of UX, not an afterthought:

- **Minimize number of wires/callouts**
  - Prefer:
    - One Apex call that returns all needed data for the view.
  - Over:
    - Many small calls to different endpoints.
  - If I need different pieces:
    - Wrap them in a service Apex method that builds a **view model**.

- **Avoid unnecessary re-renders**
  - Keep reactive tracked properties small and focused:
    - e.g., `isLoading`, `selectedId`, `rows`.
  - Avoid:
    - Updating large objects unnecessarily.
    - Creating new array instances in ways that cause full rerender loops.

- **Lazy loading**
  - Load heavy data only when needed:
    - Tab content on first view.
    - Detail sections after click/expansion.
  - For lists:
    - Consider paginated or “load more” patterns.

- **Client-side filtering when appropriate**
  - If:
    - Dataset is small/medium and already in memory.
  - Use:
    - Client-side filtering/sorting instead of repeated Apex calls.
  - If dataset is large:
    - Push filtering/sorting to Apex and the DB.

- **Skeleton/loading states**
  - Always show:
    - Loading indicators or skeleton screens.
  - Goal:
    - Users feel the system is working, not frozen.

---

### 13.6.9 Error UX in LWCs

Error handling is part of the design:

- **User-friendly messages**
  - Show:
    - “We couldn’t complete this action. Please try again or contact support.”
  - Optionally:
    - Include a reference code that maps to an Error Log record.

- **Differentiate user vs system errors**
  - User / business errors:
    - “This program is not available for your current status.”
    - “You need to complete step X before moving to step Y.”
  - System errors:
    - “There was an issue connecting to our systems. Please try again later.”

- **Display and logging**
  - For serious errors:
    - Log via Apex (custom log object, platform event, etc.).
  - UI:
    - Shows a summarized message, not the full internal error.

---

### 13.6.10 Testing & hardening LWCs

Even if not formalized with a giant test suite in every org, I care about:

- **Scenario-based manual testing**
  - Test LWCs with:
    - Different profiles/permission sets (what if a field is hidden?).
    - Different data states:
      - No records,
      - One record,
      - Many records.
    - Edge cases:
      - Null values,
      - Max-length strings,
      - Weird combinations of flags.

- **API contracts**
  - I design:
    - Apex methods to have stable method signatures and response shapes.
  - So that:
    - LWC changes can evolve without breaking everything.
    - I can safely refactor interior logic.

- **Resilience to partial failures**
  - For multi-step actions:
    - Make sure the LWC & Apex handle:
      - Some sub-actions failing,
      - Partial data returns.
  - Avoid:
    - Leaving the UI stuck in an “unknown” state.

---

### 13.6.11 Design specifics for known LWCs I’ve actually built (patterns behind them)

Without naming any orgs:

- **Fraud / risk score card LWC**
  - Inputs:
    - Person/case context (IDs).
  - Behavior:
    - Displays score, confidence levels, and reasons (where available).
    - Uses color + text to signal risk level.
    - Handles:
      - No score yet,
      - Score outdated,
      - System unavailable.
  - Patterns:
    - One Apex call to fetch score & metadata.
    - Clear fallback states (“No score available”).

- **Program selection LWC (admissions context)**
  - Inputs:
    - Applicant profile (level, eligibility).
  - Behavior:
    - Shows list of available programs with filters (modality, level, etc.).
    - Disables or hides ineligible options.
    - Writes selection back to underlying record(s).
  - Patterns:
    - Client-side filtering based on user input.
    - Validations based on config-driven program flags.

- **Console / summary LWCs**
  - Show:
    - Multi-object snapshots (person, applications/cases, notices, tasks).
  - Action buttons:
    - Quick create, quick navigate, quick re-evaluate.
  - Patterns:
    - Aggregate data in Apex into a view model.
    - Render multiple “cards” from one payload.

These examples show how I use LWCs to **reduce cognitive load** for users dealing with complex, multi-system realities.

---

### 13.6.12 Anti-patterns I try to avoid in LWCs

There are certain front-end behaviors I consider red flags:

- **Putting SOQL in LWCs (via UI API misuse)**
  - I keep:
    - All complex querying in Apex, not multiple scattered `getRecord` calls.

- **Tightly coupling LWCs to specific record types or org-specific details**
  - I prefer:
    - Passing config (e.g., allowed record types, labels) instead of hard-coding them.

- **Massive LWCs with too many responsibilities**
  - Hard to test, hard to modify.
  - I’d rather:
    - Break into container + child components.

- **Hard-coded strings and labels everywhere**
  - Future pain:
    - For translation.
    - For rebranding.
  - I try to:
    - Use Custom Labels or a central mapping object when feasible.

- **Ignoring accessibility**
  - Building only for mouse users with perfect vision is not acceptable; I treat that as an incomplete implementation.

---

### 13.6.13 To Validate – LWC specifics

As you compile this into your knowledge base, you’ll want to define more concretely:

- **Canonical LWC categories you use**
  - E.g.:
    - `SummaryCard` LWCs,
    - `SelectionWizard` LWCs,
    - `LogViewer`/`HistoryViewer` LWCs,
    - `PortalAction` LWCs.

- **Styling approach**
  - Decide:
    - How much you standardize on:
      - SLDS only,
      - SLDS + minimal custom CSS,
      - Utility classes, etc.

- **Shared component library**
  - Document:
    - Shared components you already have or want:
      - Spinners,
      - Error banners,
      - Step indicators,
      - Confirm dialogs.

- **Formal testing practice**
  - Confirm:
    - If/where you’ve used Jest or other automated tests.
    - How much you want to standardize automated testing vs structured manual testing.

- **Accessibility checklist**
  - Write:
    - A small internal checklist you mentally use:
      - Keyboard usage,
      - Screen reader text,
      - Color/contrast,
      - Error message semantics.

## 13.7 Boomi / Integration-Platform Patterns – My Template Library

### 13.7.1 How I think about the integration layer

I don’t see Boomi (or any iPaaS) as “a pipe that moves data.” In my head it’s:

- A **policy enforcement layer**:  
  Validations, mappings, throttling, retries.
- A **translation layer**:  
  Salesforce model ↔ SIS/ERP/legacy models.
- A **control plane**:  
  Where we see volumes, failures, performance trends.

Mental rules:

> “Salesforce is system of engagement. SIS/ERP is system of record.  
> Boomi is the adult in the middle making sure they don’t lie to each other.”

> “If the same logic will be needed more than once, it belongs in a **reusable process**, not copy-paste shapes.”

---

### 13.7.2 Core Boomi process patterns I actually use

I tend to re-use a small set of patterns and combine them:

1. **Salesforce → Boomi → Database (SIS/Oracle)**
   - Trigger:
     - Scheduled process or event-driven feed from Salesforce (Platform Events, data exports, or report/date filters).
   - Flow:
     - Get IDs / records from Salesforce.
     - Build canonical payload (minimal, normalized).
     - Batch into DB queries or stored procedures.
     - Write results back to Salesforce or staging object.

2. **Database (SIS/Oracle) → Boomi → Salesforce**
   - Trigger:
     - Scheduled jobs (nightly, hourly) or event-based from DB/log tables.
   - Flow:
     - Query DB with filters (last-changed datetime, ID lists, etc.).
     - Transform into Salesforce object shapes.
     - Upsert into Salesforce via External IDs.

3. **Multi-system fan-out**
   - One upstream change (e.g., student update) drives:
     - Salesforce upserts,
     - Analytics/warehouse feeds,
     - Notifications to other services.
   - Flow:
     - Boomi receives change set.
     - Branches:
       - One branch for each target system.
     - Common error handling & logging.

4. **Backfill / one-time migration processes**
   - Specialized flows used to:
     - Load historical data.
     - Perform “one massive migration” in a controlled way.
   - Designed to be:
     - Re-runnable with idempotent behavior (no duplicates if re-run).
     - Traceable via job IDs, timestamps, and log outputs.

---

### 13.7.3 The 300K+ ID batching pattern (concrete pattern I’ve actually built)

This is a very specific pattern that lives in my head:

**Goal:**  
Work with ~300K EMPLIDs (or similar keys) daily to fetch related records from an Oracle/SIS DB using legacy connectors that expect an `IN (...)` clause.

**Pattern:**

1. **Get ID universe (from Salesforce or another system)**
   - Source:
     - Salesforce export,
     - Or previous Boomi process writing ID lists to disk/database.

2. **Write IDs to disk (staging)**
   - Use Boomi to:
     - Write all IDs into a local disk file (or files) as one ID per line or CSV format.
   - Reason:
     - Avoid holding 300K IDs only in process memory.
     - Allow restart/reuse without re-pulling everything.

3. **Read back and batch**
   - Second Boomi process:
     - Read from the disk file.
     - Chunk IDs into batches:
       - e.g., 500 / 1000 per SQL call, depending on DB limits & performance tests.
     - Build dynamic SQL like:
       - `SELECT ... FROM table WHERE EMPLID IN (...)` with the current batch.

4. **DB query**
   - Use Boomi’s DB connector:
     - Execute the dynamic IN-clause query.
   - Return result sets per batch.

5. **Map and upsert**
   - Map DB results into a canonical structure.
   - Upsert into Salesforce (or another target) using External IDs.

6. **Monitoring**
   - Log:
     - How many IDs processed per batch.
     - Success/failure counts.
     - Any “no match found” situations.

**Why this matters:**

- Works around limitations of:
  - Integration connectors that don’t handle huge parameter sets.
  - Database or network timeouts for giant queries.
- Is **scalable and repeatable** for large, ID-driven syncs.

---

### 13.7.4 Reusable Boomi components I actually care about

Instead of “one-off” processes, I like building **reusable sub-processes** and patterns:

1. **Standardized logging sub-process**
   - Input:
     - Process name,
     - Correlation/job ID,
     - Record counts,
     - Error details (if any).
   - Behavior:
     - Write to:
       - Salesforce log object,
       - Database log table,
       - Or file/boomi Atom logs with consistent formatting.
   - Purpose:
     - Every process can call this on success/failure → consistent observability.

2. **Error routing sub-process**
   - Input:
     - Error type (system/business),
     - Payload,
     - Context (system/user).
   - Behavior:
     - Decide:
       - Retry queue?
       - Send to error topic?
       - Log & notify only?
   - Purpose:
     - Central place for “what do we do when a process fails?” not duplicated logic.

3. **Canonical mapping sub-process**
   - Converts:
     - Source system formats into a canonical integration format.
   - Then:
     - Other processes map canonical → target (Salesforce, SIS, etc.).
   - Benefit:
     - When source or target changes, only one side of the mapping must adapt, not all flows.

4. **Common utilities**
   - Reusable shapes or sub-processes for:
     - Date/time conversions (e.g., DB timestamps ↔ Salesforce DateTime).
     - ID concatenations (for composite keys).
     - Simple transformations (uppercasing, trimming, null-handling).

This turns Boomi from a tangle of one-off flows into a **library**.

---

### 13.7.5 Environment configuration & promotion strategy

I’m strict about **not hard-coding env-specific stuff** inside logic:

- **Environment properties**
  - Store:
    - Endpoint URLs,
    - Credentials (ideally via secure mechanisms),
    - Batch sizes,
    - Feature flags,
    - Logging verbosity levels.
  - Access:
    - Read from environment properties at process start.
  - Benefit:
    - Same process artifact can move from DEV → QA → PROD with only property changes.

- **Properties for batch sizes & limits**
  - Keep:
    - `MAX_IDS_PER_QUERY`, `MAX_RECORDS_PER_MESSAGE`, etc. as properties.
  - Reason:
    - Tune performance per environment and scaling scenario.
    - Avoid editing process diagrams just to adjust performance.

- **Promotion**
  - Work pattern:
    - Build & validate in lower env.
    - Export/import or promote through Boomi’s deployment mechanism.
  - I treat:
    - Process versions and environment properties as part of the release plan.

---

### 13.7.6 Mapping & transformation patterns

I treat mapping as first-class logic, not as an afterthought:

- **Explicit field-level mapping**
  - Use Boomi’s mapping step to:
    - Map each source field → target field.
  - Add:
    - Clear comments for non-trivial mappings (calculated fields, lookups, defaulting logic).

- **Lookup tables & reference data**
  - Where codes differ (SIS vs Salesforce picklist values):
    - Maintain lookup tables (in Boomi, or external ref data).
  - Make sure:
    - These mappings are version-controlled and not just “in someone’s head.”

- **Null & default handling**
  - Explicitly decide:
    - When to set a default,
    - When to leave null,
    - When to log an error & skip.
  - This avoids:
    - Surprising or invalid data silently creeping into Salesforce.

- **Key & ID handling**
  - Always:
    - Map primary keys to External ID fields in Salesforce.
  - For composite keys:
    - Build them consistently in Boomi and Salesforce formulas.
  - No “hidden keys” buried in text fields.

---

### 13.7.7 Error handling patterns in Boomi

Error handling in Boomi connects to the larger error/observability strategy:

- **Try/Catch shapes**
  - Wrap:
    - Callouts,
    - DB queries,
    - Salesforce connectors.
  - On failure:
    - Route to:
      - Error logs,
      - Retry queues,
      - Notifications.

- **Business vs system errors**
  - Business errors:
    - Validation fails (e.g., data missing required fields or invalid combination).
    - Often:
      - Logged as “bad data,” with record IDs and reasons.
      - May generate a work queue in Salesforce.
  - System errors:
    - Network failures, timeouts, 5xx errors.
    - Treated as:
      - Retriable (to a point).
      - Higher urgency to ops teams.

- **Dead-letter / parking patterns**
  - For records that repeatedly fail:
    - Move them to a “parking lot”:
      - A DB table,
      - File,
      - Or special Salesforce object.
    - They can be:
      - Reviewed manually.
      - Reprocessed once the issue is fixed.

- **Notifications**
  - Use:
    - Email, chat, or monitoring integrations to notify when:
      - Errors exceed a threshold,
      - A process fails completely,
      - A critical integration is down.

---

### 13.7.8 Job design: scheduling, dependencies, and throttling

I think about **when** and **how often** jobs run as part of the architecture:

- **Scheduling strategy**
  - Group jobs by:
    - SLA (real-time, hourly, nightly).
    - Data dependencies (e.g., “Programs must sync before Enrollments”).
  - Avoid:
    - Overlapping jobs that compete for DB/SF resources.

- **Dependencies**
  - Define:
    - Upstream/downstream relationships:
      - e.g., Program sync before Student sync, Student sync before Enrollment sync.
  - For multi-step flows:
    - Use job-level flags or logs to indicate:
      - “Step 1 complete, Step 2 ready.”

- **Throttling**
  - For high-volume or sensitive systems:
    - Limit:
      - Number of records per run.
      - Concurrency with other jobs.
    - Use:
      - Batch size properties.
      - Sleep/wait patterns where absolutely necessary.

- **On-demand jobs**
  - Some processes built to:
    - Run ad hoc (manual start) for:
      - Backfills,
      - Reprocessing subsets (e.g., only Program X or Term Y).

---

### 13.7.9 Integration with Salesforce: patterns I actually use

Patterns I’ve actually leaned on:

- **Salesforce connector with External IDs**
  - Always configure:
    - Upserts based on External IDs, not Salesforce Id.
  - Behavior:
    - Insert if no record exists,
    - Update if match found by external key.

- **Selective field updates**
  - Map:
    - Only fields that belong to the source-of-record.
  - Avoid:
    - Overwriting Salesforce-only fields which represent user input or engagement data.
  - Sometimes use:
    - Partial updates or field-level rules.

- **Soft delete / status management from integration**
  - When records are “gone” in source:
    - Often:
      - Set “inactive” or “end-dated” in Salesforce.
      - Or mark with a soft-delete flag.
  - Avoid:
    - Hard deletes driven directly by integration, unless well-controlled.

- **Handling Salesforce limits**
  - Consider:
    - API call count,
    - Batch sizes,
    - Payload sizes.
  - Design:
    - Batches small enough to avoid hitting limits but large enough to be efficient.

---

### 13.7.10 Monitoring, metrics & health checks

Boomi is not just “fire and forget”; I care about health:

- **Per-process metrics**
  - Track:
    - # of documents processed,
    - # of successes,
    - # of failures,
    - Duration per run.
  - Use these to:
    - Spot regressions.
    - Tune batch sizes and schedules.

- **Trend monitoring**
  - Look at:
    - Error trends over days/weeks.
    - Drop-offs in volume (e.g., fewer updates than usual).
  - These often:
    - Reveal upstream issues before users complain.

- **Integration SLIs (Service Level Indicators)**
  - Examples:
    - “Time from SIS change → Salesforce updated.”
    - “% of integration runs completing successfully per day.”
    - “Number of records in error state longer than X hours.”

- **Alerting**
  - Configure:
    - Alerts when a job fails.
    - Alerts when error rate > threshold.
    - Alerts when no data flows for a job that usually runs regularly.

---

### 13.7.11 How I use Boomi alongside other tools (MuleSoft, event bus, etc.)

Even when Boomi isn’t the only integration tech, I mentally place it in a larger picture:

- **Boomi as batch/ETL workhorse**
  - Great for:
    - High-volume DB queries,
    - Complex mappings,
    - Nightly or scheduled syncs.
  - Ideal for:
    - SIS/ERP <-> Salesforce data alignment.

- **MuleSoft / API gateway / event bus**
  - Better fit for:
    - Real-time APIs,
    - Public-facing API products,
    - Cross-domain event routing.
  - In some designs:
    - Salesforce → Platform Events → Event Bus → Mule/Boomi/other consumers.

- **Design principle**
  - Don’t force one tool to do everything.
  - Use:
    - Boomi where it’s the right hammer (ETL, DB-heavy).
    - Other tools where API management, multi-tenant API governance, or event orchestration matter.

---

### 13.7.12 To Validate – Boomi / Integration specifics

As you compile this into your knowledge base, you’ll want to fill in:

- **Exact reusable Boomi subprocesses you’ve built**
  - Names and roles:
    - e.g., “Student Upsert Canonical Mapper”, “Oracle Batch Fetch”, “Error Logger”.

- **Environment property standards**
  - Decide:
    - Names for batch-size properties,
    - Prefix conventions for URLs, credentials, flags.

- **Error handling conventions**
  - Standardize:
    - When to route to dead-letter/parking.
    - Which errors lead to automatic retry vs manual intervention.

- **Health dashboards**
  - Document:
    - Which Boomi / external dashboards are “canonical” for:
      - Volume,
      - Errors,
      - Latency.

- **Cross-tool boundary**
  - Clarify:
    - Where you want Boomi to be the default vs where an API gateway or Mule-style integration takes over.
    - So future designs follow the same split instead of ad-hoc choices.

## 13.8 Sandbox / Environment & Release Strategy – How I Avoid Breaking Stuff

### 13.8.1 How I think about environments

I don’t see “sandbox” as just “another org”. In my head, each environment has a **job**:

- Some are for **experiments** and spikes.
- Some are for **integration & system tests**.
- Some are for **user validation** and **training**.
- One (prod) is for **real life** and must be treated as a crime scene:  
  no changes without a trail.

Mental rules:

> “Every environment has a purpose. If you use it for something else, you pay for it later.”

> “Promotion is one-way. You don’t ‘fix prod’ and then forget to apply it to lower envs.”

---

### 13.8.2 Typical environment lineup & their purposes

I generally think in this stack (names vary, but the roles are similar):

1. **Scratch orgs / Personal DEV sandboxes**
   - Purpose:
     - Spikes, prototypes, small self-contained features.
     - Learning new patterns, trying packages, POCs.
   - Behavior:
     - Can be broken freely and discarded.
     - Used with local tooling (VS Code + CLI) for isolated work.

2. **Team DEV / Integrated DEV**
   - Purpose:
     - Integrate multiple developers’ work.
     - Early integration with external systems (dev APIs, mock services).
   - Behavior:
     - Still okay to be a bit messy, but:
       - Metadata is under version control.
       - People don’t directly hack in prod-only logic here.

3. **QA / System Test**
   - Purpose:
     - Stable environment to validate:
       - End-to-end flows,
       - Integration test cases,
       - Regression tests for new changes.
   - Behavior:
     - Controlled changes via deployment pipeline.
     - Used by testers / QA analysts.
     - Often has more realistic data than DEV.

4. **UAT (User Acceptance Test)**
   - Purpose:
     - Business users validate new features before go-live.
   - Behavior:
     - Contains near-prod config & test data.
     - Only changes that are candidates for upcoming releases.
     - UAT sign-off == “we’re comfortable taking this to prod”.

5. **PERF / Performance / Load Test**
   - Purpose:
     - Capacity, performance, concurrency testing.
     - Sometimes used for integration performance tests with external systems.
   - Behavior:
     - Data volumes closer to prod.
     - Used selectively, not for everyday config changes.

6. **TRAINING / DEMO (optional but useful)**
   - Purpose:
     - Training new staff.
     - Running workshops and demos.
   - Behavior:
     - Stable scenario data.
     - Not tied tightly to release cadence.

7. **PROD**
   - Purpose:
     - Real users, real consequences.
   - Behavior:
     - No “quick hacks” without:
       - Change tracking,
       - Backups,
       - A clear plan to propagate fixes back down if needed.

---

### 13.8.3 Metadata & config management (no “clicks in prod”)

I’m opinionated about how changes are made:

- **Everything important is in version control**
  - Source of truth:
    - Git repository with:
      - Apex,
      - LWCs,
      - Flows,
      - Object / field metadata,
      - Profiles/permission sets (as much as practical),
      - Experience Cloud metadata.
  - Tools:
    - CLI-based deployments (not only change sets).
    - Local diff & review in code editor.

- **No “one-off” prod config changes without tracking**
  - If I must:
    - Temporarily change something in prod (feature toggle, minor Flow condition tweak),
  - Then:
    - I record it in:
      - Change records/tickets,
      - Or update the metadata in source so the next deployment doesn’t overwrite it silently.

- **Branching strategy aligned to environments**
  - Typical mental model:
    - `main` / `trunk` ~ what’s ready for prod (or just deployed).
    - `release/x.y` ~ what’s staged for a particular release.
    - Feature branches ~ experiments and work-in-progress.
  - Promotion:
    - DEV → QA → UAT → PROD via merges & deployments.

---

### 13.8.4 Data strategy per environment

Data is as important as metadata:

- **DEV / scratch**
  - Minimal but **purposeful** test data:
    - Representative student/client cases, not full dumps.
    - Edge cases: special programs, exception scenarios.
  - Sometimes synthetic data only (no real PII).

- **QA**
  - Slightly richer data:
    - Enough variations to test:
      - Different programs/services,
      - Multiple personas (student, staff, provider, etc.),
      - Portal vs internal flows.
  - May use:
    - Partial copies of prod with masking or selective refresh.

- **UAT**
  - Needs to be **believable**:
    - Data that looks like real scenarios business users expect.
    - Names and records that users recognize conceptually (but anonymized).
  - Often:
    - Subset of prod or carefully curated sample.

- **PERF**
  - High volume:
    - Closer to real record counts.
    - Focus on:
      - Load patterns,
      - Batch job behavior,
      - Integration throughput.
  - Must:
    - Avoid real PII if regulations require masking or synthetic generation.

- **PROD**
  - Real, regulated, sensitive.
  - Any data fix:
    - Must be deliberate,
    - Loggable,
    - Reproducible in lower envs if it represents a structural change.

---

### 13.8.5 Sandbox refresh & post-refresh checklist

Refreshing isn’t just pushing a button; I have a mental checklist:

1. **Decide which sandbox and why**
   - Don’t refresh everything blindly.
   - Refresh:
     - QA / UAT periodically to keep close to prod.
     - Not DEV constantly, if dev config would be blown away.

2. **Post-refresh tasks (critical)**
   - Re-configure:
     - **SSO / auth**:
       - Connected apps,
       - Auth providers,
       - Callback URLs,
       - Certificates (sandbox vs prod).
     - **Integrations**:
       - Named Credentials → point to non-prod endpoints.
       - Remote Site Settings, API keys, secrets (sandbox values).
     - **Email & notifications**:
       - Org-wide email addresses,
       - Email deliverability (often set to “System Email Only” in sandboxes).
     - **Background jobs**:
       - Schedules for Batch/Scheduled Apex (often disabled or adjusted).
       - Third-party packages (dedupe, marketing, etc.) scheduled tasks.
   - Recreate:
     - Essential test data lost during refresh.
     - Integration user accounts & permission sets if needed.

3. **Protect prod-like config**
   - After refresh:
     - Re-apply:
       - Any environment-specific configuration that should **never** match prod (test log endpoints, etc.).

4. **Document**
   - Maintain:
     - A standard “Post-refresh runbook” so no step depends on memory.

---

### 13.8.6 Release planning & cadence

I don’t like random, uncoordinated releases:

- **Release units**
  - Group changes into:
    - “Release trains” per sprint or per 2–4 weeks.
  - Each release:
    - Has a name/number,
    - Has a set of tickets/features it includes.

- **Promotion path**
  - For each release:
    - Build in DEV (feature branches).
    - Integrate & stabilize in integrated DEV.
    - Deploy to QA for system testing.
    - Deploy to UAT for user acceptance.
    - Deploy to PROD after sign-off.

- **Change “package” mindset**
  - Each release’s contents:
    - Are known (tickets, commits, metadata changes).
    - Can be re-deployed or rolled back if necessary.

- **Blackout windows**
  - Avoid:
    - Deploying big risky releases during:
      - Critical business periods (registration deadlines, benefit cycles).
      - Government reporting windows.
  - Schedule:
    - Releases outside those windows or with extra safeguards.

---

### 13.8.7 Feature toggles & configuration-driven behavior

I don’t want every change to require a deploy:

- **Feature toggles (config)**
  - Use:
    - Custom Metadata,
    - Custom Settings,
    - Flags on objects/records.
  - Examples:
    - “New advisor workflow enabled?”
    - “Use new portal flow for program selection?”
  - Behavior:
    - Toggle gradually:
      - OFF in prod initially.
      - ON in lower environments.
      - Turn ON in prod after UAT and communication.

- **Safe rollout patterns**
  - Techniques:
    - Percentage rollout of new logic (e.g., only for certain programs/regions).
    - “Pilot” toggles:
      - Only for a subset of users or agencies.

- **Benefits**
  - You can:
    - Deploy code/config ahead of actual activation.
    - Roll back behavior by flipping a config instead of redeploying.

---

### 13.8.8 Pre-deployment quality gates

Before anything hits prod, mentally I want:

1. **Design reviewed**
   - For significant changes:
     - Architecture/design notes reviewed by at least one other senior person.
     - Check:
       - Data model impact,
       - Integration dependencies,
       - Performance impact.

2. **Test coverage**
   - Automated:
     - Apex coverage above required threshold on touched components.
     - Key integration code covered with mocks.
   - Manual:
     - Core flows executed in QA.
     - Regression testing on critical paths (login, search, key record updates).

3. **Change list clarity**
   - Know exactly:
     - Which objects, fields, Flows, Apex classes, LWCs, and integrations are changing.
   - No “mystery metadata”.

4. **Rollback strategy**
   - At least:
     - Understand what’s hard to roll back (data migrations, destructive changes).
   - If possible:
     - Plan for:
       - Ability to disable new logic with feature toggles.
       - Backups for changed data (pre-deployment exports).

---

### 13.8.9 Deployment mechanics (how I actually move stuff)

Mechanically, I favor:

- **Scripted or automated deployments**
  - Using:
    - CLI commands or pipelines.
  - Repeatable:
    - Same commands for DEV → QA, QA → UAT, UAT → PROD (with env-specific variables).

- **Deploy in logical chunks**
  - Example:
    - Changes to data model + Apex classes + Flows + Experience Cloud config.
  - Avoid:
    - Deploying random mixes from different branches.

- **Pre- and post-deploy steps**
  - Pre:
    - Put system in “safe” state:
      - Temporarily deactivate conflicting Flows/triggers if needed.
      - Pause certain scheduled jobs that could clash.
  - Post:
    - Activate new Flows,
    - Run data fix/backfill scripts if required,
    - Kick off initial sync jobs.

---

### 13.8.10 Post-deployment validation & monitoring

After a release, I have a mental checklist:

- **Smoke tests**
  - Immediately check:
    - Log in (internal and portal).
    - Create/edit key records.
    - Run key Flows/LWCs.
    - Verify integration endpoints respond as expected.

- **Key reports/dashboards**
  - Look at:
    - Error logs (spikes?).
    - Integration error dashboards.
    - Portal activity (any drop or spike in failures?).
    - Any SLA metrics (queue backlog, etc.).

- **User feedback channels**
  - Monitor:
    - Support channels (incidents, service desk).
    - Direct feedback from power users and business owners.
  - For the first hours/days:
    - Expect some noise; check for patterns.

- **Formal sign-off**
  - Once stable:
    - Mark the release as “done” in tracking tools.
    - Note any follow-up items (backlogged enhancements, known issues).

---

### 13.8.11 Hotfix strategy (when production is on fire)

Not all issues can wait for the next release train:

1. **Triage**
   - Determine:
     - Is it a **true** prod incident?
       - Data corruption risk?
       - Downtime for key user group?
   - Decide:
     - Hotfix vs next scheduled release.

2. **Targeted changes**
   - Scope:
     - Minimal fix needed:
       - Condition tweak in Flow,
       - Small Apex patch,
       - Config change or toggle flip.
   - No piggybacking:
     - Don’t slip in extra features “since we’re deploying anyway”.

3. **Fix in lower env + fast promotion**
   - Ideally:
     - Reproduce in lower environment.
     - Apply fix there.
     - Deploy through path quickly (DEV → QA → PROD) with targeted package.
   - If time doesn’t allow:
     - Apply fix in PROD with:
       - Extra care,
       - Immediate follow-up to align lower envs.

4. **Post-hotfix cleanup**
   - Update:
     - Git repo / metadata to reflect fix.
     - Documentation / design docs if it was a structural change.
   - Add:
     - Tests (if missing) to prevent regression.

---

### 13.8.12 Cross-system release coordination

I’m aware that Salesforce isn’t alone; releases touch:

- Salesforce config + code.
- Integration platforms (Boomi, other ETL).
- External APIs (benefits engines, SIS/ERP, notice systems).
- Identity providers and SSO.
- Portals / websites / mobile apps.

**Patterns:**

- **Joint release planning**
  - For multi-system features:
    - Co-ordinate:
      - Which version of the API is required.
      - Which job schedules need updates.
      - When DNS / URL / SSO changes go live.
  - Use:
    - Shared change calendar.

- **Order of operations**
  - Often:
    - Deploy “backwards-compatible” pieces first.
      - E.g., API changes supporting both old and new Salesforce behavior.
    - Switch behavior via config when all pieces are live.

- **Shared fallback plan**
  - Decide:
    - If one system fails (e.g., external API), what does Salesforce do?
      - Queue requests?
      - Degrade gracefully?
      - Block actions with clear messages?

---

### 13.8.13 Environment & release anti-patterns I avoid

Things I consider red flags:

- **Developing directly in QA/UAT/PROD**
  - Making “quick” Flow or Apex edits in non-DEV environments without source control.

- **Random sandbox refreshes without communication**
  - Wiping out config or test data another team depends on.

- **Untracked manual prod fixes**
  - Adjusting data, metadata, or config in prod with no record of:
    - What changed,
    - Why,
    - How to reproduce in lower envs.

- **Mixing features in a release without clear grouping**
  - Deploying “whatever is ready today,” making it impossible to:
    - Roll back a single feature.
    - Understand which change caused an issue.

- **Overusing full refreshes as a “fix”**
  - Refreshing sandboxes to “fix weirdness” instead of:
    - Understanding root cause.
    - Designing stable, repeatable env setups.

---

### 13.8.14 To Validate – Environment & Release specifics

When you compile this into your knowledge base, you’ll want to lock in:

- **Official environment roles**
  - Define:
    - Exactly what each sandbox is for (DEV, QA, UAT, PERF, TRAINING).
    - Who owns it and what is allowed there.

- **Release cadence**
  - Decide:
    - Target frequency (e.g., every 2 weeks, monthly).
    - How hotfixes fit around the cadence.

- **Post-refresh runbook**
  - Document:
    - The full checklist you *always* do after sandbox refresh:
      - SSO/IdP changes,
      - Named Credential changes,
      - Email settings,
      - Schedules & jobs,
      - Test users/data.

- **Hotfix rules**
  - Define:
    - What qualifies as a hotfix.
    - Who must approve.
    - How to make sure hotfix changes are merged back into the main branch.

- **Cross-system dependency map**
  - Capture:
    - Which external systems each environment talks to (dev/test/prod endpoints).
    - Any coordination required for joint releases.

## 13.9 Change Intake, Triage & “Saying No Nicely” – How I Decide What To Build

### 13.9.1 Where changes come from (intake channels in my world)

In real life, work doesn’t arrive in a clean backlog. It shows up as:

- Service tickets (incidents, work orders, “react” tasks).
- Emails from business stakeholders (“Can we quickly add X?”).
- Direct chats/calls with leaders (“We need this for next term/next cycle.”).
- Integration/ops noise (recurring errors, sync failures).
- My own architectural / tech debt observations (“This Flow is one change away from collapsing.”).

Mentally I classify each incoming thing as:

- **Bug / Incident** – something broken vs expected behavior.
- **Enhancement / Feature** – fundamentally new behavior or UI.
- **Tech Debt / Cleanup** – making the system safer, faster, maintainable.
- **Compliance / Risk** – security, audit, policy-driven changes.
- **Experiment / Spike** – discovery work to learn what’s possible.

That classification drives how I treat it, not just who shouted loudest.

---

### 13.9.2 First move: separate “solution requests” from “actual problems”

Almost nobody comes with a pure problem; they come with **their favorite solution**:

- “Add this field here.”
- “Create an automated email when X happens.”
- “Just give us an export.”
- “We need a button that…”

My first move is to pull it back to **problem space**:

- Who is struggling?
- What can’t they do today?
- What decision do they need help making?
- How often does this happen?
- What’s the consequence if we do nothing?

Mentally I ask:

> “If I wasn’t allowed to change Salesforce at all, how would I solve this?”  
> If there’s still a clear problem at that level, then it’s worth design.

This avoids doing design-by-request where every “add this checkbox” becomes permanent debt.

---

### 13.9.3 Triage dimensions – how I rank and sort work

When I triage, it’s never just “high/medium/low”. In my head I juggle:

- **Impact**
  - How many users?
  - How many records?
  - Does it affect critical processes (eligibility decisions, funding, term start, etc.)?

- **Urgency / timing**
  - Is there:
    - A hard deadline (go-live, semester start, reporting window)?
    - A policy or contract date?
  - Or is it:
    - Annoying but tolerable for a bit?

- **Risk**
  - Data corruption potential?
  - SSO / security implications?
  - Breaks integrations downstream?

- **Effort / complexity**
  - Quick configuration vs multi-sprint build?
  - One object vs cross-system chain?

- **Dependencies**
  - Does it rely on:
    - Another team (SIS, integration, security)?
    - A future migration?
    - Platform limits/constraints?

- **Reversibility**
  - Can we roll back easily?
  - Or is it a “once you do it, you own it forever” kind of change (data migrations, major model changes)?

I don’t always say all of this out loud, but this is the mental grid behind “yes / no / not now”.

---

### 13.9.4 Categories of work and how I treat each

I implicitly treat categories differently:

1. **Bugs / Incidents**
   - Priority driven by:
     - Severity (data loss, inability to work, bad decisions).
     - Scope (one user vs all users).
   - Typical actions:
     - Triage quickly, reproduce, identify blast radius.
     - Patch + hotfix if critical.
     - Add tests / logging to prevent recurrence.

2. **Enhancements / Features**
   - Priority driven by:
     - Business value (time saved, quality improved, outcomes improved).
     - Strategic importance (tied to key initiative).
   - Actions:
     - Require a minimal **problem statement**.
     - Clarify acceptance criteria and “definition of done”.

3. **Tech Debt / Refactors**
   - Priority driven by:
     - Risk of things breaking later (brittle Flows, hard-coded IDs).
     - Cost of delay (does waiting make it much harder/expensive?).
   - Actions:
     - Often bundled with feature work (“while we’re here, let’s fix the foundation”).
     - Sometimes pitched explicitly: “We need to spend X time cleaning Y so we don’t melt later.”

4. **Compliance / Security / Audit**
   - Priority driven by:
     - External requirements (policy, law, audit findings).
     - Potential harm/fines if ignored.
   - Actions:
     - Escape the usual backlog when necessary.
     - Get explicit leadership acknowledgment if something must be deferred.

5. **Experiments / Spikes**
   - Purpose:
     - Reduce uncertainty, not deliver a finished feature.
   - Actions:
     - Time-box (“we’ll spend N hours to figure out what’s possible”).
     - Produce a clear outcome:
       - “Yes we can, and here’s what it costs.”
       - “No we can’t, here’s why and alternatives.”

---

### 13.9.5 Config vs code vs report vs integration – picking the right lever

When a change is valid, the next question is: **what kind of change is it really?**

I run a mental decision tree:

1. **Is this just visibility / insight?**
   - If yes:
     - Try **report/dashboard** first.
     - Maybe add a field that’s computed elsewhere and report on it.
   - Avoid:
     - Building automation if someone only needed a list and a filter.

2. **Is it a simple rule on one object?**
   - If yes:
     - Try **Flow / validation/picklists** first.
   - Use:
     - Record-triggered Flow for basic behavior.
     - Validation rules for business constraints.

3. **Does it touch multiple objects and have real branching?**
   - If yes:
     - Consider:
       - **Flow** if still manageable, or
       - **Apex** if rules are too complex, or multi-tenant/performance-sensitive.

4. **Does it require external systems / data?**
   - If yes:
     - It’s **integration** work, not just a field.
     - Needs:
       - Coordination with ETL/iPaaS,
       - Error handling,
       - Logging.

5. **Does it really belong outside Salesforce?**
   - Sometimes:
     - The right answer is to change the SIS/ERP, not Salesforce.
   - I mentally ask:
     - “Who should own this truth?” and push logic closer to that system.

This keeps me from defaulting to “just code it” or “just add a Flow” for everything.

---

### 13.9.6 How I prevent “just one more field” and “quick tweak” chaos

People love to say “it’s just one field” or “tiny change”. My defense mechanisms:

- **Ask where else it will show**
  - Layouts (which ones?),
  - Flows (do we need to include it?),
  - Integrations (does it need to sync?),
  - Reports (do we need new metrics?).

- **Ask who will maintain the logic**
  - Is this a stable rule?
  - Will business rules change soon?
  - If yes:
    - Config-driven approach (metadata, feature flag) over hard-coding.

- **Ask what happens if more teams want it**
  - If this pattern will spread:
    - I design it that way from day one (generic where reasonable).

- **Be honest about effort**
  - I don’t let:
    - “Quick change” language downplay real complexity.
  - I respond in terms of:
    - Impacted components,
    - Testing needs,
    - Environments.

---

### 13.9.7 Saying “no” or “not now” without burning bridges

I rarely say a flat “no”. I reframe in a few ways:

1. **Tradeoff framing**
   - “If we do X now, Y and Z will slip. Is that acceptable?”
   - Forces:
     - Stakeholder to acknowledge we can’t do everything simultaneously.

2. **Option A vs Option B**
   - Example pattern:
     - **Option A:** Quick, smaller change, solves 60–70% of the issue, low risk.
     - **Option B:** Bigger, more complete solution, solves 90–100%, higher effort and risk.
   - Then:
     - Ask which one they want to sponsor now.

3. **Problem-first reframe**
   - “Instead of adding these 5 fields, let’s step back. What decisions are you trying to drive?”
   - Suggest:
     - A simpler or more robust design aligned to the real problem.

4. **Deferral with clarity**
   - “This is valid, but based on current priorities I’d slot this for after [milestone]. I’ll document it and link it to [initiative].”
   - Key:
     - The requestor feels heard and knows where the request sits.

5. **Escalation path**
   - “If you feel this should jump the queue, I’m okay with that as long as [sponsor/leader] agrees we’ll drop X to make room.”
   - Puts:
     - Prioritization responsibility at the right level.

---

### 13.9.8 How I handle conflicting priorities and loudest-voice problems

When multiple stakeholders pull in different directions:

- **Unify by problem space**
  - Group requests by:
    - Domain (advising, portal UX, integration health),
    - Audience (students, staff, providers),
    - Initiative (term start, new program, compliance).
  - Present:
    - “Here’s the cluster of issues in each domain.”

- **Use simple prioritization frameworks**
  - Even if informal:
    - Impact vs Effort,
    - Must-have vs Nice-to-have,
    - Risk reduction vs shiny new features.
  - Sometimes:
    - RICE/WSJF-like thinking without saying the acronyms out loud.

- **Make dependencies visible**
  - Show:
    - “We can’t safely do B before A, or else we risk X.”
  - This often:
    - Helps stakeholders see why their request isn’t plug-and-play.

- **Let leadership choose**
  - I give:
    - Clear options and consequences.
  - They decide:
    - Which pain they’re willing to live with now vs later.

---

### 13.9.9 Structuring work: tickets, epics, and acceptance criteria

Once a change is accepted, I shape it into something buildable:

- **Epics / work streams**
  - Large bodies of work:
    - New portal flows,
    - Major integration redesigns,
    - Data model overhauls.
  - Break into:
    - Smaller tickets: “admin-config”, “Apex/LWC”, “integration”, “data cleanup”, “reporting”.

- **Good tickets contain:**
  - Problem statement:
    - “Today users cannot X; this causes Y.”
  - Scope:
    - What’s in / out.
  - Acceptance criteria:
    - Specific conditions observable by testers/business.
  - Dependencies:
    - Other tickets, systems, data migrations.

- **Definition of Done (DoD) in my head**
  - Code/config deployed to prod.
  - Tests updated/passing.
  - Integration/jobs updated if impacted.
  - Documentation and/or runbook updated.
  - Users and support know how to use it / what changed.
  - Monitoring/logging updated if needed.

If these aren’t met, I don’t consider the change “done”, just “deployed.”

---

### 13.9.10 Guarding against scope creep mid-implementation

Once something is in-flight:

- **Freeze scope per release**
  - If new asks appear:
    - “That’s valid, but we’ll treat it as Phase 2 unless it’s fixing a defect in what we’re building now.”

- **Distinguish bugs vs new requirements**
  - Bugs:
    - Implementation doesn’t match agreed design.
  - New requirements:
    - Stakeholder changed their mind or discovered extra needs.
  - Response:
    - Fix bugs inside the release;
    - Put new requirements into backlog.

- **Avoid “while you’re in there…” traps**
  - If “small extras” keep appearing:
    - I list them and negotiate priority:
      - Either bundle them if truly tiny,
      - Or push to a later ticket.

---

### 13.9.11 Closing the loop: how I communicate decisions

I don’t let decisions live only in my head:

- **Written responses**
  - In tickets or email:
    - Summarize:
      - Problem as I understand it.
      - Options discussed.
      - Decision taken.
  - That becomes:
    - The record everyone can refer back to later.

- **Meeting summaries**
  - For big topics (user model, provider licensing, SSO flows):
    - Follow up with:
      - Bullet-point recap of decisions.
      - Open questions still unresolved.
  - Helps:
    - Prevent “we never agreed to that” later.

- **Visibility on backlog status**
  - I try to make:
    - Backlog or roadmap visible to key stakeholders:
      - “Your request is in this release / planned for after X / parking lot.”
  - People:
    - Get less frustrated when they can see where they stand.

---

### 13.9.12 Change intake & triage anti-patterns I avoid

Patterns I treat as red flags:

- **Building exactly what was asked, without asking why**
  - Leads to:
    - Cluttered data models,
    - Conflicting automations,
    - Fragile integrations.

- **Letting urgency override architecture every time**
  - Everything becomes a “critical” change.
  - Leads to:
    - Layered hacks and emergency patches that never get cleaned.

- **Mixing bugs and features in the same change**
  - Makes it:
    - Hard to know what caused a regression.
    - Hard to roll back safely.

- **Prioritizing by loudest voice**
  - Ignoring:
    - Quiet but high-impact issues (e.g., integration errors, data integrity problems).

- **Invisible “side work”**
  - Doing:
    - Lots of untracked analysis, data fixes, or config changes that never hit the backlog.

---

### 13.9.13 To Validate – Change Intake & Triage specifics

As you compile this into your knowledge base, you’ll want to clarify:

- **Your explicit priority framework**
  - Decide:
    - Whether you want to document something like:
      - A simple Impact/Effort scale,
      - Or a RICE/WSJF-style approach.
  - Write:
    - The exact definitions you’ll use (e.g., what “High impact” means).

- **Standard template for new requests**
  - Define:
    - Fields you want for every request:
      - Problem statement,
      - Impacted users/processes,
      - Urgency,
      - Dependencies,
      - Success criteria.
  - This can live:
    - In your ticket system as a custom form.

- **Rules for hotfix vs backlog**
  - Decide:
    - Which conditions automatically move something to “hotfix” lane (data corruption, downtime, compliance).
    - Which must go through the standard backlog.

- **Communication norms**
  - Write:
    - A short internal guide for yourself:
      - How you’ll phrase “no / not now”.
      - How you’ll document decisions (ticket comments, email format, etc.).

- **Parking lot / idea backlog**
  - Decide:
    - How you’ll store and periodically review “not now” ideas.
    - So they don’t simply vanish, but also don’t clog the main backlog.

## 13.10 Governance, Design Reviews & Architecture Decision-Making – How I Keep Chaos Manageable

### 13.10.1 How I see my architecture role (guardrails, not gatekeeping)

In my head, “architect” is not “the person who says no”; it’s:

- **Translator**
  - Business → data model, flows, integrations.
  - Policies and constraints → technical patterns.

- **Risk manager**
  - Spot where something will blow up:
    - Data volume,
    - Security/PII,
    - Multi-agency boundaries,
    - Integration fragility.

- **Framework provider**
  - Give teams:
    - Patterns,
    - Reference designs,
    - Naming conventions,
    - Checklists.
  - So they can move fast **without** reinventing fundamentals every time.

Mental rule:

> “My success is when teams can make 80% of decisions themselves because the guardrails are clear — and only pull me in for the 20% that really matter.”

---

### 13.10.2 When something “needs architecture” (and not just a quick solution)

I don’t drag everything into a big architecture review. My triggers:

- **Cross-system impact**
  - Salesforce + one or more:
    - SIS/ERP,
    - Payment/benefits engine,
    - Document generation/notice system,
    - Identity provider.
- **New data model elements**
  - New objects or major relationships.
  - Anything involving:
    - Canonical IDs,
    - “System of record” questions,
    - Historical data.

- **Security/scope impact**
  - New external users (portal/community).
  - New agency/partner onboarding.
  - Changes to SSO, profiles, or permission set strategy.

- **High-volume or performance-sensitive features**
  - Large data loads,
  - Batch jobs,
  - Search-heavy flows,
  - Anything that could hit governor limits or DB scalability.

- **“One-way door” changes**
  - Things that are hard to undo:
    - Data migrations,
    - Splitting/merging record types,
    - Switching integration patterns (polling → events, etc.).

If it trips one of these, it’s not “just a ticket” — it’s an architecture topic.

---

### 13.10.3 Inputs I collect before making a decision

Before deciding anything important, I mentally run through these lenses:

- **Business lens**
  - Who is the user (student, advisor, case worker, provider, back-office)?
  - What is the business outcome?
  - Is this a “nice UX upgrade” or tied to:
    - Funding,
    - Policy changes,
    - Regulatory deadlines?

- **Data lens**
  - What is the system of record?
  - Do we need:
    - New keys,
    - New canonical objects,
    - New historical tracking?
  - How will this look at 10x data volume?

- **Integration lens**
  - Which systems need to know about this?
  - Is the flow:
    - Salesforce → external,
    - External → Salesforce,
    - Or bi-directional?
  - Do we need:
    - Real time,
    - Near real time,
    - Batch?

- **Identity & access lens**
  - Which identities are involved:
    - External OIDC users,
    - Internal SAML users,
    - Service accounts/system users?
  - What is the sharing/visibility story?

- **Operations & support lens**
  - Who will support this at 2 AM when it fails?
  - What logs will they see?
  - Can they retry safely?
  - Can we explain this in an email or runbook that non-developers understand?

- **Compliance / risk lens**
  - Does this touch:
    - PII,
    - Sensitive financial or health data,
    - Government or institutional audit controls?
  - Do we need:
    - Extra approvals,
    - Data minimization,
    - Retention policies,
    - Encryption decisions?

I don’t always formalize each lens, but this is what shapes “yes/no/how”.

---

### 13.10.4 How I frame architecture options (not just “yes/no”)

For any non-trivial change, I prefer options with trade-offs:

- **Option A – Minimal / incremental**
  - Uses:
    - Existing data model,
    - Existing integration paths,
    - Small config or code changes.
  - Pros:
    - Faster, easier, less risk.
  - Cons:
    - Might not address root cause fully,
    - Could be a short-term patch.

- **Option B – Structural / long-term**
  - Introduces:
    - New objects,
    - New events or APIs,
    - Cleaner separation of concerns.
  - Pros:
    - Better for long-term scale and clarity.
  - Cons:
    - More effort,
    - Requires cross-team coordination.

- **Option C – “Do nothing now” with clear consequences**
  - Explicitly state:
    - What happens if we don’t change anything.
  - Sometimes:
    - This is the right call, as long as we acknowledge:
      - Risk,
      - Pain points,
      - Debt we’re accepting.

Mental pattern:

> “I don’t show up with a single ‘take it or leave it’ design. I show a small menu of realistic paths with consequences clearly spelled out.”

---

### 13.10.5 Design artifacts I actually use (not just buzzwords)

I don’t produce architecture fluff; I create artifacts that someone will **actually read/use**:

- **High-level context diagrams**
  - Boxes & arrows:
    - Salesforce clouds,
    - SIS/ERP,
    - Integration platform,
    - Identity providers,
    - Notice engines, etc.
  - Focus:
    - Which system initiates,
    - Direction of data/commands,
    - Trust boundaries.

- **Data flow diagrams / sequence diagrams**
  - Show:
    - Who calls whom in what order.
    - Where decisions happen (eligibility engine, Flow, Apex, external).
    - Where errors are surfaced/logged.

- **Logical data models / ERDs**
  - Focus on:
    - Key objects and relationships.
    - External IDs and “source of truth” per field.
  - Includes:
    - Which attributes are managed by which system.

- **FDD / TDD-style docs**
  - **Functional Design**:
    - What the feature does from user perspective.
    - Scenarios, rules, edge conditions.
  - **Technical Design**:
    - Objects/fields,
    - Apex classes, Flows, LWCs,
    - Integrations,
    - Batch jobs & schedules,
    - Error handling and logs.

- **Architecture Decision Records (ADRs) – lightweight or implicit**
  - A short note (in doc or ticket) that captures:
    - Context,
    - Options considered,
    - Decision,
    - Rationale,
    - Consequences.

The artifacts are as lightweight as they can be **while still being useful later**.

---

### 13.10.6 How I run design reviews (in practice)

A typical design review in my world has phases:

1. **Pre-review alignment**
   - Share:
     - Short design doc or summary ahead of time.
   - Highlight:
     - Decisions that are open vs already constrained (e.g., must use existing integration platform, must support existing IdP model).

2. **Walkthrough**
   - Start with:
     - Problem statement and constraints.
   - Then:
     - Show the high-level architecture first,
     - Dive into details only where needed (data model, flows, integrations).

3. **Challenge & refinement**
   - Invite:
     - Questions about failure modes, future growth, alternative patterns.
   - Specifically:
     - Ask about overlooked systems (legacy, reporting, security).
   - Adjust:
     - Where strong counterarguments land.

4. **Decision & follow-up**
   - Confirm:
     - What’s approved as-is.
     - What needs iteration.
   - Capture:
     - Action items, open questions, owners.
   - Update:
     - Design doc and tickets to match decisions.

I care less about “formal committees” and more about clear, recorded decisions with the right people in the room.

---

### 13.10.7 Working with external reviewers / advisors (e.g., enterprise architecture, consultants)

When external reviewers (enterprise architects, consultants, reviewers) are involved:

- **I separate two things:**
  - Validation of **principles**:
    - Zero trust, data minimization, event-driven design, etc.
  - Implementation **details**:
    - Exact object names, specific Flow vs Apex choice.

- **I frame my proposals this way:**
  - “This is how we’re meeting your principles:
    - Data stays in region X.
    - Sensitive fields are encrypted / masked.
    - We use events instead of polling where possible.
    - We centralize logs for audit.”
  - Then:
    - Show concrete flows and diagrams.

- **I use them as a sanity check, not as pure gatekeepers**
  - If they:
    - Raise a concern aligned to valid risk, I adjust.
  - If they:
    - Push generic patterns that don’t match the project realities, I:
      - Explain the trade-offs,
      - Document carefully why we’re choosing differently.

---

### 13.10.8 Security, compliance & data governance inside architecture decisions

For public-sector and higher-ed work, security/governance is not optional decoration:

- **Data classification**
  - Identify:
    - What fields contain PII,
    - Sensitive financial or benefit-related info,
    - Academic history, etc.
  - Ensure:
    - Profiles/permission sets and sharing rules reflect who really needs it.

- **Access model**
  - Decide:
    - Org-wide defaults,
    - Role hierarchy use,
    - Portal sharing sets and account relationships.
  - Make sure:
    - External users see only their own data or data they’re allowed to see.

- **Audit & traceability**
  - Ensure:
    - Critical changes can be traced:
      - Who changed eligibility,
      - Who updated a program/benefit decision,
      - Which system changed what and when.
  - Use:
    - Field history,
    - Custom log objects,
    - External log platforms if needed.

- **Compliance frameworks**
  - Think in terms of:
    - Data residency,
    - Encryption in transit and at rest,
    - Logging retention,
    - Least privilege.
  - Make sure:
    - Technical decisions (which cloud, which integration path) align with policies.

- **System users**
  - Dedicated integration users for:
    - ETL,
    - Notice engines,
    - External portals.
  - Never:
    - Use personal accounts for system processes.
  - Give them:
    - Least possible permissions and explicit license planning.

---

### 13.10.9 Multi-org / multi-agency / multi-tenant governance

In multi-agency or multi-tenant scenarios on Salesforce:

- **Tenant boundaries**
  - Decide:
    - Single org with strict record-level partitioning,
    - Or multiple orgs with integration between them.
  - Consider:
    - Data sharing requirements,
    - Admin autonomy,
    - Compliance boundaries.

- **Licensing & user segmentation**
  - Distinguish:
    - Internal staff,
    - Partner/providing org staff,
    - End-clients/students/beneficiaries.
  - Design:
    - License strategy and user models that don’t explode costs.

- **Configuration & customization boundaries**
  - Avoid:
    - Each agency or school getting bespoke objects/fields that break the core model.
  - Instead:
    - Use:
      - Generic models with config-driven variations (record types, field sets, metadata).

- **Governance forums**
  - For multi-stakeholder contexts:
    - Regular architecture or design syncs where:
      - Changes affecting multiple groups are discussed,
      - Conflicting requests are reconciled.

---

### 13.10.10 Managing implementation teams through governance (vendors, offshore, internal devs)

I use governance to align multiple teams, not micromanage them:

- **Guardrails instead of micromanaging every line of code**
  - Provide:
    - Naming conventions,
    - “Do / Don’t” lists,
    - Example patterns for:
      - Apex,
      - Flows,
      - LWCs,
      - Integrations.
  - Expect:
    - Teams to follow these unless there’s a compelling reason not to.

- **Design reviews for major changes**
  - Require:
    - Short design docs or diagrams for:
      - New objects and flows,
      - New integrations,
      - New portals.
  - Review:
    - Once at design time, not after everything is built.

- **Code & config review**
  - Spot check:
    - Apex classes for bulkification & error handling.
    - Flows for:
      - Sprawl,
      - Fault connectors,
      - Hard-coded values.
    - LWCs for:
      - Data access patterns,
      - Accessibility basics.
  - Raise:
    - Specific issues (“this will fail in bulk”, “this breaks multi-tenant sharing”) with examples.

- **Feedback loops**
  - Use:
    - Tickets, comments, and recurring syncs.
  - Reward good patterns:
    - Document them as “golden examples”.

---

### 13.10.11 Architecture principles I actually use (my “core rules”)

I have a small set of principles I keep coming back to:

1. **Data has a home**
   - Every piece of data has:
     - A system of record,
     - A clearly owned lifecycle.
   - Salesforce:
     - Is not automatically the source of truth for everything.

2. **Make the data model boring**
   - Prefer:
     - Clear, normalized objects.
   - Avoid:
     - Overloaded fields,
     - Many meanings crammed into one picklist or text field.

3. **Events & logs over hidden side effects**
   - Prefer:
     - Explicit Platform Events, log entries, or status fields.
   - Avoid:
     - Magic behind-the-scenes updates nobody can trace.

4. **Config first, code when necessary**
   - Use:
     - Flows, declarative sharing, and metadata where they fit.
   - Use Apex:
     - For complexity, performance, reuse, integrations.

5. **Observability from day one**
   - Build:
     - Error logs,
     - Integration logs,
     - Dashboards.
   - Not:
     - “We’ll add logging later”.

6. **User experience is part of architecture**
   - Bad UX:
     - Causes incorrect data, workarounds, and shadow systems.
   - LWCs and Experience Cloud:
     - Are design choices, not afterthoughts.

7. **Prefer reversible changes**
   - If possible:
     - Implement things in ways that can be turned off or rolled back.
   - Use:
     - Feature toggles,
     - Additive schemas instead of destructive changes.

---

### 13.10.12 Handling disagreements & escalations

Architecture is political as well as technical. My patterns:

- **Strong opinions, weakly held**
  - Come in with:
    - A clear recommendation.
  - Be willing to:
    - Change when better arguments or constraints appear.

- **Separate “we can’t” from “we shouldn’t”**
  - “We can’t”:
    - Platform limits, legal/compliance barriers.
  - “We shouldn’t”:
    - Bad trade-off, but technically possible.
  - Be honest about which it is.

- **Make trade-offs explicit**
  - Instead of “this is bad,” say:
    - “If we do it this way, here’s what we gain, here’s what we lose.”

- **Escalate the decision, not the conflict**
  - When stakeholders disagree:
    - Summarize:
      - Options,
      - Risks,
      - Impacts.
    - Ask:
      - Higher-level sponsor to choose.

- **Document dissent**
  - If decision goes against recommendation:
    - Record:
      - What was recommended,
      - What was decided,
      - Why.
  - So:
    - Future revisits have context.

---

### 13.10.13 Governance & architecture anti-patterns I avoid

Things I consider dangerous:

- **Architecture by accident**
  - Letting patterns emerge from random ticket work with no overall view.

- **Architecture theater**
  - Big diagrams and committees with:
    - No impact on actual implementation choices.
  - I push:
    - For concrete, enforceable guardrails.

- **“One-size-fits-all” patterns**
  - Forcing:
    - Every use case into the same template.
  - Instead:
    - Reuse patterns where they fit,
    - Adapt where realities differ.

- **Ignoring operations**
  - Designs that:
    - Look clean on paper,
    - But are impossible to observe, debug, or support.

- **No feedback from actual users**
  - Architecture decisions made in a vacuum without:
    - Advisors,
    - Case workers,
    - Students/clients,
    - Integration teams.

---

### 13.10.14 To Validate – Governance & Decision-Making specifics

When you compile this into your knowledge base, you’ll want to fill in:

- **Your explicit architecture principles**
  - Turn your internal rules into a short, written list:
    - E.g., “Data has a home”, “Make data model boring”, “Observability from day one”.
  - These become:
    - Your personal “constitution” for designs.

- **Standard design doc templates**
  - Decide:
    - Minimal fields you want in:
      - Functional design notes,
      - Technical design notes,
      - ADR-style records.
  - You can keep them:
    - Lightweight but consistent.

- **Formal vs informal review triggers**
  - Document:
    - Which changes always require a design review:
      - New external integrations,
      - New portals,
      - New core objects,
      - SSO / identity changes.
    - Which can proceed with informal guidance.

- **Governance forums**
  - Define:
    - Which recurring meetings are de facto architecture forums:
      - Cross-team syncs,
      - Integration councils,
      - Portal governance groups.
  - Clarify:
    - What types of decisions get made where.

- **Decision log**
  - Choose:
    - Where your architecture decisions live:
      - A shared doc,
      - Repo folder,
      - Ticket tags.
  - Make it:
    - Searchable for “why did we choose this pattern?” years later.

## 13.11 Documentation & Knowledge-Sharing – How I Actually Keep Brains in Sync

### 13.11.1 How I think about documentation

I don’t write docs “because process says so.” In my head documentation is:

- **A memory prosthetic**
  - So future-me doesn’t have to reverse-engineer:
    - Why a flow exists,
    - What a field means,
    - How an integration is supposed to behave.

- **A coordination tool**
  - So:
    - Admins,
    - Developers,
    - Integration teams,
    - Business owners  
    can all look at the same page and not argue about “what we meant”.

- **A safety mechanism**
  - So when something breaks:
    - There’s a runbook.
    - There’s a diagram.
    - There’s an email with decisions and tradeoffs.

Mental rule:

> “If I had to explain this to a new admin in 6 months, what would I wish existed? That’s the doc I try to write.”

---

### 13.11.2 The main doc types I actually use

I’m not trying to build a library of 50 formats. In practice I keep reusing:

1. **Meeting notes / recap**
   - Structured bullet summaries:
     - Context,
     - Decisions,
     - Open items,
     - Next steps / owners.
   - Often sent by email or stored in a shared doc.

2. **Stakeholder emails (mini design + decisions)**
   - Clear subject + short intro.
   - Bullets for:
     - What I reviewed,
     - Questions,
     - Options,
     - Final ask.

3. **Tickets (Jira / Service desk / similar)**
   - Used as:
     - The spine of work – bugs, enhancements, tasks.
   - Fields:
     - Problem statement,
     - Environment,
     - Impact,
     - Steps to reproduce,
     - Acceptance criteria.

4. **Functional Design Notes (FDD-style)**
   - For features large enough to matter:
     - New flows,
     - Experience Cloud changes,
     - Integration behavior.
   - Contains:
     - Business context,
     - User stories / scenarios,
     - Rules and edge cases.

5. **Technical Design Notes (TDD-style)**
   - For the same features:
     - How we actually implement.
   - Contains:
     - Objects/fields,
     - Flows/Apex/LWCs,
     - Integrations,
     - Error handling,
     - Schedules/jobs.

6. **Runbooks / SOPs**
   - For recurring or sensitive activities:
     - Sandbox refresh steps,
     - Integration connectivity tests,
     - Data fix procedures,
     - Release steps.
   - Step-by-step, checklist style.

7. **Architecture / integration diagrams**
   - Diagrams for:
     - “How Salesforce talks to SIS/ERP/benefit systems.”
     - User login flows (portal, internal).
     - Major data flows and events.

8. **How-to guides for admins/support**
   - Short “do this, not that” docs:
     - How to re-run a sync.
     - How to troubleshoot a Flow error.
     - How to interpret certain fields and statuses.

I reuse these patterns rather than inventing new doc types every time.

---

### 13.11.3 Meeting notes / recap patterns (what I actually do)

From how I’ve written and refined notes, my pattern is:

- **Clear subject + one-line purpose**
  - Subject line that tells people:
    - Topic + angle, e.g. “User Migration – Clarifications on Data Model & Login Handler”.
  - Opening sentence:
    - “Here’s a summary of what we discussed and what’s still open.”

- **Structured sections**
  - I naturally segment as:
    - Context / background,
    - Updates,
    - Decisions,
    - Open items,
    - Next steps / owners.
  - Use:
    - Numbered sections with headings or bold labels.

- **Bulleted, not paragraphs**
  - Users don’t want an essay.
  - I write:
    - Short bullets that can be scanned quickly.
  - Example structure:

    - Context:
      - Why were we in the meeting?
    - Updates:
      - Facts, what we learned.
    - Decisions:
      - Bullet list of decisions with minimal words.
    - Open Items:
      - The things nobody answered.
    - Next Steps:
      - Who does what by when (if known).

- **Crisp formatting**
  - I care about:
    - Removing extra blank lines and random separators when they reduce clarity.
  - I adjust:
    - When notes look cluttered or lose structure.

These notes often double as **lightweight design docs** for small topics.

---

### 13.11.4 Stakeholder email patterns

I consistently use emails as mini-design and decision vehicles:

- **Subject line with action or topic**
  - Examples (anonymized pattern):
    - “Task Automation Priorities – Need Your Input”
    - “User Migration – Questions on Client vs Vendor Model”
    - “Provider User Creation – License Strategy Options”
  - Goal:
    - Recipient knows what this is *about* before opening.

- **Lead with what I’ve done**
  - Pattern:
    - “I went through X (the doc, the plan, the login flows)…”
  - Shows:
    - This isn’t a vague ask; it’s based on specific review.

- **Organize with bullets**
  - Questions and decisions are organized in bullet lists:
    - Q1, Q2, Q3 with short explanations.
  - I often:
    - Ask multiple related questions together so people see the full picture.

- **Reference tickets / IDs cleanly**
  - I include:
    - Ticket IDs / incident numbers when relevant,
    - But avoid burying them in long text.
  - Pattern:
    - “This email covers:
      - Task Automation – Ticket A
      - Incorrect Advisor Workflow – Ticket B
      - Trimester Task Flow – Ticket C”

- **Tone: direct but not stiff**
  - I tend to:
    - Avoid overly formal corporate-speak.
    - Use:
      - Clear, conversational language.
  - When unsure:
    - I ask to “double check priorities” rather than telling people what they must do.

Emails like this become:
- A **record of decisions**.
- A **reference** when someone asks “why did we do X?”.

---

### 13.11.5 Ticket writing patterns (incidents, work orders, tasks)

From how I’ve framed issues, a typical “good ticket” for me includes:

- **Problem statement**
  - “We have an issue where leads of type X converted into contacts are now failing Y process with error Z.”
  - Includes:
    - Brief background,
    - Where/when it started happening (“Started around [approx date]”).

- **Environment**
  - Specify:
    - Org (sandbox vs production),
    - System context (Salesforce vs integration vs SIS).

- **Evidence**
  - Links:
    - Record links (anonymized in docs when needed).
  - Queries:
    - SOQL queries to reproduce:
      - “Run this to find other affected records…”
  - Screenshots or error snippets when useful.

- **Impact**
  - Who is affected:
    - All admins,
    - All users of X process,
    - Only a specific queue, etc.
  - What the impact is:
    - Cannot convert leads,
    - Data corruption risk,
    - Reports/analytics wrong, etc.

- **What I already tried**
  - That I:
    - Checked logs,
    - Reviewed history records,
    - Looked at integration history,
    - Verified configuration.

- **Ask / outcome needed**
  - “Need vendor to confirm why record type is required and how to align our config.”
  - “Need decision: follow Option 1 vs Option 2 for provider user creation.”

This is true for:
- External vendor tickets,
- Internal IT tickets,
- REACT / incident / work order style requests.

---

### 13.11.6 Functional design notes (what I actually capture)

When a feature is non-trivial, my FDD-style notes tend to cover:

- **Problem / Background**
  - What process we’re improving:
    - e.g., automation of tasks, new advisor flows, migration of users, portal login model.
  - Current issues:
    - Manual steps,
    - Confusion,
    - Errors.

- **In-Scope / Out-of-Scope**
  - Clearly note:
    - What this change will handle.
    - What it will not touch (other objects, other flows, other user groups).

- **User stories / scenarios**
  - E.g.:
    - “As an advisor, when a student changes program, I need X so I can Y without manually…”
    - “As a vendor staff user, when I log in from external IdP, the system should Z…”
  - Scenarios:
    - Happy path,
    - Variation by program/role,
    - Edge cases (missing data, optional fields).

- **Business rules**
  - Clear bullet list:
    - When tasks should be created,
    - Who owns them,
    - Status transitions,
    - Conditions for sending notices/emails.

- **UX expectations**
  - For portal or internal:
    - What screens appear,
    - What fields are required,
    - What messages are shown on success/failure.

These functional notes are written in **plain language** so non-technical stakeholders can sign off.

---

### 13.11.7 Technical design notes (how I capture the actual build)

For the same feature, my technical notes typically include:

- **Objects & fields**
  - Which objects:
    - Are created,
    - Are updated,
    - Are new vs existing.
  - Include:
    - API names,
    - Field types,
    - External IDs,
    - Relationships (lookup/master-detail).

- **Automation**
  - Flows:
    - Names,
    - Triggers (record-triggered vs subflow vs screen flow),
    - Key decisions and paths.
  - Apex:
    - Class names,
    - Responsibilities (service vs selector vs integration),
    - Key methods and signatures.

- **Integrations**
  - Which integration platform is involved (if any).
  - How data flows:
    - Salesforce → integration → external,
    - External → integration → Salesforce.
  - Field mapping highlights.

- **Permissions & sharing**
  - Which profiles/permission sets:
    - Need update.
  - New permission sets or roles:
    - For new user types or system users.

- **Error handling & logging**
  - Which log objects or mechanisms are used.
  - What gets recorded on failure:
    - Error types,
    - Correlation IDs,
    - Impacted record IDs.

- **Jobs / schedules**
  - Any new batch or scheduled jobs:
    - Names,
    - Frequencies,
    - Dependencies.

This is where I make sure **no important behavior lives only in someone’s head**.

---

### 13.11.8 Runbooks / SOP patterns (how I write “do this when X happens”)

For operational work, I like checklist-style docs:

- **Purpose & scope**
  - “This runbook explains how to…”
    - Refresh a sandbox,
    - Run the nightly SIS backfill manually,
    - Rebuild an integration connection,
    - Validate SSO configuration.

- **Preconditions**
  - What you need:
    - Access level,
    - Tools,
    - Credentials (integration user, VPN, etc.).

- **Step-by-step**
  - Numbered steps:
    - Clear, unambiguous actions (click here, run this query, check this dashboard).
  - Screenshots if necessary (but not always).

- **Expected results**
  - At key points:
    - “After step 4, you should see X…”
    - “If Y is not visible, stop and escalate.”

- **Failure handling**
  - Sections like:
    - “If you see error A: try B.”
    - “If this fails twice: log ticket and do C.”

- **Post-conditions**
  - What to verify after completion:
    - Check certain records,
    - Confirm logs are healthy,
    - Ensure jobs are rescheduled/enabled.

These runbooks are for:
- Myself,
- Other admins,
- Integration/ops team members who handle day-to-day issues.

---

### 13.11.9 Architecture & diagram patterns

When I draw diagrams, I’m usually aiming to answer:

- **“Who talks to whom, and when?”**
- **“Where does this piece of data start and end?”**
- **“What are the trust boundaries?”**

Patterns I use:

- **Context diagrams**
  - Salesforce as one box with internal modules:
    - Core CRM,
    - Portal,
    - Case management,
    - Integrations.
  - Surrounding systems:
    - SIS/ERP,
    - IdPs,
    - ETL, analytics, notice engines, etc.
  - Arrows:
    - Label direction and type (events, APIs, batch).

- **Sequence / flow diagrams**
  - For:
    - User login flows (portal, provider staff, internal staff).
    - Application/benefit workflows end-to-end.
  - Show:
    - Time order,
    - Who calls what,
    - Where decisions happen.

- **Logical data model diagrams**
  - Highlight:
    - Objects (student/person, program, application, case, notice, transaction, enrollment).
  - Show:
    - One-to-many and many-to-many relationships,
    - External IDs and links to external systems.

Diagrams are not art; they are **debug tools for humans**.

---

### 13.11.10 Knowledge sharing with admins & business users

I don’t hoard knowledge; I try to distribute it:

- **Admin-focused explanation**
  - When handing over:
    - New flows,
    - New fields,
    - New permission models,
  - I explain:
    - Why we did it this way,
    - What’s safe to change,
    - What should be escalated before touching.

- **Business-focused explanation**
  - For new processes:
    - Brief guides or short screen-recordings (if available) explaining:
      - Where to click,
      - What fields mean,
      - How edge cases are handled.
  - Tie:
    - Changes back to business language (“students”, “cases”, “benefits”), not sysadmin language.

- **Reinforcing via emails and tickets**
  - Many of my written communications:
    - Double as informal documentation.
  - Example:
    - An email to clarify migration or login behavior becomes:
      - A reference doc for later.

---

### 13.11.11 Documentation anti-patterns I avoid

There are doc behaviors I implicitly treat as red flags:

- **Wall-of-text emails**
  - Hard to parse, nobody reads them fully.
  - I prefer:
    - Structured bullets and headings.

- **Docs with no decision captured**
  - Meeting notes that:
    - List discussion but never say what we decided.
  - I try to always include:
    - A “Decisions” section.

- **Out-of-date diagrams / docs**
  - Diagrams:
    - That no longer match reality.
  - I prefer:
    - A small set of current diagrams rather than a huge library of stale ones.

- **Over-formal templates that block speed**
  - 10-page templates for small changes.
  - I choose:
    - Lightweight structure that’s fast to fill but consistent.

- **Everything stuck in chat**
  - Important decisions that exist only in Slack/Teams.
  - I try to:
    - Extract those into tickets or email recaps.

---

### 13.11.12 How I use AI / tools to assist documentation

Practically:

- **Drafting emails & summaries**
  - Use:
    - AI to generate first pass of:
      - Meeting summaries,
      - Emails,
      - Clarification questions.
  - Then:
    - Edit to match my tone and context.

- **Turning scattered notes into structured docs**
  - Take:
    - Chat history, bullet notes, raw thoughts,
  - And convert into:
    - Proper FDD/TDD sections,
    - Runbooks,
    - Tickets with clear fields.

- **Rewriting for clarity / brevity**
  - Especially:
    - When something feels too formal or too long.
  - I aim for:
    - “Crisp, clean, minimal fluff” docs.

I treat tools as **accelerators**, not as replacements for judgment.

---

### 13.11.13 To Validate – Documentation & Knowledge-Sharing specifics

As you assemble this into your knowledge base, it’s worth explicitly defining:

- **Your standard “one-pager” template**
  - For small/medium features:
    - Problem,
    - Scope,
    - Scenarios,
    - Design summary,
    - Decisions,
    - Open questions.
  - Use this instead of reinventing each time.

- **Your preferred folder / repo structure**
  - Where FDD/TDD live relative to:
    - Code,
    - Runbooks,
    - Diagrams.
  - E.g.:
    - `/docs/functional/…`
    - `/docs/technical/…`
    - `/docs/runbooks/…`
    - `/docs/diagrams/…`

- **Runbook index**
  - A simple index that lists:
    - Critical runbooks (sandbox refresh, user migration, major integrations).
    - Links and owners.

- **Decision log conventions**
  - How you capture:
    - Decision summaries in tickets/emails.
  - E.g.:
    - A tag `[Decision]` or a short ADR entry per major topic.

- **Hand-off / onboarding packet**
  - For new admins/devs on a project:
    - A small bundle of:
      - Key docs and diagrams,
      - Top 5 runbooks,
      - One-pager on environments and release flow.

These make your documentation system **coherent, findable, and sustainable**, instead of a random pile of good intentions.

## 13.12 Career Positioning, Mentoring & How I Explain My Work

### 13.12.1 How I actually think about my role (beyond “Salesforce dev”)

In my own head, I’m not “just a Salesforce Developer” or “just an Admin.” I sit at the intersection of:

- **Architecture**
  - I design:
    - Data models,
    - Integration patterns,
    - Identity flows,
    - Environment/release strategies.
  - I care about:
    - System of record boundaries,
    - Long-term maintainability,
    - Observability and error handling.

- **Delivery**
  - I still:
    - Build Flows, Apex, LWCs,
    - Design Boomi processes,
    - Tune report/dashboards,
    - Do data migrations and backfills.
  - I’m comfortable:
    - Owning a feature end-to-end from idea → prod.

- **Business translation**
  - I spend a lot of time:
    - Writing emails,
    - Framing options,
    - Clarifying requirements,
    - Pushing back when a request is a band-aid vs a solution.

- **Mentoring / pattern-setting**
  - I create:
    - Patterns, templates, naming conventions,
    - “Do this, not that” guidelines for admins/devs.
  - I make:
    - My thinking explicit so others can be consistent.

Mental position:

> “I’m a Salesforce-centered architect who can still ship real features, but my real leverage is in patterns, decisions, and how I help others think about the system.”

---

### 13.12.2 How I “package” my experience (when telling my story)

When I explain my career to someone (hiring manager, peer, stakeholder), I don’t list random tasks. I cluster around themes:

1. **Industry domains**
   - Public sector:
     - State-level public benefits / case management,
     - Multi-agency portals,
     - Government cloud / high-compliance environments.
   - Higher education:
     - Admissions and enrollment,
     - Student lifecycle,
     - Grants and academic programs.
   - Cross-domain:
     - Any place where Salesforce sits between people and legacy core systems.

2. **Technical pillars**
   - Salesforce platform:
     - Core, Service, Experience, industry clouds.
     - Flows, Apex, LWCs, permission models.
   - Integrations:
     - ETL/iPaaS,
     - Real-time vs batch,
     - Platform Events and CDC,
     - Legacy databases / SIS / ERP.
   - Identity & security:
     - External identity providers (OIDC),
     - Internal SAML,
     - Portal user models,
     - System users and access boundaries.

3. **Execution capabilities**
   - End-to-end delivery:
     - From intake → design → build → test → deploy → support.
   - Firefighting:
     - Incidents, data fixes, integration failures.
   - Long-term shaping:
     - Governance,
     - Documentation,
     - Architecture decisions,
     - Patterns others follow.

That’s how I turn “a thousand tickets” into a coherent story.

---

### 13.12.3 How I explain my value to non-technical stakeholders

When I talk to non-technical leaders, I don’t lead with “Flows and Apex.” I frame my value in terms they care about:

- **Reliability**
  - “I design things so that when a portal or integration fails, we:
    - Know about it quickly,
    - Understand what broke,
    - Have a way to recover without corrupting data.”

- **Scalability**
  - “I build in a way that works for:
    - 10K records now,
    - 10x that later,
    - Without rewriting everything.”

- **Risk reduction**
  - “I think about:
    - Where we can accidentally expose data,
    - Where we can make wrong eligibility/benefit decisions,
    - How to keep audit trails.”

- **Time-to-value**
  - “I balance:
    - Quick wins (config, simple automation),
    - With structural changes that prevent constant rework.”

- **Change-ready systems**
  - “I assume:
    - Programs, policies, and requirements will change.
    - So I build with:
      - Configuration,
      - Feature toggles,
      - Clear separation of concerns,
      - So we can actually adapt.”

Whenever possible, I translate “technical choice X” into “business risk/cost/benefit Y.”

---

### 13.12.4 How I mentor juniors / peers (what I actually emphasize)

When I help other admins/devs, I don’t just review syntax. I pull them toward patterns:

- **From “do this task” → “understand the pattern”**
  - Example:
    - If they’re building a Flow:
      - I talk about:
        - Fault paths,
        - Reusable subflows,
        - Hard-coding vs config,
        - Performance / bulk.
  - I show:
    - How this Flow fits into:
      - The object model,
      - Other automations,
      - Integrations.

- **Teach them to think in layers**
  - UI:
    - What user sees (page layouts, LWCs, portals).
  - Platform logic:
    - Flows, Apex, rules.
  - Integration:
    - How external systems are involved.
  - Data:
    - Keys, relationships, external IDs.

- **Name and repeat patterns**
  - I give patterns names in conversation:
    - “Error log pattern”,
    - “Staging object pattern”,
    - “Soft-delete flag pattern”,
    - “Single source-of-truth pattern”.
  - Once it’s named, they can:
    - Recognize when to use it again.

- **Model good communication**
  - I share:
    - Email drafts,
    - Ticket descriptions,
    - Meeting summaries,
  - So they see:
    - How to explain technical things clearly to non-technical people.

---

### 13.12.5 How I coach people to debug and reason, not just fix

When someone is stuck on a bug or design problem, I don’t just hand them the answer. I push their thinking:

- **Ask for their hypothesis**
  - “What do you think is happening?”
  - “What’s your current mental model?”

- **Push them to isolate**
  - “Can we reproduce this with:
    - One record?
    - A different user?
    - In a different environment?”
  - Teach:
    - The habit of narrowing variables.

- **Point them to the right tools**
  - For Salesforce:
    - Debug logs,
    - Flow error emails,
    - Developer console / query tools,
    - Field history / audit objects.
  - For integrations:
    - ETL logs,
    - Error objects,
    - External system logs.

- **Ask “what will you change in your mental model?”**
  - After they find the root cause:
    - “What did we learn about this system?”
    - “What will you do differently next time you design something like this?”

Mentoring for me = improving how they *think*, not just how they click.

---

### 13.12.6 How I approach public speaking / knowledge sharing (talks, workshops, etc.)

When I prepare content for others (internal workshops, community events), I gravitate toward:

- **Concrete, lived topics**
  - Real patterns:
    - Querying data safely at scale,
    - Integration strategies,
    - Portal adoption metrics,
    - Data migration pitfalls,
    - Governance and release discipline.

- **“From messy reality to structured pattern”**
  - Start with:
    - A messy real-world scenario (multiple systems, conflicting requirements).
  - Show:
    - How to tease out:
      - Objects,
      - Flows,
      - Integrations,
      - Logs.
  - End with:
    - Reusable checklists and designs.

- **Code + diagrams + emails**
  - I don’t treat:
    - Code and diagrams and emails as separate worlds.
  - Talks often show:
    - How a design appears:
      - In diagrams,
      - In Apex/Flows,
      - In status emails and ticket descriptions.

- **Actionable takeaways**
  - I like:
    - Checklists,
    - Patterns,
    - Templates,
  - So people leave with:
    - Things they can implement Monday morning, not just inspiration.

---

### 13.12.7 How I frame my “level” (senior/architect) without hype

I don’t claim I know everything. But there are signals that I operate at an architect/lead level:

- **I own cross-system designs**
  - I don’t just code to a spec:
    - I help define the spec across:
      - Salesforce,
      - Integration platforms,
      - Core systems,
      - Identity.

- **I think in time horizons**
  - Short:
    - Today’s incident, current sprint.
  - Medium:
    - Upcoming releases, migrations, adoption.
  - Long:
    - System shape 1–3 years out,
    - How patterns will scale.

- **I understand both user pain and technical risk**
  - I’m comfortable:
    - In meetings with:
      - Advisors/case workers,
      - Business owners,
      - Security/compliance,
      - Integration teams.
  - I can:
    - Translate constraints and concerns across those groups.

- **I leave patterns behind**
  - When I move on:
    - I leave:
      - Documents,
      - Runbooks,
      - Object models,
      - Integration patterns
    - That others can maintain and extend.

That’s “architect” in my practice: not a title, but behavior.

---

### 13.12.8 How I manage being multi-threaded (multiple projects, side ventures)

I don’t live in a single clean project:

- **Multiple client/org contexts**
  - Public sector:
    - High-governance, security-heavy, multi-agency stakeholder landscape.
  - Education:
    - Complex data and lifecycle, legacy SIS, admissions/governance.
  - Each:
    - Has its own environments, workflows, integrations.

- **Side venture mindset**
  - I’m also thinking like:
    - A founder/consultant:
      - Packaging patterns into reusable assets,
      - Thinking about positioning and offerings,
      - Tracking time and value.

- **Cognitive habits**
  - To survive this:
    - I rely heavily on:
      - Checklists,
      - Written notes,
      - Runbooks,
      - Consistent naming conventions,
      - Shared patterns (so I can mentally “port” patterns between projects).

This multi-threaded reality is part of why my documentation and pattern instincts are so strong.

---

### 13.12.9 How I future-proof my career (not just skills)

I don’t assume “Salesforce dev” as a static label forever. I invest in:

- **Conceptual skills**
  - System design,
  - Data modeling,
  - Integration architecture,
  - Identity & security thinking,
  - Observability / SRE-like habits.

- **Communication & influence**
  - Clear writing:
    - Emails, docs, runbooks,
    - Option framing and tradeoffs.
  - Meeting discipline:
    - Summaries,
    - Decisions,
    - Next steps.

- **Teaching & content**
  - Talks, guides, mentoring juniors.
  - This:
    - Reinforces my own patterns,
    - Builds reputation beyond any one org.

- **Versatility**
  - Experience with:
    - Government, higher-ed, and other regulated environments.
  - Strong in:
    - Both config and code,
    - Both Salesforce and integration platforms.

That way, I stay valuable whether:
- The project is Salesforce-first,
- Or Salesforce is just one piece of a bigger architecture.

---

### 13.12.10 How I’d explain “what I do” to a non-technical friend

If I strip away all jargon, it’s something like:

- “I work on software that sits in the middle between:
  - People (students, case workers, providers),
  - And the big old systems that store their data.

- I make sure:
  - The right people see the right information,
  - Processes are automated where it’s safe,
  - Different systems don’t contradict each other.

- I also:
  - Help teams choose how to build things,
  - Watch for ways the system might break,
  - And document everything so others can actually run and fix it later.”

That’s the human-level description of all these patterns.

---

### 13.12.11 To Validate – Career & Positioning specifics

As you build your knowledge base, you’ll want to explicitly spell out:

- **Your “headline”**
  - A one- or two-line description:
    - “Salesforce + Integration Architect focused on public-sector and higher-ed systems, with deep experience in portals, SIS/ERP integrations, and high-compliance environments.”
  - Calibrated for:
    - LinkedIn,
    - Conference bios,
    - Intros to new clients.

- **Your 3–5 signature patterns**
  - The things you are known for:
    - e.g., “error log architecture”, “high-volume SIS integrations”, “portal adoption metrics”, “governed release processes”.
  - These become:
    - Core bullets on resumes, proposals, talks.

- **Your mentoring & leadership story**
  - Concrete examples:
    - How you’ve helped teams avoid incidents,
    - How you’ve trained admins/juniors,
    - How you improved processes (not just code).

- **Your target future direction**
  - Decide if you want to lean more into:
    - Enterprise/solution architecture,
    - Product/platform ownership,
    - Independent consulting/fractional architect,
    - Or blended technical + business leadership.

- **Artifacts you want public vs private**
  - Which patterns to:
    - Turn into public talks/blogs.
  - Which:
    - Stay internal for specific clients/projects.

This section is the “meta layer” on top of all your technical patterns: how you present and use them to shape your career and opportunities.

## 13.13 System & Integration Users, Background Jobs & “Robots in the Org”

### 13.13.1 How I think about non-human access

I don’t treat system users as “just another admin account.” In my head they are:

- **Interfaces**, not people:
  - Each one represents a connection:
    - Integration platform,
    - Notice/letters engine,
    - Data warehouse,
    - External portal/back-end,
    - Logging/monitoring system.

- **Responsibility boundaries**:
  - “If this user did it, that means this system was involved.”
  - That’s how I trace who changed what from where.

- **Risk concentration points**:
  - They often have wide access.
  - If they’re misconfigured, they can:
    - Corrupt a lot of data,
    - Leak sensitive info,
    - Trigger massive email storms.

Mental rule:

> “Every non-human that touches Salesforce should have a named, intentional identity with minimum power and maximum traceability.”

---

### 13.13.2 The system user types I actually define

I don’t like “one big integration user.” I think in categories:

1. **Core integration users**
   - Owned by:
     - Integration platforms (ETL/iPaaS, API gateways).
   - Use cases:
     - SIS/ERP sync,
     - Grants/benefits engines,
     - Data warehouse feeds.
   - Pattern:
     - One per major integration domain, not one per every little flow.

2. **Messaging/notification users**
   - Owned by:
     - Systems that send messages on behalf of Salesforce:
       - Email relay,
       - Notice/letter systems,
       - Bulk comms tools (where native features aren’t used).
   - Purpose:
     - Separate messaging footprint from core data integrations.

3. **Logging/monitoring users** (if applicable)
   - For:
     - External logging tools,
     - API-based monitoring.
   - Often:
     - Read-only or limited to logs/diagnostic objects.

4. **Automation/system-process users**
   - For:
     - Internal batch jobs or flows that:
       - Should not run as a real end-user.
   - Example use:
     - Large backfills,
     - Maintenance jobs,
     - “Robot” workflows that must be clearly identifiable in field history.

5. **Support / break-glass users** (very restricted)
   - Rarely, in highly controlled orgs:
     - Special accounts used for emergency support scenarios.
   - Protected with:
     - Tight controls,
     - Additional approvals,
     - Strong auditing.

I’d rather have **4–8 clearly named system users** than one monster account doing everything.

---

### 13.13.3 Permission strategy for system users

I design permissions for system users with the same discipline as human ones:

- **Permission sets over profiles**
  - Base profile:
    - Very restrictive.
  - Permission sets:
    - Layered by capability:
      - “Read SIS Student Data”,
      - “Write Case Records for Integration X”,
      - “Access Custom Log Object”.
  - Benefits:
    - Easier to review,
    - Easier to adjust per system,
    - Reusable patterns.

- **Object/field-level specificity**
  - Avoid:
    - “System admin” style access unless truly required.
  - For each integration:
    - Precisely grant:
      - Objects,
      - CRUD operations,
      - Field-level access.
  - Especially careful with:
    - PII,
    - Financial or benefit-related fields,
    - Sensitive flags (fraud/risk, status decisions).

- **Login & IP restrictions (where policy allows)**
  - Use:
    - Login IP ranges,
    - Login hours,
    - SSO binding.
  - Idea:
    - System users should log in only:
      - From known integration endpoints,
      - Through known IDP / connection paths.

- **Scope by integration domain**
  - Example pattern:
    - SIS integration user:
      - Access to student/program/enrollment objects.
    - Notice engine user:
      - Access to notice objects + attachments, not everything else.
    - Warehouse feed user:
      - Read-only on broad set, no writes.

---

### 13.13.4 Integration user vs “feature service account” vs admin

I’m deliberate about **not** overloading one account:

- **Integration user**
  - Primary responsibility:
    - Moving data between Salesforce and external systems.
  - Behavior:
    - Appears in field history/logs when external changes occur.
  - Should not:
    - Own all records by default unless there’s a reason.

- **Feature service account**
  - Tied to:
    - Specific feature or product (e.g., dedupe tool, address verification, CTI).
  - Needs:
    - Access only to:
      - Objects and fields that tool needs.
  - Helps:
    - Quickly tell which tool did what in history logs.

- **Admin user**
  - Should not:
    - Be used for integrations or automation.
  - Reason:
    - When field history shows “Admin changed X”, you can’t tell if:
      - A person did it,
      - An automated job did it.
    - Audit and blame become messy.

Mental rule:

> “A system user should map to a system. If the system has a different job, it deserves a different user.”

---

### 13.13.5 Email, relay, and “who sent this communication?” patterns

Emails/notifications cause a lot of confusion if system identity is unclear:

- **Org-wide email addresses**
  - Use:
    - Generic but recognizable names:
      - “Student Services Portal”,
      - “Benefits Case Notifications”.
  - Avoid:
    - Sending from specific staff accounts for automated messages.

- **Separation of transactional vs marketing**
  - Transactional:
    - Status updates, case changes, decision notices.
    - Often from:
      - Support/portal/branded service address.
  - Marketing/engagement:
    - Should go through:
      - Proper marketing/automation platform identity.

- **DKIM/subdomain patterns (conceptually)**
  - Use:
    - Subdomains or branded domains for outbound email (where allowed).
  - Keep:
    - Config documented:
      - “This sub-domain is for transactional portal emails.”
      - “This one is for newsletters/marketing.”

- **Logging outbound communications**
  - For critical communications:
    - Store:
      - At least a record/log that:
        - This type of notice/communication was sent,
        - To which person,
        - By which system user.

The goal is:
- When someone asks:
  - “Why did I get this email?”  
- You can answer:
  - “Because system X, using user Y, triggered it at time Z.”

---

### 13.13.6 Background jobs & schedules – who runs what, and how

Background jobs (Apex, Flows, integration schedules) should not be ghosts:

- **Naming discipline**
  - Job names reflect:
    - Domain,
    - Direction,
    - Frequency.
  - Example pattern (conceptual):
    - `SIS_StudentSync_Nightly`,
    - `GrantAwards_Backfill_OneTime`,
    - `PortalActivity_Aggregation_Hourly`.

- **Ownership**
  - Logical owner:
    - A team or role (not just “who wrote it”).
  - Documentation:
    - Runbooks explaining what the job does,
    - Where it’s configured,
    - What happens if it fails.

- **Schedules by environment**
  - Different behaviors:
    - In DEV/QA:
      - Jobs often disabled or run manually.
    - In UAT/PERF:
      - Reduced frequency or synthetic data sets.
    - In PROD:
      - Full schedule with monitoring.

- **System user context**
  - Jobs run as:
    - Dedicated system user where possible.
  - This ensures:
    - All writes/updates are clearly attributed.

---

### 13.13.7 Credentials & secrets (tool-agnostic but principled)

I can’t store secrets for you, but I do have patterns:

- **No secrets in code**
  - Never:
    - Hard-code passwords,
    - API keys,
    - Tokens in Apex or LWC.
  - Use:
    - Secure config:
      - Named credentials / secure storage native to the platform.
      - External secret managers where integrated.

- **Rotate regularly**
  - Treat:
    - System user passwords and tokens like real secrets:
      - Rotation plans,
      - Documentation on rotation steps.

- **Least-knowledge principle**
  - Only a very small group:
    - Should know how to obtain/rotate certain secrets.
  - Scripts/runbooks:
    - Explain how, but not the actual secrets themselves.

- **Separation of duties**
  - Ideally:
    - People who design systems,
    - People who approve access,
    - People who hold certain secrets  
    are not all the same single person.

---

### 13.13.8 Audit & traceability for robots

If a robot does something important, I want a trail:

- **Field history & audit objects**
  - Use:
    - Field history tracking for key fields (status, owner, decision dates).
  - Add:
    - Custom audit objects for:
      - Important decisions,
      - Integration updates,
      - Notice generations.

- **Correlation IDs**
  - Where possible:
    - Pass correlation IDs through:
      - Integration calls,
      - Logs,
      - Events.
  - Store:
    - Correlation ID on Salesforce log records:
      - Link one external job run to many record updates.

- **Event logs for changes**
  - For sensitive operations:
    - Consider:
      - Platform Events or log objects that capture:
        - Who changed something,
        - Which system user,
        - Why (metadata like “job name / integration name”).

- **Replayability**
  - Design:
    - Integration flows with possibility to:
      - Re-play/re-process certain record sets using logs and IDs.
  - Helps:
    - When you need to recover from partial failures.

---

### 13.13.9 Managing system users across environments

System users in lower envs are not clones of prod users:

- **Different usernames & emails**
  - Use:
    - Environment-specific usernames/emails so:
      - Test environments never send email as real prod identities.
  - Sometimes:
    - Route sandbox emails to internal/test email addresses.

- **Different access levels**
  - DEV/QA:
    - System users may have:
      - Broader access because data is synthetic/anonymized.
  - PROD:
    - Tight, least-privilege access.

- **SSO vs local login**
  - For system users:
    - Decide whether to:
      - Use API-specific auth flows,
      - Or local login with IP restrictions (depending on policy).
  - Keep:
    - Config consistent *by pattern* across envs, even if credentials differ.

- **Refresh & cloning awareness**
  - After sandbox refresh:
    - Validate:
      - System user passwords/tokens are reset.
      - Named credentials point to lower env endpoints, not prod.
      - Automated jobs are reconfigured or disabled as needed.

---

### 13.13.10 System-user anti-patterns I avoid

There are patterns I consider dangerous:

- **Single “god” integration user that does everything**
  - Hard to:
    - Audit,
    - Limit,
    - Debug.
  - If that account is compromised:
    - The entire org is wide open.

- **Using human admin accounts as integration users**
  - When admin leaves:
    - Everything breaks or becomes un-auditable.
  - Field history is polluted:
    - You never know if a human or system made a change.

- **Excessively privileged system users**
  - Granting:
    - Modify All Data / View All Data as default.
  - This:
    - Increases blast radius of any bug or compromise.

- **No monitoring on system users**
  - No alerts if:
    - They suddenly start failing logins,
    - They trigger abnormal volumes of changes.

- **Copying prod system users into sandboxes without changes**
  - Risk:
    - Accidentally hitting real endpoints from sandbox jobs.
    - Sending test emails to real customers/beneficiaries.

---

### 13.13.11 To Validate – System & Robot Patterns

When you fold this into your knowledge base, it’s worth explicitly deciding:

- **Canonical system user inventory**
  - List:
    - Each system user type,
    - What system it represents,
    - What objects/operations it should perform.
  - This becomes:
    - A living register.

- **Standard naming convention**
  - Decide:
    - How system users are named:
      - e.g., `INT_SIS_SFDC`, `INT_PORTAL_SFDC`, `SYS_LOGGING_SFDC`.
  - Helps:
    - Anyone reading a log knows “who” that is.

- **Permission set library for robots**
  - Define:
    - A small set of reusable permission sets:
      - `PS_INT_SIS_ReadWrite`,
      - `PS_INT_NOTICE_ReadWrite`,
      - `PS_SYS_LOG_ReadOnly`.
  - Assign:
    - These to system users instead of ad-hoc access.

- **Runbook for system user lifecycle**
  - Create:
    - Steps to:
      - Create a new system user,
      - Grant the right permissions,
      - Configure SSO/auth,
      - Rotate credentials,
      - Decommission safely.

- **Monitoring rules**
  - Define:
    - What constitutes “suspicious” activity for a system user:
      - Too many failures,
      - Logins from unknown locations (if visible),
      - Unusual data volumes.
  - Connect:
    - Those rules to alerts/dashboards.

This section is the “robot management” layer of your org hygiene — a huge part of staying secure, auditable, and sane at scale.

## 13.15 SOQL & Data Access Patterns – How I Query Real, Messy Orgs

### 13.15.1 How I think about querying in Salesforce

In my head, SOQL isn’t “just how you fetch records”. It’s:

- **The main contract between logic and data**
  - Every Flow, Apex job, LWC, integration depends on good queries.

- **A performance & reliability lever**
  - Good queries:
    - Are selective,
    - Avoid row locks,
    - Respect sharing,
    - Don’t blow governor limits.
  - Bad queries:
    - Become invisible landmines that only explode at scale.

Mental rule:

> “If a query looks fine on 1000 records but would be scary on 10 million, it’s not ready.”

---

### 13.15.2 Baseline patterns: “scoping first, then filtering”

When I design a query, I don’t start with fields, I start with **scope**:

1. **Narrow the object**
   - Ask:
     - “Which subset of this object do I *really* need?”
   - Techniques:
     - Filter by:
       - Status,
       - Date ranges,
       - Owner / queue,
       - Type / record type.
   - Example mindset:
     - “Don’t query all incidents, query only open high-priority incidents changed in the last N days.”

2. **Use indexed and selective filters when possible**
   - Think about:
     - `Id`, `OwnerId`, `RecordTypeId`, `CreatedDate`, `LastModifiedDate`, standard lookups.
   - For custom fields:
     - Decide which ones should be indexed (e.g., cleanup flags, status flags, external IDs).

3. **Only then pick fields**
   - Retrieve:
     - Only fields needed for:
       - Display,
       - Logic,
       - Integration payloads.
   - Avoid:
     - Massive “SELECT *” behavior (pulling every field “just in case”).

This “scope → index → fields” mindset is true whether I’m writing Apex, helping Flow-driven queries, or designing SOQL for integration jobs.

---

### 13.15.3 Concrete SOQL patterns I actually use

A few patterns we *know* sit in your head:

#### a) Audit / diagnostic queries (history & logs)

- Example:
  - Contact / Lead history:
    - `SELECT Id, Field FROM ContactHistory WHERE (Field = 'contactCreatedFromLead' OR Field = 'created') ORDER BY CreatedDate DESC`
  - Purpose:
    - Answer questions like:
      - “Where did this record come from?”
      - “Was it converted from a lead?”
      - “When did this flag change?”

- Pattern:
  - Use history tables to reconstruct:
    - Pipelines,
    - Conversions,
    - Key field changes.

#### b) State-specific user queries

- Example you already use:
  - `SELECT Name, IsActive, LastLoginDate, Id, Username FROM User WHERE IsActive = TRUE AND Id IN (SELECT UserId FROM UserLogin WHERE IsFrozen = TRUE)`
  - Purpose:
    - Find:
      - Users who are active from a license perspective,
      - But login is frozen (e.g., for governance / cleanup).

- Pattern:
  - **Subquery on related login / session tables**:
    - The mental move:
      - “I don’t want *all* active users, I want active + frozen.”

#### c) Integration / error diagnostics

- For integration error fields:
  - e.g., Traction Complete / dedupe tools storing error messages & timestamps:
    - `SELECT Id, SomeErrorField__c, DateOfLastCompletion__c FROM Lead WHERE Converted_from_Lead__c = TRUE AND SomeErrorField__c != NULL AND OwnerId = '...'`
  - Purpose:
    - Find:
      - Records where automation failed post-conversion.
    - Debug:
      - “Why is the vendor tool failing for this subset?”

- Pattern:
  - Combine:
    - Business flags (`Converted_from_Lead__c`),
    - Tool-specific error fields,
    - Ownership / date constraints,
  - To create **precise slices** you can troubleshoot.

#### d) Cleanup / batch candidate queries

- Example mindset:
  - For large cleanup:
    - Index a boolean like `To_be_deleted__c` on Task (or another object),
    - Then query:
      - `SELECT Id FROM Task WHERE To_be_deleted__c = TRUE LIMIT ...`
  - Purpose:
    - Safely identify batch chunks without scanning entire tables.

- Pattern:
  - Use:
    - Indexed flags to drive:
      - Batch jobs,
      - Asynchronous deletes,
      - Archival operations.

---

### 13.15.4 Large data volume mindset (indexes, selectivity, row locking)

I implicitly design queries with **large orgs** in mind (public sector / higher-ed scale):

- **Selectivity first**
  - I aim for:
    - Queries that filter on indexed fields with relatively low cardinality.
  - When I see:
    - Filters only on unindexed picklists, formula fields, or text — that’s a red flag.

- **Index-worthy custom fields**
  - Candidates:
    - External IDs (keys from SIS/ERP),
    - Cleanup flags (`To_be_deleted__c`),
    - “Active/inactive” status fields,
    - High-usage search keys.
  - I think:
    - “What will we filter by in nightly jobs or admin scripts?”  
      Those often need indexes.

- **Row-locking awareness**
  - I try to avoid:
    - Massive updates that touch:
      - The same parent,
      - The same “hot” records.
  - Tactics:
    - Break updates by:
      - Parent group,
      - Time window,
      - Record type / status.
    - Stagger:
      - Batch jobs and integrations that hit the same objects.

- **Governor limit awareness**
  - Simple rule:
    - “Would this query be safe in a loop?”  
      → If the answer is “no,” I refactor: collect IDs, then query once.

This is the mental foundation behind your SOQL talk idea: teaching people to think like **DBAs with guardrails**, not just “developers who grab records.”

---

### 13.15.5 SOQL in Apex vs SOQL for integrations vs SOQL for reporting

I mentally separate *three* main SOQL use-cases:

#### a) SOQL in Apex (request/response scale)

- Constraints:
  - Governor limits,
  - Per-transaction performance,
  - Security context (WITH/WITHOUT SHARING).

- Patterns:
  - Use **selector/service classes**:
    - Centralize complex queries so they’re not duplicated everywhere.
  - Make queries:
    - Bulk-safe (never in loops),
    - Reusable by logic and LWCs,
    - Respectful of sharing model.

- Example patterns I care about:
  - “Selector class per aggregate domain”:
    - e.g., CaseSelector, StudentSelector, ProgramSelector.

#### b) SOQL for integrations (bulk/ETL scale)

- Constraints:
  - Volume, schedules, cross-system SLAs.
- Patterns:
  - Use:
    - Date-based or flag-based filters:
      - `LastModifiedDate >= :lastSyncTime`,
      - `Needs_Sync__c = TRUE`.
  - Combine:
    - SOQL that identifies changed records,
    - With integration platform logic that:
      - Batches,
      - Retries,
      - Logs.

- Mental design:
  - “Salesforce job to *identify* delta + ETL job to *move* delta.”

#### c) SOQL for admin/reporting (ad-hoc & support)

- Usage:
  - Admins and support staff running queries:
    - To understand data patterns,
    - To validate migrations,
    - To help debug weird records.
- Patterns:
  - I build:
    - “Support-friendly” read-only queries:
      - Minimal complexity,
      - Clear WHERE clauses.
  - Use:
    - History tables,
    - Simple joins,
    - Not heavy aggregations.

I treat these as different “modes” and design queries accordingly.

---

### 13.15.6 Parent-child and child-parent query patterns

I use relationship queries when they express the domain clearly:

- **Child → parent**
  - Common pattern:
    - `SELECT Id, Name, Parent__r.Name, Parent__r.External_Id__c FROM Child__c WHERE ...`
  - Use cases:
    - Show parent context in LWC,
    - Feed integration payloads that require parent identifiers.

- **Parent → child (semi-careful)**
  - Pattern:
    - `SELECT Id, Name, (SELECT Id, Status__c FROM Child__r WHERE ...) FROM Parent__c WHERE ...`
  - Use cases:
    - Summarize related data in **read-heavy** contexts (e.g., console views, one-off reports).
  - Caution:
    - Avoid using nested queries like this inside loops or high-volume batch jobs.

Relationship queries give me expressive power, but I still anchor them to performance considerations.

---

### 13.15.7 SOQL + security model – how I think about it

Queries don’t live in a vacuum; they sit on top of security decisions:

- **WITH SHARING vs WITHOUT SHARING**
  - For business logic:
    - I default to respecting sharing (WITH SHARING or default class behavior).
  - For:
    - Admin tools,
    - System diagnostics,
    - Integration layers:
      - I use more privileged contexts carefully.

- **Where to enforce security**
  - I decide:
    - Which layer owns access rules:
      - SOQL (filtering by allowed scopes),
      - Sharing rules (org-wide defaults, roles, sharing sets),
      - Post-query filtering (rare, and only if necessary).

- **Portal / Experience Cloud**
  - For external users:
    - I lean on:
      - Sharing sets,
      - Account relationships,
    - Then ensure:
      - SOQL never bypasses those patterns (no accidental broad queries in controllers for community LWCs).

This mental model ensures I don’t accidentally use “clever SOQL” to defeat the security model.

---

### 13.15.8 SOQL anti-patterns I actively avoid

There are certain things I treat as red flags:

- **SOQL in loops**
  - Classic:
    - Running a query inside `for (record : records)` instead of collecting IDs first.
  - I reflexively rewrite these:
    - Aggregate IDs → single query → map results.

- **Unbounded queries in batch/async jobs**
  - “SELECT Id FROM HugeObject__c” with no WHERE:
    - Disaster at scale.
  - I always:
    - Scope by status, date, or flags.

- **Relying on formula fields for heavy filtering**
  - Formulas:
    - Often not indexed.
  - Instead:
    - Compute the rule in automation,
    - Store result in a real indexed field if it’s filter-critical.

- **Using SELECT fields you don’t need**
  - Pulling:
    - Entire records in bulk when you only need keys or two columns.
  - I prefer:
    - Minimal fieldsets:
      - Especially in integration and batch jobs.

- **Ignoring row-locking symptoms**
  - When I see:
    - “UNABLE_TO_LOCK_ROW” errors,
  - I treat them as:
    - Design problems:
      - Jobs overlapping,
      - Hot parents,
      - Wrong batch grouping.

---

### 13.15.9 How SOQL ties into your talks / teaching patterns

You’ve explicitly wanted to talk about:

- “SOQL queries, effective usage, best practices” for admins/newer devs.

The way you naturally explain things maps into:

- **Story-first teaching**
  - Start with:
    - Real scenarios (integration, cleanup, reporting),
    - Actual org constraints (millions of records, portals, external systems).
  - Then show:
    - How a naive query fails,
    - How a better query performs,
    - What needs indexing or refactoring.

- **Pattern naming**
  - Examples:
    - “Delta query pattern” (LastModifiedDate or flags),
    - “Support drill-down query” (for debugging),
    - “Cleanup flag pattern” (index + batch),
    - “History-based origin trace” (using History tables).

- **Bridging admin → dev**
  - You tend to:
    - Translate complex Apex/ETL queries into admin-understandable thinking:
      - “Here’s why we filter by this field,”
      - “Here’s why this checkbox exists,”
      - “Here’s how you can safely query it in Workbench/Dev Console.”

That’s the core of your “SOQL brain dump” that belongs in your knowledge base.

---

### 13.15.10 To Validate – SOQL & Data Access specifics

To keep this fully honest, here’s what’s worth explicitly confirming or filling in later:

- **Exact large-volume tricks you’ve used**
  - e.g., PK chunking, skinny tables, specific index requests you’ve made to Salesforce.
  - We know you think in “large data” terms; we don’t have explicit logs of each technique.

- **Canonical selector classes you’ve built**
  - Names and patterns:
    - `CaseSelector`, `StudentSelector`, etc.
  - So you can document them as reusable “house style”.

- **Formal query guidelines for your teams**
  - If you haven’t yet:
    - Writing a simple internal doc:
      - “When writing SOQL in this org, always do X/Y/Z; never do A/B/C.”

- **Specific data volumes per key object**
  - To:
    - Tie patterns to reality:
      - “We designed this pattern because Object X has ~Y million rows.”

- **Examples of queries used in your real-time event patterns**
  - When Platform Events or CDC feed integrations:
    - It’s worth capturing a couple of example queries/filters that prep data for events.

## 13.16 Logging, Error Handling & Observability – How I Know What Broke, Where, and Why

### 13.16.1 How I think about logging

For me, logging is not an afterthought. It’s how I answer four questions **without** reverse-engineering everything at 2 AM:

1. What failed?
2. Where did it fail (Salesforce, integration tier, external system)?
3. Who/what did it affect (which users / records)?
4. Can I safely retry / repair it?

Mental rule:

> “If I can’t tell what happened from logs and data alone, the design isn’t finished.”

---

### 13.16.2 Layers of logging in my typical architectures

In the ecosystems I work in (public sector portals, higher-ed CRM + SIS, ETL platforms, external engines), I think of logging in layers:

1. **Salesforce layer**
   - Custom log objects,
   - Field history,
   - Platform Events (sometimes),
   - Error handling in Flows/Apex.

2. **Integration layer**
   - ETL/iPaaS logs (Boomi and similar),
   - Job execution logs,
   - Dead-letter / retry queues.

3. **External system layer**
   - Logs in SIS/ERP,
   - External decision engines / notice engines.

4. **Centralized log/search layer**
   - Aggregation into a search/analytics platform (e.g., centralized logging cluster),
   - Used for:
     - Cross-system traces,
     - Volumetric and error trends.

When I design an integration or feature, I decide **what gets logged at each layer** and how those logs connect.

---

### 13.16.3 Custom log objects in Salesforce – my baseline pattern

I almost always end up with one or more **custom log objects** in Salesforce for app-level events:

- **Integration Log object (conceptual)**
  - Fields typically include:
    - `Integration_Name__c` (which integration),
    - `Direction__c` (Inbound / Outbound),
    - `Status__c` (Success / Failed / Partial),
    - `Source_System__c` / `Target_System__c`,
    - `Related_Record_Type__c` and `Related_Record_Id__c`,
    - `Error_Code__c`, `Error_Message__c`,
    - `Request_Payload_Snippet__c` / `Response_Payload_Snippet__c` (truncated),
    - `Correlation_Id__c`,
    - `Run_Timestamp__c` or `Batch_Id__c`.

  - Usage:
    - Apex integrations log entries here.
    - Flows that call external systems or critical subflows log failures here.
    - Admins/support use list views + reports to investigate.

- **Process / Job Log object (conceptual)**
  - For:
    - Long-running jobs,
    - Data migrations,
    - Backfills,
    - Nightly syncs.
  - Fields:
    - `Job_Name__c`,
    - `Job_Type__c` (Batch / Scheduled / Manual),
    - `Status__c`,
    - `Total_Records__c`, `Successful__c`, `Failed__c`,
    - `Started_At__c`, `Completed_At__c`,
    - `Triggered_By__c` (user / integration user),
    - `Notes__c`.

  - Usage:
    - One row per job execution,
    - Linked to more granular per-record logs if needed.

- **Error Work Item / “Parking Lot” object (conceptual)**
  - For:
    - Records that repeatedly fail to process.
  - Fields:
    - `Primary_Record_Id__c`,
    - `Primary_Object__c`,
    - `Last_Error_Message__c`,
    - `Retry_Count__c`,
    - `Assigned_To__c` (queue/user),
    - `Status__c` (New / In Progress / Resolved),
    - `Integration_Name__c` or `Process_Name__c`.

  - Usage:
    - Backlog of items needing human intervention,
    - Helps separate **business data issues** from **system errors**.

I prefer log objects over relying only on debug logs or email alerts, because they’re queryable, reportable, and sharable with admins and support staff.

---

### 13.16.4 Error handling in Flows – my “no silent failures” rule

I don’t accept default “Flow failed, check email” behavior in production:

- **Fault paths on every external call / critical subflow**
  - Whenever Flow:
    - Does a callout (via action),
    - Inserts/updates critical records,
    - Invokes subflows that can fail,
  - I:
    - Add fault connectors,
    - Log context to a log object,
    - Optionally notify a queue or system user via a task/notification.

- **Logging minimum context**
  - For Flow errors, I want at least:
    - Which Flow,
    - Which path/element,
    - Which record(s) (Id, object),
    - High-level error message (stack traces truncated if needed),
    - User invoking the Flow (or system user if automated).

- **User-facing vs system-facing messages**
  - To end users:
    - Show:
      - Simple, non-technical messages (“We ran into an issue saving your request; the team has been notified.”).
  - To logs:
    - Capture:
      - Full technical error message,
      - IDs and context.

- **Retry strategy in Flows**
  - Where safe, I:
    - Distinguish between:
      - Temporary system errors (network, callout timeouts),
      - Permanent business errors (validation rules, missing data).
  - For permanent errors:
    - Log and route to the “parking lot” object for manual fix or data cleanup.

This turns “Flow crashed” into “Flow logged incident X that we can track and resolve.”

---

### 13.16.5 Error handling in Apex – structured, not ad-hoc

In Apex, my mental pattern is:

- **Wrapper / result objects**
  - Instead of:
    - Throwing raw exceptions everywhere,
  - I often:
    - Return a result object with:
      - `success`, `message`, `errorCode`, `details`, `recordsProcessed`.
  - Then:
    - Caller decides:
      - Log-only vs user-facing error vs rethrow.

- **Centralized logging utilities**
  - A small utility class that:
    - Creates records in log objects,
    - Handles correlation IDs,
    - Applies truncation rules for long messages/payloads.

- **Catch and classify**
  - Catch `Exception e` and classify:
    - Integration error,
    - DML error,
    - Validation/business rule violation,
    - Unexpected bug.
  - Log classification:
    - So dashboards can slice by error type.

- **Bulk-safety of logging**
  - When logging within bulk operations:
    - Collect log entries in memory,
    - Insert in bulk afterwards,
    - Avoid a DML per record error.

This is consistent with how I treated error logs in integration discussion: logging is treated as a **responsibility** of the code, not optional.

---

### 13.16.6 Integration-layer logging & correlation

In any serious architecture (Salesforce + ETL + SIS/ERP + external engines), my pattern is:

- **Job-level and record-level logs**
  - **Job logs**:
    - One entry per execution:
      - Start/end time,
      - Volume processed,
      - Success/failure counts,
      - Upstream/downstream system status.
  - **Record logs**:
    - Per-record or per-group:
      - Key IDs,
      - Status,
      - Error details if any.

- **Correlation IDs across systems**
  - For each integration run (or logical transaction):
    - Generate a **correlation ID** in the integration layer or Salesforce.
  - Pass it through:
    - Outbound request payloads,
    - External system logs,
    - Salesforce log objects.
  - Result:
    - You can trace a single “event” across:
      - Salesforce log object,
      - Integration logs,
      - External service logs,
      - Centralized log/search.

- **Layer responsibilities**
  - Integration platform:
    - Owns:
      - Detailed transport logs,
      - Retry logic,
      - Mapping errors.
  - Salesforce:
    - Logs:
      - What **Salesforce** did or saw:
        - Which records updated,
        - Which Flows/Apex ran,
        - What user/system context was used.

- **Parking lot pattern**
  - For records failing repeat attempts:
    - Integration layer:
      - Marks them as “permanent failure” or “manual review”.
    - Salesforce:
      - Tracks them as:
        - Error Work Items / special queue items,
        - So business/ops can resolve data issues and re-queue.

This is aligned with the 300K+ ID Boomi pattern: log at both ends and in the middle so you always know where a failure lives.

---

### 13.16.7 Centralized logging / search (e.g., log cluster, OpenSearch-like tools)

On larger, regulated projects, logs don’t just live in Salesforce or ETL—they also feed a **central search/analytics cluster**:

- **Why central logs matter**
  - Salesforce logs:
    - Good for record context.
  - ETL logs:
    - Good for integration pipeline specifics.
  - Central log/search:
    - Where:
      - Application logs,
      - API gateway logs,
      - Infrastructure logs (containers, nodes)  
      all come together.

- **What I care about in central logging**
  - Ability to query:
    - By correlation ID,
    - By user/system user,
    - By integration name,
    - By time window,
    - By error codes.
  - Dashboards for:
    - Error rate over time,
    - Latency distributions,
    - Volume anomalies.

- **Storage & architecture considerations**
  - For high-compliance state workloads, I’ve had to think about:
    - Whether logs live on:
      - Encrypted block storage attached to compute clusters,
      - Or cheaper object storage for cold logs.
    - Network/VPN and tenancy boundaries for where log cluster runs.

- **Salesforce + central logs interaction**
  - Sometimes:
    - Salesforce log object stores a `Central_Log_Id__c` or correlation ID.
  - That lets:
    - Ops engineers pivot from a Salesforce log record → central log search with one parameter.

Even when I’m not the primary owner of the logging cluster, I keep this integration in mind when designing app-level logs.

---

### 13.16.8 Observability for scheduled jobs & batch processes

For scheduled tasks (both in Salesforce and integration tools), I think in terms of **observability**, not just “did it run”:

- **Health signals per job**
  - Expected signals:
    - Ran at expected time (or within acceptable delay),
    - Processed expected-ish number of records,
    - Error rate below threshold,
    - Duration within normal range.

- **Dashboards / reports**
  - In Salesforce:
    - Reports on:
      - Job log object,
      - Error Work Items.
  - In integration platform:
    - Dashboards for:
      - Job success/failure count per day,
      - Throughput,
      - Behavior per endpoint/system.
  - In central logging:
    - Combined views:
      - “Show all errors for integration X across all layers in last 24 hours.”

- **Alerting**
  - Triggers when:
    - A critical job fails,
    - Error rate spikes,
    - Volume deviates sharply from baseline,
    - A job doesn’t run at all.

Observability for me = you don’t have to wait for users to complain to know something is wrong.

---

### 13.16.9 Backups, data recovery & “oh no we broke it” thinking

In high-stakes orgs, I also mentally tie logging to **backups & recovery**:

- **Backups**
  - Use:
    - A backup solution or export strategy for:
      - Core objects,
      - Configuration/metadata,
      - Sometimes logs themselves (if they represent audit records).
  - Idea:
    - If a bad deploy or integration run corrupts data, we can reconstruct.

- **Before/after snapshots**
  - For some high-risk operations:
    - Take snapshots of impacted records (or rely on backup+logs) before running:
      - Massive updates,
      - Data migrations,
      - Cleanup jobs.

- **Replay and repair**
  - Using:
    - Log objects,
    - Integration logs,
    - Correlation IDs,
  - We design:
    - Ways to:
      - Re-run a subset of transactions,
      - Undo/override bad changes.

- **Documentation link**
  - Runbooks usually include:
    - “If this job corrupts/overwrites data due to a bug, here’s how we restore/repair using logs + backups.”

This is why I care about structured logs & IDs; they’re not just for debugging, but for **safe rollback paths**.

---

### 13.16.10 Portal / Experience Cloud observability (high level)

Without going into Experience Cloud details yet (that deserves its own section), I still treat portal behavior as something that should show up in logs:

- **Key questions**
  - Which external user types are hitting which flows/LWCs?
  - Where are they dropping off (errors vs confusing UX)?
  - Are we seeing spikes in specific error types for portal logins or transactions?

- **Basic patterns**
  - Logging:
    - Portal actions that represent:
      - Submissions,
      - File uploads,
      - Sensitive data changes,
      - Key status changes (applications, cases, etc.).
  - Using:
    - Custom log objects,
    - Platform events (in some designs),
    - External analytics events if available.

This sets up the next deep dive (Experience Cloud tracking) while staying in the logging/error space.

---

### 13.16.11 Logging & observability anti-patterns I avoid

Things I implicitly treat as red flags:

- **Relying solely on debug logs**
  - Great for developers,
  - Terrible for long-term ops & admins.

- **Logging only in integration platform, not in Salesforce**
  - Leaves:
    - Admins blind; they can’t see what the integration did to records from inside Salesforce.

- **Unstructured text blobs**
  - Logs that are:
    - Giant unstructured messages,
    - No consistent fields to slice/dice by integration, type, or error class.

- **No correlation IDs**
  - Impossible to trace:
    - One real-world transaction across different layers.

- **Alerting only on total failure**
  - “Alert if job doesn’t run,” but:
    - No alert if job runs and fails 50% of records quietly.

---

### 13.16.12 To Validate – Logging & Observability specifics

To keep this honest and useful, here’s what’s worth explicitly filling in later when you build your knowledge base:

- **Exact log object names & field sets you’ve already implemented**
  - So your systemized version matches your actual orgs.

- **Standard correlation ID strategy**
  - Decide:
    - Where correlation IDs are generated (Salesforce vs integration),
    - How they are passed to external systems,
    - How they are stored in Salesforce logs.

- **Alert thresholds & runbook triggers**
  - Define:
    - What “normal” error rates look like for each integration,
    - When alerts fire,
    - Which runbooks they trigger.

- **Backup + log integration**
  - Document:
    - Concrete scenarios:
      - “If integration X writes bad data, use steps 1–N to identify impacted records by log, then recover from backup.”

- **Portal/Experience usage metrics**
  - Decide:
    - Which specific portal actions should be logged,
    - What you want to see on dashboards:
      - Adoption metrics,
      - Error funnels,
      - User cohorts.

## 13.17 Real-Time Event Patterns – Platform Events, Event Channels & EventBridge

### 13.17.1 How I think about “events” vs “APIs” vs “batches”

In my head, real-time event architectures solve a specific problem:

- **APIs** are about:
  - “I need this *right now*. Here’s a request, give me a response.”

- **Batches / ETL** are about:
  - “Move a lot of data, not necessarily immediately, but reliably and in bulk.”

- **Events** are about:
  - “Something happened. Anyone who cares can react, in their own way, in their own time.”

Mental rule:

> “Use events when the *fact that something happened* matters to multiple consumers, and you don’t want Salesforce tightly coupled to any one of them.”

This is exactly the pattern in your **Salesforce → EventBridge → external systems** design:
- Publish once from Salesforce,
- Let the event bus fan out to SIS, data warehouse, etc.

---

### 13.17.2 Main building blocks you actually use

Your event stack (from your own article/pattern) looks like:

- **Salesforce side**
  - Platform Events (custom event objects),
  - Event Channels and Channel Members,
  - Flows for publishing events (record-triggered & subflows),
  - Sometimes Apex for advanced publishers/consumers.

- **Event bus side**
  - A cloud event bus (e.g., EventBridge),
  - Rules/mappings to:
    - Route events to SNS, queues, lambdas or equivalent consumers,
    - Filter by event type, source, payload fields.

- **Downstream consumers**
  - Student Information System (SIS) / ERP integrations,
  - Logging / observability sinks,
  - Analytics / data lake,
  - Optional internal “micro-services” or ETL processes.

Conceptually:

> Salesforce is the *domain source* for certain business events;  
> The event bus is the *distribution layer*;  
> Consumers are *loosely coupled subscribers*.

---

### 13.17.3 Canonical Salesforce → Event Bus pattern (what you actually designed)

Your “canonical pattern” for real-time outbound events from Salesforce roughly looks like this:

1. **Business change in Salesforce**
   - A record changes in a core object:
     - e.g., Application/Case/Student/Program record in a:
       - Higher-education CRM,
       - Public sector case management org,
       - Grants or benefits context.

2. **Flow detects and prepares payload**
   - A record-triggered Flow:
     - Evaluates conditions:
       - Is this event-worthy? (e.g., status changed to “Submitted”, “Approved”, “Withdrawn”, etc.)
     - Collects:
       - External IDs (link to SIS/ERP),
       - Key attributes (program, term, decision, user),
       - Correlation IDs if used.

3. **Flow publishes Platform Event**
   - The Flow:
     - Calls “Create Platform Event” action,
     - Populates:
       - Event type,
       - Aggregate payload fields (JSON string or structured),
       - Reference IDs (Contact, Account, Application, external key).

4. **Salesforce Event Channel**
   - Platform Event is:
     - Published to an Event Channel (or directly to the bus connector, depending on setup).
   - Channel Members:
     - Define which external subscribers (e.g., “EventBridge endpoint”) get which events.

5. **Event Bus (e.g., EventBridge) ingest**
   - EventBridge:
     - Receives the event from Salesforce (via native integration or middleware),
     - Classifies it by:
       - Event source,
       - Event detail type (e.g., `ApplicationStatusChanged`),
       - Possibly by `eventType` in payload.

6. **Routing rules**
   - EventBridge rules:
     - Filter based on:
       - Event type (`ApplicationCreated`, `StatusChanged`, `StudentUpdated`, etc.),
       - Target system needs,
       - Tenancy/agency/college (for multi-tenant use).
     - Route to:
       - SIS integration(s),
       - ETL / data lake,
       - Notification systems,
       - Logging sinks.

7. **Downstream processing**
   - Each consumer:
     - Gets its own copy of the event,
     - Applies its own logic:
       - Look up additional data,
       - Call external APIs (SIS, ERP),
       - Update its own DB,
       - Emit further events internally.

8. **Feedback / error pipeline (optional)**
   - Failed processing:
     - Logged in downstream systems,
     - May send back:
       - Error events,
       - Status updates,
       - Or log-only entries that operators use.

The key point: Salesforce publishes once; EventBridge fans it out.

---

### 13.17.4 Event schema & versioning – how you think about the payload

You treat event schema deliberately, not as a random JSON blob.

**Core design ideas:**

- **Stable event “type”**
  - Each event has:
    - A clear conceptual type:
      - e.g., `ApplicationSubmitted`, `ApplicationDecisionChanged`, `StudentProfileUpdated`, `CaseStatusChanged`.
  - Type naming:
    - Maps to business language, not implementation details.

- **Idempotence-friendly identifiers**
  - Every event carries:
    - Strong keys:
      - Salesforce Id(s),
      - External IDs (SIS/ERP keys, if known),
      - Correlation IDs (for cross-system tracing).
  - So consumers can:
    - Safely handle duplicates (idempotent processing).

- **Payload fields**
  - Only the fields that:
    - External systems need,
    - Or that are useful for filtering/routing.
  - Avoid:
    - Dumping entire record snapshots unless needed.

- **Versioning strategy**
  - When schema changes:
    - You prefer:
      - Additive evolution:
        - Add new fields,
        - Keep old fields for backward compatibility.
      - Or new event types (e.g., `ApplicationSubmittedV2`) if necessary.
  - This protects:
    - Existing consumers from breaking.

You design events as **contracts** between domains, not just “whatever the record had”.

---

### 13.17.5 CDC vs custom Platform Events – selection logic

You have at least the conceptual distinction:

- **Change Data Capture (CDC)**
  - Pros:
    - Automatically fires on record changes,
    - Captures before/after values,
    - Good for data replication and generic sync.
  - Cons:
    - Less semantically focused (it’s “field X changed” vs “business event Y just happened”),
    - Harder to express high-level domain events (like “application is now ready-to-package for SIS”).

- **Custom Platform Events**
  - Pros:
    - Represent:
      - Clear business actions/decisions (e.g., “ApplicationSubmitted”, “TermRegistrationCompleted”),
    - Can be published only when event-worthy business rules are met,
    - Payload is curated.
  - Cons:
    - Extra design work,
    - You must explicitly manage publishing logic.

Your pattern (as seen in your event-article design):

- **CDC** for:
  - Broad data movement / replication,
  - Cases where external systems want “anything changed on this object”.

- **Platform Events** for:
  - Business events,
  - Integration patterns that express domain-level moments,
  - Real-time coordination between multiple systems.

You’re not dogmatic; you pick based on what the consumer needs.

---

### 13.17.6 Internal consumers via Channel Members (multi-consumer inside Salesforce)

Even though the main pattern is **Salesforce → event bus → external systems**, you also leverage Salesforce internal consumers:

- **Channel Members**
  - Allow:
    - Different internal subscribers to listen to the same Channel (Flow, Apex, maybe other tools),
    - Without coupling them directly to each other.
  - Examples (conceptual):
    - Internal automation that:
      - Listens for `ApplicationSubmitted` to create a checklist,
      - Or tracks metrics,
      - Or triggers follow-up tasks.

- **Internal consumer types**
  - **Flows**:
    - Subscribe to Platform Events and update:
      - Additional internal objects,
      - Internal-only log/metric objects.
  - **Apex**:
    - Listens for:
      - High-volume or complex event handling logic.
  - **Analytics / logging**
    - Some events purely feed:
      - Internal dashboards,
      - Operational logs (without further external calls).

Pattern:

> “Same Platform Event → multiple internal and external consumers, each responsible for their own domain.”

---

### 13.17.7 Error handling & retries in event architectures

Error behavior is different in event-driven patterns:

- **Salesforce publisher side**
  - If publishing fails:
    - Flow/Apex logs:
      - Event creation error (e.g., DML failure on Platform Event),
    - Use:
      - Your logging patterns (log object, correlation IDs).
  - Rare, but important:
    - For example, if event bus integration is misconfigured.

- **Event bus side (EventBridge)**
  - Has its own:
    - Retry policies for failed targets,
    - Dead-letter configurations (queues, topics) when retries exhaust.
  - Patterns:
    - Route dead-letter events to:
      - Special queues,
      - Alerting pipelines,
      - Logging sinks.

- **Downstream consumers**
  - Each consumer:
    - Implements its own retry strategy (idempotent processing).
  - If a consumer fails permanently:
    - It logs:
      - Enough info for manual intervention.

You treat errors as **first-class** in the design, not something tacked on later.

---

### 13.17.8 Ordering, duplication & eventual consistency – how you think about trade-offs

You don’t treat event systems as magically ordered/transactional:

- **Ordering**
  - Acknowledge:
    - Events may arrive out of order, especially across systems.
  - Design:
    - Consumers to:
      - Use timestamps and version numbers,
      - Apply “last-write wins” or domain-specific precedence rules.

- **Duplication**
  - Assume:
    - Events might be delivered more than once.
  - Design:
    - Consumers to be idempotent:
      - Use keys and version numbers,
      - Ignore already-seen events, or update only if newer.

- **Eventual consistency**
  - Accept:
    - External systems may be a few seconds behind Salesforce.
  - Make sure:
    - Business processes that depend on absolute sync understand this:
      - UI messaging, statuses, and support docs explain “near real-time”.

This mental model is key in higher-ed & public sector contexts where:

- SIS,
- CRM,
- Portals  
must agree eventually, but not always instantly.

---

### 13.17.9 Example event use cases you *actually* lean toward

Based on your domains (higher-ed CRM + SIS; public sector portals + case engines), your most natural event use cases are:

- **Student / Person lifecycle events**
  - `StudentProfileCreated/Updated`,
  - `ProgramEnrollmentCreated/Updated`,
  - Change in “active term” or “school of record”.

- **Application / Case lifecycle**
  - `ApplicationSubmitted`,
  - `ApplicationDecisionMade`,
  - `CaseStatusChanged`,
  - `NoticeGenerated` (or “ready to generate”).

- **Portal / external user events**
  - `PortalRegistrationCompleted`,
  - `IdentityVerified`,
  - `ExternalUserLinkedToStudentOrCase`.

- **Integration-friendly events**
  - “This application is now in a state where the SIS/benefits engine needs to:
    - Create a record,
    - Update details,
    - Start an external workflow.”

You design events around **domain moments**, not raw CRUD operations.

---

### 13.17.10 How this interacts with your Boomi / ETL patterns

Even though your event-bus pattern is primarily documented with something like EventBridge + SNS, it conceptually aligns with how you think about **Boomi** and other ETL platforms:

- **Event-driven entry**
  - Instead of:
    - Only polling Salesforce by LastModifiedDate,
  - You can:
    - Have ETL processes triggered or filtered by Platform Events from Salesforce.

- **Hybrid approach**
  - Use events:
    - To indicate *what* changed and *where to look*,
    - But let ETL/Batch:
      - Retrieve full, richer data sets via SOQL/REST queries (for complex transformations).

- **Mass operations**
  - For:
    - High-volume batch integration (e.g., 300K EMPLIDs),
  - You still:
    - Use Boomi to do:
      - Bulk processing,
      - File-based batching,
      - Oracle DB queries, etc.
  - Events:
    - Signal delta, ETL handles heavy lifting.

Event-driven doesn’t replace ETL; it **coordinates** it.

---

### 13.17.11 Event architecture anti-patterns you avoid

Things I consider red flags in event architectures:

- **Event = full record dump every time**
  - Leads to:
    - Bloated payloads,
    - Sensitive data leaking to consumers that don’t need it.

- **Overloading one event for everything**
  - Single “RecordChanged” event with:
    - Complex branching logic in every consumer.
  - Prefer:
    - Multiple domain events with clear meaning.

- **Coupling to a single consumer**
  - Designing events:
    - Only to satisfy one existing integration,
    - Ignoring future consumers.
  - Instead:
    - Design them as domain contracts from day one.

- **No replay story**
  - If:
    - Consumers miss events,
    - There’s no way to:
      - Re-seed them from logs or CDC.
  - I try to:
    - Ensure at least:
      - A manual or automated replay mechanism.

- **Treating event bus as opaque “magic black box”**
  - Without:
    - Clear routing rules,
    - Metrics,
    - Error logs.
  - You explicitly care about:
    - Where the logs go,
    - How events are inspected.

---

### 13.17.12 To Validate – Real-Time Event Patterns specifics

To tighten this in your knowledge base, it’s worth explicitly documenting:

- **Concrete event types & schemas you’ve already defined**
  - E.g., actual:
    - Platform Event names,
    - Fields,
    - Example payloads.

- **This project’s exact topology**
  - A simple diagram:
    - Salesforce → Event Channel → EventBridge → {SIS, data lake, logs, others}.
  - Including:
    - Which rules route what where.

- **Publishing-side implementation details**
  - Which events are:
    - Published by Flow (and how),
    - Published by Apex (and where),
    - Derived from CDC vs custom logic.

- **Consumer catalog**
  - Which consumers exist *today*:
    - SIS integration,
    - Analytics,
    - Logging,
    - Internal automations.
  - For each:
    - What they read,
    - How they process,
    - How they handle errors.

- **Replay & backfill strategy**
  - For when:
    - New consumers are added (e.g., new downstream system),
    - Or when:
      - An existing consumer needs to backfill historical events.

This section captures your **real-time integration brain**: Platform Events as clean domain signals, an event bus as distribution, and disciplined logging/handling around it.

## 13.18 Salesforce Flows & Native Automation – How I Wire the Org Together

### 13.18.1 How I see Flow in my world

In my actual projects (public sector portals + higher-ed CRMs):

- **Flow is the primary automation engine.**
- Apex is:
  - For complex logic,
  - For integrations,
  - For performance-critical or reusable domain services.
- Declarative tools (Flow, validation rules, assignment rules, etc.):
  - Are where business logic *starts*.

Mental rule:

> “If a rule can be safely expressed declaratively and stay maintainable, it lives in Flow. Apex is for when we hit complexity, performance, or reuse thresholds.”

---

### 13.18.2 Types of Flow I actually lean on

You’re not using Flow in just one way; you naturally think in flavors:

1. **Record-triggered Flows (workhorse)**
   - For:
     - Creating/updating related records,
     - Status transitions,
     - Notifications,
     - Simple cross-object logic.
   - Key objects:
     - Applications/cases, students/person accounts, tasks, custom integration objects, etc.

2. **Subflows**
   - For:
     - Reusable logic chunks:
       - Re-usable assignment rules,
       - Re-usable “create tasks” or “update status” sequences,
       - Re-usable integration call wrappers.
   - Pattern:
     - “Don’t repeat this decision tree in multiple Flows — factor it into a subflow.”

3. **Screen Flows**
   - Used in:
     - Internal apps (advisors/staff),
     - Occasionally in portal UX (where standard layouts aren’t enough).
   - Pattern:
     - Guided data capture,
     - Step-by-step case/application handling.

4. **Scheduled Flows**
   - For:
     - Periodic cleanup,
     - Periodic recalculation of simple fields,
     - Simple nightly automation that doesn’t need Apex Batch.
   - Only when:
     - Volume and complexity are safe for Flow.

All of this is standard in your “Salesforce architect who still builds things” reality.

---

### 13.18.3 Flow as orchestration vs Flow as calculator

I implicitly separate two mental roles for Flow:

- **Orchestration Flows**
  - Coordinate:
    - “When X happens, do A → B → C.”
  - Touch:
    - Multiple objects,
    - Possibly call invocable Apex,
    - Possibly publish Platform Events.

- **Calculation Flows**
  - Focused on:
    - Computing values,
    - Setting fields,
    - Determining assignments or statuses.
  - Typically:
    - Smaller,
    - More reusable as subflows.

Pattern:

> “Big flow for ‘when’ and ‘in what order’; small subflows for ‘how to calculate/decide’.”

---

### 13.18.4 Guardrails I follow for record-triggered Flows

You don’t just throw logic into record-triggered Flows; you have instincts:

- **One main record-triggered Flow per object per entry context**
  - E.g., for a key object:
    - 1 for “before save” (data prep),
    - 1 for “after save” (side effects, integration, events).
  - Instead of:
    - Having 10 competing Flows that fire on the same object/change.

- **Before vs after logic**
  - **Before save**:
    - Pure field updates on the triggering record,
    - No DML on other objects,
    - No callouts.
  - **After save**:
    - Related record operations,
    - Integration calls,
    - Platform Event publishing,
    - Tasks/notifications.

- **Entry criteria**
  - Don’t:
    - Fire Flows on every tiny change.
  - Do:
    - Restrict triggers based on:
      - Key fields (Status, Stage, RecordType),
      - Changes in specific fields (ISCHANGED-style logic).

- **Recursion control**
  - Use:
    - Flags (e.g., `Processed_By_Flow__c`),
    - OR careful design to:
      - Avoid Flow repeatedly triggering itself.
  - Especially:
    - When mixing Apex and Flow on the same object.

---

### 13.18.5 Flow + Apex – where I draw the line

You regularly combine declarative and programmatic tools:

- **Flow does:**
  - Straightforward decision trees,
  - Updates to related records,
  - Simple calculations,
  - UI orchestration (Screen Flows),
  - Triggering Platform Events via standard actions.

- **Invocable Apex does:**
  - Complex branching or state machines,
  - Heavy calculations,
  - Integration with external APIs that need:
    - Complex authentication,
    - Robust error handling,
    - Asynchronous patterns,
  - Multi-step operations where you want:
    - Stronger code reuse,
    - Abstraction.

- **Pattern: Flow → Apex → Flow**
  - Flow:
    - Gathers context,
    - Calls Apex with a clean request object.
  - Apex:
    - Does the heavy lifting (e.g., integration or complex transform),
    - Returns a result object.
  - Flow:
    - Interprets the result,
    - Handles user messages/logging/routing.

This matches your real behavior on projects with integrations and portals.

---

### 13.18.6 Naming, description & documentation habits in Flow

You already care about names in fields; same applies to Flows:

- **Flow names**
  - Reflect:
    - Object, trigger, and business purpose:
      - e.g., `App_AfterSave_ApplicationStatusOrchestration`,
      - `Case_BeforeSave_Defaulting`.
  - Avoid:
    - “TestFlow1” / “PranavFlow” anti-patterns.

- **Element names**
  - Decision/Assignment/Update elements:
    - Named by:
      - Condition or action, not “Assignment 1/2/3”.
    - E.g.:
      - `Decide_AdvisorAssignmentNeeded`,
      - `Update_Application_ReadyForSIS`.

- **Descriptions & annotations**
  - Flows and key elements:
    - Have descriptions with:
      - When they’re used,
      - What assumptions they rely on,
      - Links to design docs or tickets when relevant.

- **Version discipline**
  - Keep:
    - Old versions only as long as needed for rollback/history.
  - Document:
    - In ticket/docs:
      - “This change corresponds to Flow version X.”

That’s how your admins/future you don’t hate you later.

---

### 13.18.7 Flow + Experience Cloud / portals (high-level)

Even before we deep-dive portals, there are native Flow patterns you use for external users:

- **Screen Flows embedded in Experience pages**
  - For:
    - Guided applications,
    - Case/problem submission,
    - Updating certain profile data.
  - Key constraints:
    - Respecting portal sharing model,
    - Minimizing confusion for users with limited context.

- **Record-triggered Flows responding to portal actions**
  - When portal users:
    - Submit or update something,
    - Flows:
      - Create internal tasks/cases,
      - Trigger notifications,
      - Emit events for integrations.

- **Validation & messaging**
  - Flows:
    - Implement business validations,
    - Provide human-readable error messages.
  - Critical for:
    - Higher-ed and public-sector contexts where forms are non-trivial.

This sits at the intersection of “native automation” and “portal behavior.”

---

### 13.18.8 Flow for integration scaffolding (native side)

Even when the “heavy lifting” is done by Boomi or another ETL tool, you use Flow as glue:

- **Flag-setting**
  - Record-triggered Flows:
    - Mark records as:
      - “Ready for integration”,
      - “Integration error present”,
      - “Do not export”.
  - ETL jobs:
    - Use those flags to pick candidates.

- **Event publishing**
  - Flows:
    - Publish Platform Events when:
      - Application reaches event-worthy status,
      - Case crosses a threshold,
      - Some student/program condition is met.

- **Error capture**
  - When integration writes back:
    - Flows:
      - Interpret status fields and error message fields,
      - Update user-facing flags,
      - Possibly create Error Work Items or tasks.

This keeps a lot of integration behavior understandable by admins, not just coders.

---

### 13.18.9 Flow-driven task & queue automation patterns

From your emails and tickets, task/queue flows are a recurring theme:

- **Task creation flows**
  - Use:
    - Record-triggered Flows on:
      - Cases,
      - Applications,
      - Student/program objects.
  - To:
    - Create tasks:
      - For advisors/staff at specific lifecycle points,
      - With clear subject, due date, owner/queue.

- **Queue assignments**
  - Flows:
    - Decide:
      - Which queue a record should land in,
      - Based on:
        - Program,
        - Campus,
        - Status,
        - Work type.
  - Often:
    - Derived from:
      - Config tables / custom settings,
      - Not hard-coded in the Flow when possible.

- **Escalation and follow-up**
  - Flows:
    - Trigger:
      - Follow-up tasks or status changes if:
        - Tasks are not completed in time,
        - Certain milestones are missed.

You’ve explicitly thought about “Task triggering for incorrect advisor,” “advisor review flows,” etc.—this is exactly that pattern.

---

### 13.18.10 Native automation anti-patterns I avoid

There are specific Salesforce-native automation smells you implicitly avoid:

- **Mixing Flow, Process Builder, and old Workflow Rules on the same object**
  - You prefer:
    - Flow-first,
    - With older tools legacy-only and on a deprecation path.

- **Multiple Flows per object that all do “a bit of everything”**
  - Hard to debug:
    - Which one fired and in what order.
  - You lean toward:
    - Consolidated orchestration flows with clear responsibilities.

- **Hard-coded IDs in Flows**
  - Instead:
    - Use:
      - Custom metadata/config objects,
      - Named resources,
      - Lookups on config records.

- **No error paths**
  - Flows that:
    - Throw generic errors to users,
    - Don’t log anything.
  - You:
    - Wire fault connectors to log objects,
    - Provide user-friendly messages.

- **Putting integration callouts directly in heavy record-triggered Flows without limits**
  - Risk:
    - Hitting callout limits,
    - Performance issues.
  - You:
    - Use invocable Apex & async patterns where needed,
    - Or event-driven dispatch.

---

### 13.18.11 To Validate – Flow & Automation specifics

To keep this fully grounded, here are areas you can explicitly fill in later:

- **Canonical Flow names and purposes**
  - E.g.:
    - `Application_AfterSave_LifecycleOrchestration`,
    - `Student_BeforeSave_Defaults`,
    - `Portal_Submission_ScreenFlow`.
  - Capture:
    - Actual names & objects per project.

- **Shared subflows you reuse**
  - Document:
    - Common subflows like:
      - Advisor assignment,
      - Case/task creation,
      - Status recalculation.

- **Config objects / custom metadata backing your Flows**
  - If you use:
    - “Routing matrix” objects,
    - “Program rules” tables,
  - Capture:
    - Their structure and relationship to Flow.

- **Portal-specific Flows**
  - List:
    - Key Screen Flows used in portals,
    - What they do,
    - Where they’re embedded.

- **Retirement plan for legacy automation**
  - If you still have:
    - Process Builder or Workflow Rules:
      - Document:
        - Migration plan to Flow,
        - Order & risk of moving them.

## 13.19 Lightning Web Components & UI Architecture – How I Actually Use Custom UI

### 13.19.1 How I think about LWC in the overall architecture

In my world, LWCs are *surgical tools*, not decoration:

- Use **standard layouts** when:
  - CRUD and basic related lists are enough.
- Use **Flows** when:
  - I need guided steps, but UX is still simple.
- Use **LWC** when:
  - The UI needs to:
    - Combine data from multiple objects/systems,
    - Perform complex decisions,
    - Stay responsive while doing it,
    - Work cleanly inside Experience Cloud.

Mental rule:

> “If a business-critical decision or complex user journey is too awkward in standard UI or Flow, that’s where an LWC comes in.”

You’ve already done this with things like:

- A **fraud/risk scoring panel** (Fraud Score–style LWC),
- A **program selection workflow** (ProgramSelector–style LWC),
- LWCs that integrate with **external APIs** (including Google services),
- LWCs embedded in **Experience Cloud portals** for students/clients.

---

### 13.19.2 Main LWC contexts you actually operate in

You don’t build LWCs in isolation; you place them carefully:

1. **Record page components (internal)**
   - Embedded on:
     - Application/Case objects,
     - Student/person records,
     - Other key custom records.
   - Use cases:
     - Show computed metrics (e.g., risk scores, eligibility hints),
     - Provide quick actions that cross objects,
     - Present a consolidated “decision view” to staff.

2. **Experience Cloud components (external portal)**
   - Embedded in:
     - Portal home,
     - Application journeys,
     - Self-service pages for students/clients/participants.
   - Use cases:
     - Guided selection of programs/services,
     - Displaying status/next steps,
     - Collecting or updating limited data in a user-friendly way.

3. **Utility / sidebar-style components (internal console) – To Validate**
   - You likely have (or plan) LWCs that:
     - Help staff see cross-object information in a console-like view.
   - Needs explicit confirmation in your docs.

---

### 13.19.3 Program selection & complex pickers – ProgramSelector-style pattern

You’ve explicitly worked on **program selection** in higher-ed/public-sector contexts; the LWC pattern here is:

- **Problem it solves**
  - One object (Application/Case) has to be linked to:
    - A program,
    - A term,
    - A campus/location,
    - A delivery mode (online/hybrid/in-person),
    - Possibly multiple catalogs across systems (Salesforce vs SIS).

- **What the LWC does conceptually**
  - Shows:
    - A curated list of eligible programs (filtered by:
      - Level, location, modality, term, etc.).
  - Enforces:
    - Business rules:
      - “This program is only available in term X,”
      - “This combo of program + location isn’t offered.”
  - Writes:
    - Selected program/term/attributes back to:
      - Application/Case and related objects.

- **Data sources**
  - Salesforce objects representing:
    - Program catalog,
    - Terms,
    - Campus/locations,
    - Delivery modes.
  - Possibly:
    - External IDs and flags that map to SIS offerings.

- **UX considerations**
  - Multi-step filtering:
    - e.g., pick term → filter programs → pick location/delivery → show details.
  - Guardrails:
    - Disable invalid combinations,
    - Show clear messages when something is unavailable.

This pattern is central in your **Education Cloud / program workflow** work.

---

### 13.19.4 Fraud / risk score panels – FraudScore-style pattern

You also have a **Fraud Score / risk indicator** LWC pattern (even if the name changes between orgs):

- **Problem it solves**
  - Staff need a **single glance** view of:
    - Risk/eligibility indicators,
    - Cross-object signals (duplicate patterns, data inconsistencies, suspicious configurations),
    - Possibly inputs from external or internal rules engines.

- **What the LWC does conceptually**
  - Retrieves:
    - One or more “score” objects (or fields) tied to the primary record.
    - Contextual factors:
      - e.g., prior history, metadata flags, pattern matches.
  - Computes or presents:
    - A composite score or risk level,
    - With breakdown (why the score is high/low).
  - Highlights:
    - Suggested actions (review, escalate, approve, request documents).

- **Data & integration**
  - Reads:
    - Salesforce fields (flags, counts, categories),
    - Possibly:
      - External API responses persisted into Salesforce.
  - May:
    - Trigger re-calculation via an Apex call or Flow when user clicks “Recalculate.”

- **UX considerations**
  - Visual emphasis:
    - Traffic-light style (low/medium/high),
    - Plain-language explanation of risk factors.
  - Performance:
    - Load quickly on record page without blocking other components.

Even where the domain isn’t literally fraud, the pattern is the same: **computed decision UI tied to a record**.

---

### 13.19.5 LWCs integrating with external APIs (including Google)

You’ve mentioned **“different types of Google integrations”** via LWCs. Without naming exact APIs, the pattern is:

- **High-level use cases**
  - Surface external data:
    - e.g., locations/maps, documents, calendars, spreadsheets, or other Google-based resources.
  - Allow staff or portal users to:
    - Search/select/update something that actually lives outside Salesforce.

- **Architecture pattern**
  - LWC:
    - Handles UI,
    - Gathers user input (search terms, filters, selections).
  - Apex:
    - Performs:
      - Callouts to Google/external APIs via Named Credentials or secure auth.
    - Normalizes:
      - Responses into simplified DTOs for the LWC.
  - Salesforce storage:
    - Writes only what needs to be persisted:
      - IDs, links, metadata,
      - Possibly snapshots or cached values.

- **Reasons you do this**
  - Avoid:
    - Copying entire external datasets into Salesforce.
  - Let Salesforce:
    - Be the **control plane** while Google (or others) remain **system of record** for certain resource types.

- **UX considerations**
  - Search-as-you-type,
  - Paginated results,
  - Clear “linked” vs “not linked” states.

Details like **which exact Google services** should be captured later in your To Validate section.

---

### 13.19.6 Data access patterns inside your LWCs

Given your experience with SOQL and integrations, your LWC data access patterns tend to be:

- **Use Lightning Data Service (LDS) when possible**
  - For:
    - Loading the current record and simple related data.
  - Benefits:
    - Caching,
    - Respect for FLS/sharing,
    - Less boilerplate.

- **Use @wire for read operations that map cleanly to Apex**
  - When:
    - You need more specialized queries:
      - Multi-object lookups,
      - Filter parameters from LWC state.
  - Pattern:
    - `@wire(getSomething, { recordId: '$recordId', filterX: '$filterX' })`
    - The Apex method:
      - Returns only the data needed for UI (not full SObjects when avoidable).

- **Use imperative Apex calls for user-driven actions**
  - On user actions:
    - Button clicks,
    - Step completion,
    - Selection changes.
  - Imperative calls:
    - Do updates,
    - Trigger integrations,
    - Recalculate scores or program options.
  - Pattern:
    - `await someApexAction({ payload })` with:
      - Try/catch in JS,
      - Error display via toasts or inline messages.

- **No SOQL in LWC directly**
  - Data access:
    - Always via LDS or Apex,
    - Respecting security and avoiding client-side injection.

This lines up with your broader discipline around queries and security.

---

### 13.19.7 LWC in Experience Cloud portals – how you shape external UX

Your portal work (student / client / participant self-service) uses LWCs to go beyond standard page layouts:

- **Where LWCs fit**
  - On:
    - Portal homepages (status summary),
    - Application dashboards (current apps, next steps),
    - Individual application/case pages,
    - Specialized flows (program selection, document upload flows, etc.)

- **Key concerns**
  - **Security & sharing**
    - LWCs:
      - Must rely on data that:
        - External users are allowed to see via sharing sets / customer community sharing.
      - Your Apex:
        - Runs with sharing where appropriate.
  - **Clarity for non-technical users**
    - Plain language,
    - Clear steps,
    - Error messages that make sense to a student/client, not a developer.

- **Portal-specific patterns**
  - “My status” widgets:
    - LWCs showing:
      - Application/case status,
      - Deadlines,
      - Required documents.
  - “Guided choice” widgets:
    - Program/benefit selection,
    - Eligibility pre-checks,
    - Next best action suggestions.
  - “Action launchers”:
    - Buttons/cards that:
      - Launch Screen Flows,
      - Open forms,
      - Trigger support/contact workflows.

LWCs here become the **face** of complex backend logic and integrations.

---

### 13.19.8 UX & error-handling patterns in LWC

Given your focus on observability and admin-friendliness, your LWC UX patterns are:

- **Spinners/skeletons**
  - Show:
    - Loading indicators while:
      - Apex calls run,
      - External API data comes back.
  - Avoid:
    - “Blank for 3 seconds, then suddenly everything appears.”

- **Toast + inline error combo**
  - For user-visible errors:
    - Use:
      - Lightning toasts for “something went wrong”,
      - Inline sections to show:
        - Specific issues or what to do next.
  - For purely system issues (logged elsewhere):
    - Show:
      - Generic or minimal error externally,
      - Full details in logs.

- **Graceful degradation**
  - If an external integration fails:
    - LWC:
      - Shows fallback information or instructions,
      - Does not hang the entire page.
  - If underlying data is inconsistent:
    - Components:
      - Detect invalid state and guide the user to either:
        - Contact support,
        - Fix missing prerequisites.

- **Accessibility & responsiveness**
  - Especially important in portals:
    - Use semantic HTML where possible,
    - Ensure keyboard navigation works,
    - Layouts adapt to mobile/desktop.

Exact implementation details (e.g., which base components) can be refined later in your docs.

---

### 13.19.9 LWC anti-patterns you avoid

Based on how you think about architecture and maintainability, these are the LWC patterns you implicitly avoid:

- **“God components” that do everything**
  - Single giant LWC that:
    - Handles many unrelated responsibilities,
    - Talks to many objects,
    - Becomes untestable and fragile.
  - You prefer:
    - Smaller, focused components orchestrated by a parent.

- **Business logic buried in JavaScript**
  - Complex rules:
    - Duplicated in JS that diverges from Apex/Flows.
  - Instead:
    - Keep domain logic:
      - Centralized in Apex services or Flow,  
      - LWC handles presentation and user interaction.

- **Ignoring security/sharing**
  - Apex methods:
    - That run in overly-permissive contexts for portal users.
  - You instead:
    - Align Apex with sharing model,
    - Only expose what external users truly need.

- **Hard-coding IDs and magic constants**
  - Component JS:
    - Hard-coding record types, queue Ids, program codes, etc.
  - You’d rather:
    - Use:
      - Custom metadata/config objects,
      - Named values passed as properties,
      - Or Apex that looks them up centrally.

- **No telemetry from LWC**
  - Components:
    - That fail silently when integration calls die.
  - You instead:
    - Tie:
      - Errors to your logging framework via Apex,
      - Show enough UI feedback to users.

---

### 13.19.10 To Validate – LWC & UI specifics

To make this airtight in your knowledge base, you’ll want to explicitly fill in:

- **Concrete components**
  - Actual LWCs you’ve built, e.g.:
    - `fraudScorePanel` / `riskScorePanel`,
    - `programSelector`,
    - `studentStatusSummary`,
    - `integrationDebugPanel`,
    - Google-integration-based LWCs.
  - For each:
    - Purpose,
    - Inputs/outputs,
    - Data sources.

- **Standard LWC patterns/templates you reuse**
  - E.g.:
    - “Search + select” modal template,
    - “Record summary panel” template,
    - “Stepper/wizard” template.

- **Exact Google/external integrations**
  - Which APIs:
    - Maps? Places? Drive? Sheets? Calendar?
  - How:
    - Auth is handled (Named Credentials vs external gateway),
    - Data is cached or persisted.

- **Portal vs internal variants**
  - Do you:
    - Use the same LWC with different behavior inside and outside portal?
    - Or have separate bundles for internal vs portal views?
  - Capture:
    - That pattern explicitly.

- **Testing strategy**
  - Unit tests:
    - Jest or similar for LWC logic.
  - Integration tests:
    - How you validate LWC + Apex + Flow works end-to-end in key flows.

This section is the **custom UI brain dump**: LWCs as the front-end for complex decisions, integrations, and portal UX, wired to the data and logging patterns you’ve already built.

## 13.20 Experience Cloud – User Model, Sharing & Tracking (Generic, Platform-Only)

### 13.20.1 How I think about Experience Cloud

Experience Cloud is, for me, a way to:

- Let **external users** interact with Salesforce data,
- Control **exactly** what they can see and do,
- Sit cleanly on top of:
  - Account/Contact model,
  - Sharing model,
  - Identity/SSO setup.

Mental rule:

> “Portal design = user model + identity + sharing + UX. Get those right first, components and pages come later.”

---

### 13.20.2 Portal user types (generic)

I think in generic **roles**, not specific industries:

- **External individuals**
  - One person tied to one or more Contacts.
  - Typical use:
    - See “my stuff” (my records, my cases, my applications).

- **External organization users**
  - Users who act **on behalf of** an Account (company/org).
  - Typical use:
    - See records related to their Account, sometimes across multiple orgs.

- **Internal staff via portal (optional)**
  - Occasionally internal users use the portal UI for certain flows.
  - They still get:
    - Internal licenses, internal profiles/permission sets.

Everything else is just configuration on top of these three ideas.

---

### 13.20.3 Account / Contact model patterns (generic)

I use a small set of patterns for representing people/orgs behind portal users:

1. **Individual-as-person pattern**
   - Either:
     - Person Accounts, or
     - Standard Account + Contact where the Account represents an individual.
   - Portal user:
     - Is always tied to a Contact.
   - Data rule:
     - Identity attributes (name/email/external IDs) belong primarily on Contact.

2. **Organization-with-contacts pattern**
   - Business/Org = Account.
   - Users = Contacts under that Account.
   - Portal users:
     - Created from those Contacts.
   - Use when:
     - External users act on behalf of an organization.

3. **Many-to-many person ↔ org pattern (generic)**
   - A person can belong to multiple organizations.
   - Implement with:
     - Junction object between Contact and Account,
     - Plus custom sharing logic or rules based on that junction.

I avoid mixing these patterns on the same portal unless there’s a clear strategy.

---

### 13.20.4 Generic portal user creation flows

I think about three core flows, without tying to any particular project:

1. **Self-registration**
   - User signs up through the site.
   - Experience Cloud:
     - Creates Contact (+ Account if needed),
     - Creates User and links it.
   - Use when:
     - It’s safe to let people register themselves with minimal pre-validation.

2. **Pre-created Contact → first login**
   - Contact created beforehand (data load / admin / integration).
   - On login (via SSO or standard):
     - Salesforce finds the Contact (by external ID/email/etc.),
     - Creates the User and links it.
   - Use when:
     - You want tight control and don’t want random self-created Contacts.

3. **Admin-created users**
   - Admin chooses a Contact and hits “Enable Portal User.”
   - Use when:
     - User onboarding is tightly controlled,
     - Or the portal population is small/specialized.

I aim to **pick one primary model per portal** and avoid mixing patterns casually.

---

### 13.20.5 Identity & SSO – generic patterns

Technically, I think in two broad categories:

- **External IdP (OIDC/SAML)**
  - Used for:
    - External users (partners/customers/individuals).
  - IdP handles:
    - Credentials, MFA, account lockout.
  - Salesforce receives:
    - A stable subject ID (GUID) + optional claims (email, name, role).
  - Login handler pattern:
    - Find matching Contact by external ID/email,
    - Create or update User as needed,
    - Enforce eligibility rules (only some Contacts can become users).

- **Internal IdP**
  - Used for:
    - Employees/staff.
  - Same general pattern:
    - Map internal identity attributes to User,
    - Keep internal permission sets and portal roles distinct.

Key principle:

> “Use an external ID (GUID) as the primary link; let email be secondary and changeable.”

---

### 13.20.6 Sharing & data access patterns (pure platform)

On Experience Cloud, I mostly rely on:

1. **Sharing sets**
   - Pattern:
     - “External user can see records where a lookup field points to their Contact or Account.”
   - Common variants:
     - Contact-based:
       - Records where `Contact__c = [current user’s Contact]`.
     - Account-based:
       - Records where `Account__c = [current user’s Account]`.

2. **Sharing rules**
   - For:
     - Role-based expansion (e.g., certain external users see a broader set),
     - Collaborating users within the same org.

3. **Custom sharing**
   - When:
     - Relationships are complex (e.g., many-to-many).
   - Implemented via:
     - Apex-managed sharing,
     - Custom “access” objects that define who can see what.

4. **Internal vs external separation**
   - Internal users:
     - Governed by roles, profiles, permission sets, and sharing rules.
   - External users:
     - Governed by:
       - Community user profile,
       - Permission sets,
       - Sharing sets/rules.

I always design the sharing model **before** dropping LWCs/Flows into the site.

---

### 13.20.7 Page structure & navigation (generic)

Even generically, I tend to structure portals around:

- **Home / dashboard**
  - Tiles or lists for:
    - “My records” (cases, applications, whatever the main object is),
    - Status summary,
    - Tasks/alerts,
    - Quick actions.

- **Record detail + actions**
  - Page that shows:
    - Record details (standard layout or LWC),
    - Related lists,
    - Actions:
      - Upload docs,
      - Update certain fields,
      - Start related flows.

- **Self-service flows**
  - Pages dedicated to:
    - Starting new transactions (cases/applications/etc.),
    - Guided multi-step submissions via Screen Flows or LWC steppers.

- **Support / help**
  - FAQ/articles (Knowledge),
  - “Contact us” / “raise a request” forms,
  - Possibly a “My messages/notices” area.

This is all **pure navigation, no client-specific context needed**.

---

### 13.20.8 Tracking & analytics (high-level, generic)

For any portal, I care about:

- **Logins & engagement**
  - Basic stats:
    - How many users log in,
    - How often,
    - Last login.
  - Source:
    - Standard login history,
    - Simple custom fields on User or Contact (“last portal login”).

- **Process progress**
  - Where users are in key flows:
    - Started vs submitted vs completed.
  - Implementation:
    - Status fields on main object,
    - Optional “progress” fields or helper objects.

- **Feature usage**
  - Which self-service features are used:
    - New record flows,
    - Update flows,
    - Document upload,
    - Appointment scheduling.
  - Implementation:
    - Log records created by Flows/Apex when key actions occur,
    - Portal-specific reports/dashboards.

- **Error tracking**
  - When something fails:
    - Use the **same log object / error patterns** I use elsewhere:
      - Log which user,
      - What action,
      - What error.

This stays fully generic and portable to any industry.

---

### 13.20.9 Generic portal anti-patterns

The platform anti-patterns I avoid (no company context):

- **Self-registration with no data model plan**
  - Leads to:
    - Duplicate Contacts,
    - Uncontrolled Accounts,
    - Messy data.

- **Relying only on email for identity**
  - Breaks when:
    - Email changes,
    - Multiple systems share the same address.

- **Overly broad sharing sets**
  - e.g., “Give access to everything under this Account” without understanding object relationships.
  - I prefer:
    - Narrow, object-specific rules.

- **Portal and internal users sharing the same permission sets**
  - Risk:
    - Exposing too much internal capability to external users.
  - I separate:
    - External permission sets,
    - Internal permission sets,
    - System user permissions.

- **No logging of portal failures**
  - Relying on:
    - “User says something is broken.”
  - Instead:
    - Flows and Apex log:
      - Failed portal actions,
      - Integration errors,
      - Unexpected state.
## 13.21 Case & Service Management – Generic Patterns (No Company Info)

### 13.21.1 How I think about “cases” in Salesforce

I don’t see Cases (or incident-style objects) as “just tickets.” For me they’re:

- The **control object** for:
  - Work assignment,
  - Status tracking,
  - SLAs,
  - Communications.
- A **hub** that ties together:
  - People (contacts, internal users, queues),
  - Processes (tasks, approvals, automations),
  - External systems (via integrations).

Mental rule:

> “If humans have to coordinate around a problem or request over time, it usually belongs on a case-like object.”

---

### 13.21.2 Core building blocks I actually use

Across different orgs, the same pieces repeat:

- **Primary work object**
  - Standard Case *or* a custom “Incident/Request/Application” object,
  - Holds:
    - Status,
    - Owner (user/queue),
    - Priority,
    - Category/type/subtype,
    - Key dates (opened, closed, due).

- **Tasks**
  - Represent **granular action items**:
    - “Review X,”
    - “Call Y,”
    - “Verify Z,”
  - Often automatically created based on:
    - Status changes,
    - Intake type,
    - Business rules.

- **Queues**
  - Represent **work buckets**:
    - By team,
    - By function,
    - By specialization.
  - You’ve even exported queues for cross-ref, so you think of them structurally, not ad-hoc.

- **Related custom objects**
  - For:
    - Notes/logs beyond standard case feed,
    - History of decisions,
    - Related requests grouped under a “parent case” if needed.

That’s the backbone of your service/workflows.

---

### 13.21.3 Status & lifecycle patterns (generic)

I treat status as a **formal state machine**, not a loose picklist:

- **Entry states**
  - Examples:
    - New,
    - Intake Received,
    - Awaiting Triage.
  - Behavior:
    - Minimal info, not yet assigned or fully evaluated.

- **Working states**
  - Examples:
    - In Progress,
    - Waiting on Customer,
    - Waiting on External Party,
    - Pending Internal Review.
  - Behavior:
    - Tasks active,
    - SLAs running,
    - Ownership stable.

- **Decision / completion states**
  - Examples:
    - Resolved,
    - Closed – Completed,
    - Closed – Withdrawn,
    - Closed – Incomplete.
  - Behavior:
    - Outcome recorded,
    - Follow-up tasks may be created (e.g., for downstream systems).

- **Automation rules**
  - Status changes typically:
    - Trigger Flows to:
      - Create tasks,
      - Send notifications,
      - Update related records,
      - Emit Platform Events.

I try to keep the status model small but meaningful, and closely tied to automation.

---

### 13.21.4 Queue & assignment patterns

Queues are not dumping grounds; they are **routing endpoints**:

- **By function / team**
  - Example types:
    - General support,
    - Specialized review,
    - Escalation/second-line,
    - Quality/QA.
  - Cases/tasks routed based on:
    - Category,
    - Priority,
    - Program/type,
    - Region/owner attributes.

- **Assignment via config, not hard-code**
  - Use:
    - Assignment rules,
    - Custom metadata/config objects,
    - Flow decision tables.
  - Avoid:
    - Hard-coding queue IDs or user IDs in Flows/Apex.

- **Reassignment rules**
  - When:
    - Status changes,
    - A case is returned by one team,
    - An SLA threshold is hit.
  - Flows:
    - Reassign to a different queue or escalate queue.

You’ve written about advisor assignment and task routing; this is the same brain at work, just generically.

---

### 13.21.5 Task automation patterns tied to cases/records

A big chunk of your work has been “Task automation priorities,” “Incorrect advisor tasks,” etc. Generic pattern:

- **Trigger events**
  - Case (or equivalent) created,
  - Status changed,
  - Certain fields updated (e.g., advisor/program/owner),
  - External integration signals (e.g., mismatch found).

- **Task creation logic**
  - Record-triggered Flow or Apex:
    - Creates one or more Tasks with:
      - Clear subject,
      - Due date (based on rules),
      - Owner (queue/user),
      - What/Who references correctly set.

- **Examples of task types (generic)**
  - Verify data,
  - Reach out to the person,
  - Perform a specific review,
  - Complete a checklist for a process (e.g., term review, eligibility review).

- **Priority & sequencing**
  - You often:
    - Ask for clarity on **which set of tasks should be prioritized**,
    - Consider:
      - Effort,
      - Impact,
      - Dependencies,
    - And want to avoid:
      - Building automation out of order that later needs refactoring.

The “Task Triggering for Incorrect Advisor” and “review tasks for certain scenarios” you’ve worked on are all instances of this pattern.

---

### 13.21.6 Service metrics & reporting patterns

Even generically, you care about **measuring**:

- **Volume**
  - Number of cases/incidents by:
    - Type,
    - Source,
    - Channel,
    - Time period.

- **Timelines**
  - Time to:
    - First response,
    - First meaningful action,
    - Resolution/closure.

- **Backlog & throughput**
  - Open vs closed cases by queue/owner,
  - Aging (time in status, time since last update),
  - Completed vs newly created per time period.

- **Quality / correctness**
  - Cases where:
    - Tasks were created incorrectly (e.g., incorrect advisor),
    - Integrations failed,
    - Rework needed.

Reports and dashboards give feedback loops on your routing and automation design.

---

### 13.21.7 Integration points in case/service flows (generic)

You regularly interface case-like processes with other systems (just keeping it generic):

- **Inbound**
  - External systems:
    - Create or update case-equivalent records via APIs/ETL,
    - Attach external IDs,
    - Provide status/decision codes.
  - You:
    - Map external statuses to internal status fields,
    - Ensure idempotent updates.

- **Outbound**
  - Salesforce:
    - Notifies external systems when:
      - Status changes,
      - Decisions are made,
      - Certain milestones are reached.
  - Methods:
    - API callouts,
    - ETL jobs,
    - Platform Events hooked into external event buses.

- **Notifications**
  - Case changes trigger:
    - Outbound messages (emails/SMS/other systems),
    - Notice/letter generation in external engines (via log/trigger records).

This matches your general integration approach: case record = anchor; integrations orbit around it.

---

### 13.21.8 Error handling and “parking lot” for failed processing

When automations around cases fail, you don’t want silent failures:

- **Log entries**
  - Integration and Flow errors:
    - Log to a dedicated log object (as described earlier).
  - Include:
    - Case/record id,
    - Operation type,
    - Error details,
    - Whether manual intervention is needed.

- **Error work items**
  - Separate object:
    - Represents “something went wrong with this case/record.”
  - Fields:
    - Links to main record,
    - Error description,
    - Status (new/in-progress/resolved),
    - Assigned queue/user.

- **Retry strategies**
  - For transient issues:
    - Automated retry (via scheduled jobs or re-run flows).
  - For permanent data issues:
    - Assigned as work to humans via tasks/error records.

This matches how you think about logging and detect/retry loops for integrations.

---

### 13.21.9 Native features I plug into case flows (generic)

Without tying to any branded product names or specific implementations, I generally use:

- **Email-to-case / web-to-case** (if appropriate)
  - For:
    - Intake from email/web forms.
  - Always:
    - Wrap with:
      - Normalization,
      - Categorization,
      - Automation for assignment.

- **Knowledge**
  - Cases:
    - Linked to articles as “solutions” or references.
  - Helps:
    - Reuse known fixes,
    - Support agents.

- **Macros / Quick actions**
  - For agents:
    - Speed up common actions on cases,
    - Without building custom UI for every single scenario.

These are all building blocks that combine with your custom Flows/LWCs.

---

### 13.21.10 Case & service anti-patterns I avoid

Generic red flags:

- **Too many statuses that mean nothing**
  - Status picklists:
    - With overlapping meanings (“In Review”, “Being Reviewed”, “Under Review”).
  - I’d rather:
    - Have fewer, clearer states tied directly to automation.

- **Queues as dumping grounds**
  - Cases:
    - Piled into “catch-all” queues with no ownership rules.
  - Instead:
    - Define:
      - Responsibility per queue,
      - SLA expectations,
      - Escalation paths.

- **Automation scattered across multiple tools**
  - Same field/state:
    - Updated by Workflow, Process Builder, Flows, triggers.
  - I prefer:
    - Consolidate into:
      - Flows + Apex where needed.

- **Hard-coded logic for routing/assignment**
  - E.g., directly referencing queue IDs in many Flows/Apex classes.
  - Instead:
    - Route via:
      - Config objects,
      - Custom metadata,
      - Single shared “assignment engine.”

- **No feedback loop from metrics**
  - Building routing/automation and never:
    - Watching metrics to see if:
      - Cases sit too long,
      - Work gets stuck,
      - Wrong queues receive certain categories.
  - You tend to:
    - Use reporting/metrics to refine patterns iteratively.

---

### 13.21.11 To Validate – Case/Service specifics (you fill in later)

To turn this into a fully precise knowledge base, you can later fill in:

- The **main case-like objects** you’ve used:
  - Names and their primary roles (incident, application, generic request, etc.).
- The **status lifecycle** you standardized:
  - Exact statuses and allowed transitions.
- Your **queue strategy**:
  - What queues exist conceptually (by function), how they map to teams.
- The **task patterns** you’ve actually implemented:
  - E.g., “incorrect advisor tasks,” “review tasks for certain milestones,” etc. — described generically.
- How **integrations** connect in:
  - Which systems create/update case-equivalents; which ones consume case decisions.

## 13.22 Industry-Style Data Modeling – Cross-Industry, Platform-Only

### 13.22.1 How I think about industry data models on Salesforce

Across industries (public sector, SaaS, financial services, membership, marketplaces, etc.), the core building blocks repeat:

- **People**
- **Organizations**
- **Offerings** (products/services/plans)
- **Requests** (applications/orders/tickets/intake)
- **Engagements** (subscriptions/participation/usage)
- **Cases/Issues** (problems, exceptions)
- **Transactions** (money, usage, or key events)
- **Communications** (notices, letters, email/SMS records)

Mental rule:

> “Pick clear primitives, relate them cleanly, then add external IDs and time. Most ‘industry’ is just naming and rules on top of that.”

---

### 13.22.2 People: contacts, users, members, beneficiaries

Generic pattern:

- **Core person entity**
  - Usually:
    - `Contact` as the canonical “person” object.
  - Holds:
    - Identity:
      - Name fields,
      - Email(s),
      - Phone(s),
      - Optional demographics.
    - External IDs:
      - `External_Person_Id__c` (master person ID),
      - IDs from legacy CRM, IdP, or line-of-business systems.

- **Person roles**
  - Same person can be:
    - Customer, prospect, beneficiary, employee, partner contact, etc.
  - Implemented as:
    - Role fields on Contact (for simple cases),
    - Or better:
      - **Role/relationship objects** (see below).

- **Person relationships**
  - Relationship objects to represent:
    - Person↔Person (e.g., parent/guardian, representative),
    - Person↔Organization (employee, contact person),
    - Person↔Offering (subscriber, participant).
  - Each role record can have:
    - Role type,
    - Status (active/inactive),
    - Effective dates.

This avoids hard-coding “customer vs member vs beneficiary” into one overloaded picklist.

---

### 13.22.3 Organizations: accounts, business units, partners

Generic Account modeling:

- **Core organization entity**
  - `Account` as:
    - Customer org,
    - Partner org,
    - Vendor,
    - Business unit,
    - Location, etc.
  - Differentiated via:
    - Record Type,
    - Type/Category fields.

- **Account attributes**
  - Identification:
    - External org IDs (ERP, billing system, HR system),
    - Registration numbers (tax IDs, license numbers if applicable).
  - Structure:
    - Parent–child Accounts:
      - Headquarters vs subsidiaries,
      - Divisions vs departments.

- **People ↔ Organizations**
  - Relationship object between Contact and Account for:
    - Employee, account team member, broker, agent, etc.
  - Fields:
    - Role,
    - Start/end dates,
    - Primary/secondary flags.

This supports B2C, B2B, and hybrid B2B2C without changing the core model.

---

### 13.22.4 Offerings: products, services, plans, bundles

Almost every industry has something like “what we provide”:

- **Offering object(s)**
  - Could be:
    - Standard `Product2`,
    - Or custom “Service__c”, “Plan__c”, “Offering__c`.
  - Typical fields:
    - Name/code,
    - Type/category,
    - Active flag,
    - Attributes:
      - Tier, level, segment, risk level, etc.

- **Bundles & variants**
  - Related objects for:
    - Bundles (offering composed of other offerings),
    - Options/add-ons,
    - Regional/segment variants.

- **Availability rules**
  - Configuration objects to define:
    - Which offerings are available:
      - In which regions,
      - For which customer segments,
      - Under which conditions.
  - Used by:
    - Flows/LWCs to show only valid options.

This pattern applies to subscriptions, insurance products, service packages, benefit plans, etc.

---

### 13.22.5 Requests: applications, orders, quotes, intake records

“Request” = when someone asks for something or kicks off a process:

- **Request object**
  - Examples:
    - “Application__c”, “Order__c”, “Request__c”, “Quote__c”.
  - Relationships:
    - Who requested:
      - Contact/Account or both.
    - What:
      - Offering(s) requested (lookup or junction).
    - Context:
      - Channel (portal, phone, integration),
      - Source campaign/referral.

- **Lifecycle fields**
  - Status:
    - Draft, Submitted, In Review, Approved, Rejected, Withdrawn, Canceled.
  - Dates:
    - Created, Submitted, Decision/Completion dates.
  - Reason fields:
    - Decision reason,
    - Cancellation reason.

- **Integration keys**
  - External request ID if mirrored in:
    - Order management system,
    - Core line-of-business system,
    - Underwriting/decision engine.

This covers everything from loan applications to SaaS trials to benefit applications.

---

### 13.22.6 Engagements: subscriptions, contracts, participation, usage

Engagement = “ongoing relationship with an offering”:

- **Engagement object**
  - Examples:
    - “Subscription__c”, “Contract__c”, “Participation__c”, “Membership__c”.
  - Relationships:
    - Person and/or Account,
    - Offering,
    - Possibly a Request record that originated it.

- **Core fields**
  - Status:
    - Active, Pending, Suspended, Canceled, Completed.
  - Dates:
    - Start, end/renewal, suspension/resume dates.
  - Terms:
    - Billing cycle, contract type, renewal type, term length.

- **Source & system-of-record**
  - Source:
    - Internal (created in Salesforce),
    - External (imported from billing/ERP/core).
  - External ID:
    - Link to billing/subscription/contract system.

This is where recurring value or ongoing service is modeled, regardless of industry.

---

### 13.22.7 Cases & issues: support, exceptions, disputes

Separate from “normal” lifecycle:

- **Case/Issue object**
  - Standard `Case` or custom “Issue__c”, “Ticket__c”.
  - Relationships:
    - Person/Account,
    - Often a link to:
      - Request,
      - Engagement,
      - Offering.

- **Usage**
  - Support inquiries,
  - Quality or service issues,
  - Complaints or disputes,
  - Exception handling scenarios.

- **Lifecycle**
  - Status:
    - New, In Progress, Waiting (internal/external/customer), Resolved, Closed.
  - Routing:
    - Queues or owners based on type, severity, segment.

This pattern is the same whether it’s customer service, claims, or operational support.

---

### 13.22.8 Transactions & events: money, usage, key domain events

Transactions = “discrete events with business significance”:

- **Transaction object**
  - Examples:
    - “Transaction__c”, “Payment__c”, “Usage_Record__c”, “Event_Log__c”.
  - Fields:
    - Type/category,
    - Amount/quantity/score/value,
    - Date/time,
    - Status,
    - References to:
      - Person/Account,
      - Engagement/Contract/Subscription,
      - Request or Case, if relevant.

- **Use cases**
  - Financial:
    - Payments, refunds, charges, invoices.
  - Operational:
    - Usage units (API calls, minutes, units delivered),
    - Milestone completions.
  - Risk/compliance:
    - Flags, score recalculations, policy events.

Transactions can feed analytics, billing checks, audits, and risk models.

---

### 13.22.9 Communications & notices: what we told whom, and when

Every regulated or customer-facing process needs a memory of communications:

- **Communication object**
  - Examples:
    - “Notice__c”, “Communication__c”, “Outbound_Message__c”.
  - Fields:
    - Recipient (Contact/Account),
    - Channel (email/SMS/letter/portal),
    - Category (welcome, approval, reminder, dunning, etc.),
    - Template identifier,
    - Date/time sent,
    - Delivery status.

- **Linking**
  - Related to:
    - Request,
    - Engagement,
    - Case,
    - Transaction,
    - Or all of the above via polymorphic fields.

- **Purpose**
  - Audit:
    - “What did we send, exactly when?”
  - Customer journey:
    - Understand sequencing of notifications and actions.

Pattern is the same whether it’s benefit letters, policy documents, or subscription emails.

---

### 13.22.10 External IDs, systems-of-record & integration contracts (generic)

To avoid chaos when multiple systems are involved:

- **External ID per major entity**
  - Person, Account, Request, Engagement, Transaction, etc.
  - Marked as:
    - `External ID`, often `Unique` where appropriate.
  - Used as:
    - Primary match keys for integrations.

- **System of Record flags**
  - For each object or domain:
    - Decide:
      - Salesforce vs external system as authority.
  - Example (generic):
    - Billing amounts from billing system,
    - Basic contact details from Salesforce,
    - Risk scores from scoring engine.

- **Field-level ownership**
  - Some fields:
    - Owned only by external system (read-only sync into Salesforce).
  - Others:
    - Owned by Salesforce (synced outward or not synced).

- **Contracts**
  - Document:
    - Which system is responsible for creating/updating/deleting each entity,
    - What events or APIs they publish/consume.

This keeps “who’s allowed to change what” clear and prevents data wars.

---

### 13.22.11 Configuration, rules & mapping objects

Instead of burying industry rules in code:

- **Rule/config objects**
  - Represent things like:
    - Eligibility rules,
    - Pricing tables,
    - Routing rules,
    - Risk matrices.
  - Fields:
    - Condition keys (segment, region, offering type, etc.),
    - Outputs (target queue, allowed action, limit/threshold).

- **Mapping tables**
  - Map:
    - Internal codes ↔ external codes ↔ legacy codes.
  - Examples:
    - Offering code mapping,
    - Status code mapping,
    - Reason code mapping.

- **Use by automation**
  - Flows and Apex:
    - Read these config objects,
    - Avoid hard-coded switch statements everywhere.

This makes it easier to support change-heavy industries without constant code deployments.

---

### 13.22.12 Time & history patterns (effective dating & snapshots)

Time is critical across industries:

- **Effective dating**
  - For:
    - Relationships (person↔org, person↔engagement),
    - Config/rules,
    - Pricing,
    - Terms & conditions.
  - Fields:
    - `Effective_From__c` / `Effective_To__c`.
  - Queries:
    - “What was valid on date D” vs “what’s valid right now.”

- **Snapshots**
  - For decisions or contract moments:
    - Capture:
      - Key data and rules at time of decision.
  - Implementation:
    - Copy data into “Snapshot__c” object,
    - Or version the existing record with a version field.

- **Audit trails**
  - Beyond standard field history:
    - Custom objects for:
      - Decision history,
      - Status change history,
      - Ownership change history.

This is important for compliance, disputes, and analytics regardless of vertical.

---

### 13.22.13 Industry-style data modeling anti-patterns (generic)

Things I avoid across any industry:

- **Single mega-object for everything**
  - One object used as:
    - Request + Engagement + Case + Transaction.
  - Leads to:
    - Messy fields, impossible automation, and poor reporting.

- **No explicit external IDs**
  - Relying on:
    - Names/emails/codes in text fields for integration.
  - Makes deduping and syncing painful.

- **Overloading Account vs Contact**
  - Person attributes on Account, org attributes on Contact.
  - Breaks identity, SSO mapping, and reporting.

- **Hard-coded rules everywhere**
  - Business rules:
    - Buried only in Apex/LWC without config.
  - Forces:
    - Code changes for every product/plan/rule tweak.

- **Ignoring time**
  - Overwriting:
    - Key contract/pricing/status fields in place.
  - No way to answer:
    - “What was true when we made that decision?”

---

### 13.22.14 To Validate – Your concrete (but anonymized) instances

Later, when you turn this into your personal KB, you can plug in **generic descriptions** of what you actually built, like:

- “We used `Contact` + `Account` + `PersonRole__c` for people and their roles.”
- “Our offerings were modeled on `Product2` with a `Service_Level__c` field and region-based config.”
- “Requests were `Request__c` with lifecycle statuses X/Y/Z, generated from portal/internal UI.”
- “Engagements were `Subscription__c` and `Participation__c` related to Request + Offering.”
- “Transactions were `Transaction__c` for charges/payments/events, linked to Engagement.”
- “Notices were `Communication__c`, linked to Request/Engagement/Case.”
- “We used `External_*_Id__c` fields for all major entities and config tables for routing/eligibility.”

## 13.23 Data Migration & Cleanup Patterns – How I Move & Fix Large Datasets

### 13.23.1 How I think about “migration” vs “integration”

In my head:

- **Migration** = “Get a corpus of data from old world → new world, correctly, once (or a few times).”
- **Integration** = “Keep two (or more) worlds in sync over time.”

Mental rule:

> “If the question is ‘How do we start in Salesforce with the right data baseline?’, that’s migration. If it’s ‘How do we keep it right every day after that?’, that’s integration.”

Migration design still uses a lot of the same tools (ETL, APIs, events) but with different constraints:
- Heavy volume,
- One-time mappings,
- Complex historical edge cases,
- Tight cutover windows.

---

### 13.23.2 Discovery, profiling & mapping – before touching any ETL

Before I think about tools, I think about **shape**:

- **Source profiling**
  - Understand:
    - Record counts per table/object,
    - Distribution of key fields,
    - Null/blank rates,
    - Dirty patterns (invalid emails, codes that make no sense, free-text fields misused as “status”).
  - Use:
    - Source DB queries,
    - Temporary staging tables,
    - Quick scripts.

- **Target modeling check**
  - Ensure:
    - The Salesforce data model (standard + custom objects) actually represents:
      - People,
      - Orgs,
      - Requests,
      - Engagements,
      - Cases,
      - Transactions.
  - Avoid:
    - Mapping legacy “confused” tables 1:1 without cleaning the model.

- **Field-level mapping**
  - For each field:
    - Source field(s),
    - Target field,
    - Transform rules (format, code mapping, defaults),
    - Whether it is:
      - Required,
      - Read-only,
      - System-of-record-linked.

- **Code / enumerations mapping**
  - Build mapping tables for:
    - Status codes,
    - Types,
    - Reasons,
    - Program/plan codes,
    - Region/country codes.
  - Store:
    - In config objects/ETL lookup tables, not in code comments.

This pre-work prevents “we loaded it and it looks weird” disasters.

---

### 13.23.3 Staging & ETL patterns (including very large ID sets)

I don’t stream directly from source → Salesforce; I put structure in the middle:

- **Staging layer**
  - In:
    - Files on disk,
    - Staging DB tables,
    - ETL platform data stores.
  - Use staging to:
    - Normalize,
    - Clean,
    - Enrich,
    - Split into batches.

- **Batch-friendly structures**
  - For ID-heavy processes (e.g., hundreds of thousands of IDs):
    - Generate:
      - Flat files (CSV) of IDs,
      - Or staging tables keyed by those IDs.
    - Chunk:
      - Into manageable groups (e.g., Nk IDs per batch).

- **Dynamic batching patterns**
  - Example style:
    - Write IDs to a file,
    - Read them back in the ETL tool,
    - Dynamically group into “IN-clause sized” lists for source DB queries.
  - Purpose:
    - Avoid:
      - Massive “IN (300k items)” queries,
      - Hitting DB limits or performance cliffs.

- **Re-usable ETL components**
  - Build reusable bits for:
    - Reading chunked ID sets,
    - Doing lookups,
    - Writing to Salesforce,
    - Logging successes/failures.

You can swap specific tools (ETL platform, scripting language, DB), but the pattern stays.

---

### 13.23.4 ID strategy: external IDs, surrogate keys & crosswalks

Migration lives or dies on **identity**:

- **External ID fields in Salesforce**
  - On key objects:
    - Contacts, Accounts,
    - Requests, Engagements,
    - Legacy “case-like” and “transaction-like” objects.
  - These store:
    - Source system keys,
    - Often marked `External ID` and `Unique`.

- **Crosswalks**
  - Separate tables/objects that map:
    - `Legacy_Id` ↔ `Salesforce_Id`,
    - Possibly across multiple sources (legacy CRM, legacy DB, etc.).
  - Used for:
    - Subsequent loads (children referencing parents),
    - Post-migration integrations,
    - Rollback/analysis.

- **Synthetic/surrogate keys**
  - Sometimes:
    - Legacy system doesn’t have stable keys, or keys are composite.
  - Strategy:
    - Generate surrogate keys in staging (hashes/IDs),
    - Use them consistently throughout the migration pipeline.

- **Avoid using names/emails as keys**
  - Names and emails:
    - Change,
    - Are not guaranteed unique,
    - Can be inconsistent across systems.
  - External ID fields:
    - Should be the canonical matching mechanism.

This ID discipline is what makes multi-stage migration + later integrations survivable.

---

### 13.23.5 Load ordering & dependency strategy

Salesforce migration isn’t just “load a CSV.” You need **order**:

- **Typical order (generic)**
  1. Reference/config tables (lookup values, code tables, mapping objects).
  2. Core entities:
     - Accounts/organizations,
     - Contacts/persons.
  3. Relationship/junction entities:
     - Person↔Org,
     - Team memberships,
     - Role associations.
  4. Requests / Applications / Orders.
  5. Engagements / Contracts / Subscriptions.
  6. Cases / Issues / Support records.
  7. Transactions / Activities / Events.
  8. Communications / notices, where needed.

- **Parent-before-child rule**
  - Always load parents first:
    - So child records can reference them by:
      - Salesforce Id (via crosswalk),
      - Or External ID (using upsert).

- **Incremental vs big-bang loads**
  - Patterns:
    - Seed historical data in one or a few large passes.
    - Use incremental loads for:
      - Newer or frequently changing records,
      - “Delta” changes up to go-live.

This reduces foreign key failures and weird orphan data.

---

### 13.23.6 APIs & tools: Bulk vs REST vs ETL-native loaders

Tool-specific choices are contextual, but the patterns are:

- **Bulk API v1 / v2**
  - Use when:
    - Large volumes (tens/hundreds of thousands, millions).
  - Design:
    - Batch sizes tuned to org performance,
    - Retry logic for failed batches,
    - Logging of batch results.

- **REST/SOAP APIs**
  - Use when:
    - Lower volume,
    - Complex business logic per record,
    - Real-time or near real-time constraints.

- **ETL-native connectors**
  - Use:
    - Platform’s Salesforce connector where possible,
    - But still enforce:
      - External IDs,
      - Idempotence,
      - Proper error logging.

- **Mixed approach**
  - For extremely large migrations:
    - Pre-load static / historical data via Bulk,
    - Use integration-grade operations for:
      - Final deltas,
      - Complex sequences with custom logic.

The core idea: pick the simplest tool that meets volume/performance needs and still lets you see what went wrong.

---

### 13.23.7 Validation, reconciliation & sign-off

Migration is only “done” if you can **prove** it’s right:

- **Record counts**
  - Compare:
    - Source vs staging vs Salesforce:
      - Total counts,
      - Breakdown by key categories (type/status/date ranges).

- **Referential integrity**
  - Check:
    - No child records without required parents,
    - No broken references (null where it shouldn’t be),
    - Junction objects only reference valid entities.

- **Key business slices**
  - Validate:
    - High-value or risky segments,
    - Example: “all high-priority records in the last 12 months” match across systems.

- **Spot checks**
  - Select:
    - Representative examples of:
      - Simple cases,
      - Edge cases,
      - Dirty legacy records.
  - Verify:
    - The new representation in Salesforce makes sense to business users.

- **Automated reconciliation reports**
  - ETL tools and/or Salesforce reports:
    - Show matched/unmatched records,
    - Highlight anomalies.

You’re not just moving rows; you’re moving meaning.

---

### 13.23.8 Historical vs current data strategies

Not all legacy data belongs in the live Salesforce org:

- **Hot vs warm vs cold**
  - **Hot**:
    - Frequently referenced, directly used in active processes (recent records).
  - **Warm**:
    - Occasionally needed for context (past years, open-but-older items).
  - **Cold**:
    - Rarely touched, kept mainly for compliance/audit.

- **Strategies**
  - Hot:
    - Fully migrated.
  - Warm:
    - Migrated with lighter detail or fewer related objects.
  - Cold:
    - Left in legacy DB, data warehouse, or archive,
    - Accessed via:
      - BI tools,
      - Read-only integration,
      - On-demand retrieval if needed.

- **Avoid “migrate everything forever because it exists”**
  - That leads to:
    - Massive storage overhead,
    - Slow queries,
    - Noise in UX.

Instead, design what must live in Salesforce to support current/future ops + analytics.

---

### 13.23.9 Cleanup patterns inside Salesforce (after or alongside migration)

Once data is in Salesforce, or when cleaning existing orgs, I use **structured cleanup patterns**:

- **Soft-delete flags**
  - Add a field like:
    - `To_be_deleted__c` or `Archived__c`.
  - Steps:
    1. Flag candidates via controlled queries,
    2. Review/validate with reports,
    3. Run batch jobs that:
       - Delete or archive records in chunks.

- **Archive objects**
  - Create parallel “Archive__c” objects or external storage for:
    - Old cases/requests,
    - Old transactions.
  - Move:
    - Summaries or key fields,
    - Free up core objects.

- **Deduplication patterns**
  - Use:
    - Matching rules,
    - Duplicate rules,
    - Third-party tools or custom logic.
  - Approach:
    - Identify duplicates using:
      - External IDs,
      - Match keys,
      - Fuzzy logic when needed.
    - Merge:
      - In waves (by segment),
      - With careful logging of winners/losers.

- **Field rationalization**
  - Identify:
    - Unused or redundant fields,
    - Fields used as multi-purpose free text.
  - Plan:
    - Consolidation,
    - Migration of values into structured fields,
    - Retirement of old fields over time.

Cleanup is an ongoing discipline, not a one-time project.

---

### 13.23.10 Large-volume deletion & reprocessing strategies

Deleting or reprocessing huge datasets needs care:

- **Batch Apex or ETL-driven deletes**
  - Use:
    - Batch Apex:
      - With query that only selects flagged records,
      - With moderate batch sizes to avoid row locks.
    - ETL deletes:
      - Use External IDs or Salesforce Ids,
      - With logging of what was removed.

- **Avoiding row-locking**
  - Group deletes/updates:
    - By parent or logical partition (e.g., by date, region, type).
  - Stagger:
    - Schedule across off-peak hours,
    - Avoid overlapping with integration jobs hitting same objects.

- **Reprocessing patterns**
  - For failed migration segments or logic errors:
    - Implement:
      - Idempotent ETL steps,
      - Repeatable “batch key” logic (e.g., per date range or external ID range).
    - Use:
      - Error logs to identify subsets needing re-run.

- **Safety nets**
  - Before destructive operations:
    - Export backups (via backup tool or ETL),
    - Log:
      - Which records are affected,
      - The criteria used.

This is how you can safely say “we can fix this without trashing the org.”

---

### 13.23.11 Cutover & freeze strategies

Migrating is not just “run the job”; it’s also **cutover planning**:

- **Data freeze windows**
  - Define:
    - When legacy systems are read-only or offline,
    - When Salesforce becomes the system of record.
  - Use:
    - “Delta” loads to capture changes between:
      - Initial full migration and final cutover.

- **Dress rehearsals**
  - Run:
    - Full or partial mock migrations into sandboxes.
  - Validate:
    - Durations,
    - Job steps,
    - Performance,
    - Data quality.

- **Backout plans**
  - For each cutover:
    - Know:
      - Whether you can:
        - Roll back changes,
        - Or need to “fix forward”.
  - Leverage:
    - Backups,
    - Logs,
    - Crosswalks.

- **Stakeholder communication**
  - Document:
    - Timelines,
    - Freeze periods,
    - What users can/can’t do,
    - Expected UI changes.

Good migration is as much project management as it is ETL.

---

### 13.23.12 Migration & cleanup anti-patterns I deliberately avoid

Some things I treat as “do not do this”:

- **Single giant “one-shot” job with no logs**
  - If it fails, you have:
    - No idea what succeeded vs failed,
    - No way to re-run safely.

- **Using Name/Email as matching key**
  - Leads to:
    - Wrong merges,
    - Duplicates,
    - Broken references.

- **Loading parents and children out of order with no crosswalks**
  - Results in:
    - Child rows that cannot be created,
    - Orphans, missing links.

- **Multi-purpose mega objects**
  - Using one object as:
    - Legacy everything-holder,
    - Without splitting into Requests/Engagements/Cases/etc.
  - Keeps all legacy confusion instead of improving the model.

- **No reconciliation**
  - Declaring “done” without:
    - Count checks,
    - Referential checks,
    - Business validation.

- **Deleting data without flags/backups**
  - No:
    - `To_be_deleted__c` staging,
    - No pre-deletion export,
    - No written criteria.
  - If something goes wrong:
    - There’s no clear recovery path.

---

### 13.23.13 To Validate – Your concrete (but anonymized) migration stories

Later, when you build your knowledge base, you can layer your real experiences on top of this pattern, still anonymized, for example:

- How you handled migrations with:
  - Hundreds of thousands of key IDs,
  - Dynamic batching into DB queries,
  - Writing/reading files on disk through an ETL platform.
- Specific:
  - External ID naming conventions you standardized (e.g., `LegacyCRM_Id__c`, `HRSystem_Id__c`).
- Your:
  - Cutover weekends / freeze windows,
  - Dress rehearsal learnings (what went wrong, what you fixed).
- Patterns you used for:
  - Flagging records for deletion (`To_be_deleted__c`),
  - Triaging errors (error objects, log tables),
  - Reprocessing failed subsets.





